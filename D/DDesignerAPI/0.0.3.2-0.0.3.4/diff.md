# Comparing `tmp/DDesignerAPI-0.0.3.2-py3-none-any.whl.zip` & `tmp/DDesignerAPI-0.0.3.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,31 +1,31 @@
-Zip file size: 50094 bytes, number of entries: 29
--rw-rw-r--  2.0 unx      962 b- defN 23-May-22 07:59 ddesigner_api/__init__.py
--rw-rw-r--  2.0 unx      725 b- defN 23-May-22 07:59 ddesigner_api/numpy/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-22 07:59 ddesigner_api/numpy/examples/__init__.py
--rw-rw-r--  2.0 unx     2583 b- defN 23-May-22 07:59 ddesigner_api/numpy/examples/examples_numpy.py
--rw-rw-r--  2.0 unx      738 b- defN 23-May-22 07:59 ddesigner_api/numpy/xwn/__init__.py
--rw-rw-r--  2.0 unx     4852 b- defN 23-May-22 07:59 ddesigner_api/numpy/xwn/optimization.py
--rw-rw-r--  2.0 unx      770 b- defN 23-May-22 07:59 ddesigner_api/pytorch/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-22 07:59 ddesigner_api/pytorch/examples/__init__.py
--rw-rw-r--  2.0 unx     7416 b- defN 23-May-22 07:59 ddesigner_api/pytorch/examples/examples_pytorch.py
--rw-rw-r--  2.0 unx      736 b- defN 23-May-22 07:59 ddesigner_api/pytorch/xwn/__init__.py
--rw-rw-r--  2.0 unx    65337 b- defN 23-May-22 07:59 ddesigner_api/pytorch/xwn/torch_nn.py
--rw-rw-r--  2.0 unx     5223 b- defN 23-May-22 07:59 ddesigner_api/pytorch/xwn/torch_opt.py
--rw-rw-r--  2.0 unx      873 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/__init__.py
--rw-rw-r--  2.0 unx    15071 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/dpi_blocks.py
--rw-rw-r--  2.0 unx    13010 b- defN 23-Jun-30 10:36 ddesigner_api/tensorflow/dpi_layers.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/examples/__init__.py
--rw-rw-r--  2.0 unx     7883 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/examples/examples_keras.py
--rw-rw-r--  2.0 unx     3021 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/examples/examples_tensorflow.py
--rw-rw-r--  2.0 unx      790 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/__init__.py
--rw-rw-r--  2.0 unx    16354 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/base_conv.py
--rw-rw-r--  2.0 unx    29476 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/keras_layers.py
--rw-rw-r--  2.0 unx     5083 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/keras_opt.py
--rw-rw-r--  2.0 unx     9240 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/tf_nn.py
--rw-rw-r--  2.0 unx     4167 b- defN 23-May-22 07:59 ddesigner_api/tensorflow/xwn/tf_opt.py
--rwxrwxr-x  2.0 unx     9136 b- defN 23-Jun-30 10:37 DDesignerAPI-0.0.3.2.dist-info/LICENSE
--rw-rw-r--  2.0 unx     7251 b- defN 23-Jun-30 10:37 DDesignerAPI-0.0.3.2.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Jun-30 10:37 DDesignerAPI-0.0.3.2.dist-info/WHEEL
--rw-rw-r--  2.0 unx       14 b- defN 23-Jun-30 10:37 DDesignerAPI-0.0.3.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2736 b- defN 23-Jun-30 10:37 DDesignerAPI-0.0.3.2.dist-info/RECORD
-29 files, 213539 bytes uncompressed, 45564 bytes compressed:  78.7%
+Zip file size: 58006 bytes, number of entries: 29
+-rw-rw-r--  2.0 unx     1920 b- defN 23-Jul-25 06:20 ddesigner_api/__init__.py
+-rw-rw-r--  2.0 unx     1683 b- defN 23-Jul-25 06:20 ddesigner_api/numpy/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-25 06:20 ddesigner_api/numpy/examples/__init__.py
+-rw-rw-r--  2.0 unx     3541 b- defN 23-Jul-25 06:20 ddesigner_api/numpy/examples/examples_numpy.py
+-rw-rw-r--  2.0 unx     1696 b- defN 23-Jul-25 06:20 ddesigner_api/numpy/xwn/__init__.py
+-rw-rw-r--  2.0 unx     5810 b- defN 23-Jul-25 06:20 ddesigner_api/numpy/xwn/optimization.py
+-rw-rw-r--  2.0 unx     1728 b- defN 23-Jul-25 06:20 ddesigner_api/pytorch/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-25 06:20 ddesigner_api/pytorch/examples/__init__.py
+-rw-rw-r--  2.0 unx    10918 b- defN 23-Jul-25 06:52 ddesigner_api/pytorch/examples/examples_pytorch.py
+-rw-rw-r--  2.0 unx     1694 b- defN 23-Jul-25 06:20 ddesigner_api/pytorch/xwn/__init__.py
+-rw-rw-r--  2.0 unx    56599 b- defN 23-Jul-25 06:47 ddesigner_api/pytorch/xwn/torch_nn.py
+-rw-rw-r--  2.0 unx     6181 b- defN 23-Jul-25 06:20 ddesigner_api/pytorch/xwn/torch_opt.py
+-rw-rw-r--  2.0 unx      873 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/__init__.py
+-rw-rw-r--  2.0 unx    15071 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/dpi_blocks.py
+-rw-rw-r--  2.0 unx    13010 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/dpi_layers.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/examples/__init__.py
+-rw-rw-r--  2.0 unx     7883 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/examples/examples_keras.py
+-rw-rw-r--  2.0 unx     3021 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/examples/examples_tensorflow.py
+-rw-rw-r--  2.0 unx      790 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/__init__.py
+-rw-rw-r--  2.0 unx    16354 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/base_conv.py
+-rw-rw-r--  2.0 unx    29476 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/keras_layers.py
+-rw-rw-r--  2.0 unx     5083 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/keras_opt.py
+-rw-rw-r--  2.0 unx     9240 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/tf_nn.py
+-rw-rw-r--  2.0 unx     4167 b- defN 23-Jul-25 06:20 ddesigner_api/tensorflow/xwn/tf_opt.py
+-rwxrwxr-x  2.0 unx    22114 b- defN 23-Jul-25 06:59 DDesignerAPI-0.0.3.4.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     7406 b- defN 23-Jul-25 06:59 DDesignerAPI-0.0.3.4.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-25 06:59 DDesignerAPI-0.0.3.4.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       14 b- defN 23-Jul-25 06:59 DDesignerAPI-0.0.3.4.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2743 b- defN 23-Jul-25 06:59 DDesignerAPI-0.0.3.4.dist-info/RECORD
+29 files, 229107 bytes uncompressed, 53476 bytes compressed:  76.7%
```

## zipnote {}

```diff
@@ -66,23 +66,23 @@
 
 Filename: ddesigner_api/tensorflow/xwn/tf_nn.py
 Comment: 
 
 Filename: ddesigner_api/tensorflow/xwn/tf_opt.py
 Comment: 
 
-Filename: DDesignerAPI-0.0.3.2.dist-info/LICENSE
+Filename: DDesignerAPI-0.0.3.4.dist-info/LICENSE
 Comment: 
 
-Filename: DDesignerAPI-0.0.3.2.dist-info/METADATA
+Filename: DDesignerAPI-0.0.3.4.dist-info/METADATA
 Comment: 
 
-Filename: DDesignerAPI-0.0.3.2.dist-info/WHEEL
+Filename: DDesignerAPI-0.0.3.4.dist-info/WHEEL
 Comment: 
 
-Filename: DDesignerAPI-0.0.3.2.dist-info/top_level.txt
+Filename: DDesignerAPI-0.0.3.4.dist-info/top_level.txt
 Comment: 
 
-Filename: DDesignerAPI-0.0.3.2.dist-info/RECORD
+Filename: DDesignerAPI-0.0.3.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ddesigner_api/__init__.py

```diff
@@ -1,21 +1,40 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 try:
     import tensorflow as tf
     from ddesigner_api import tensorflow
 except Exception as e:
     print('*wraning: can not import tensorflow')
```

## ddesigner_api/numpy/__init__.py

```diff
@@ -1,17 +1,36 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 
 from ddesigner_api.numpy import xwn
```

## ddesigner_api/numpy/examples/examples_numpy.py

```diff
@@ -1,21 +1,40 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 import numpy as np
 from ddesigner_api.numpy.xwn.optimization import Optimization
 
 
 
 def transform():
```

## ddesigner_api/numpy/xwn/__init__.py

```diff
@@ -1,17 +1,36 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 
 from ddesigner_api.numpy.xwn import optimization
```

## ddesigner_api/numpy/xwn/optimization.py

```diff
@@ -1,21 +1,40 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 import numpy as np
 
 
 
 class Optimization:
     def __init__(
```

## ddesigner_api/pytorch/__init__.py

```diff
@@ -1,18 +1,37 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 
 from ddesigner_api.pytorch import xwn
 from ddesigner_api.pytorch import examples
```

## ddesigner_api/pytorch/examples/examples_pytorch.py

```diff
@@ -1,21 +1,40 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 import numpy as np
 import torch
 from ddesigner_api.pytorch.xwn import torch_nn as nn
 
 
 
@@ -194,31 +213,101 @@
             max_scale=4.0,
         )
         m.weight.copy_(kernel)
         o = m(x)
         print(o)
     print('==========================')
 
+def fixed_input_tconv1d():
+    x_in = np.array([                                                                            
+        2, 1, 2, 0, 1,                                                                  
+        1, 3, 2, 2, 3,                                                                  
+        1, 1, 3, 3, 0,                                                                  
+        2, 2, 0, 1, 1,
+        3, 1, 0, 3, 1,
+        0, 0, 3, 1, 2, ])                                                              
+    kernel_in = np.array([                                                                        
+        2, 0.1, 3,                                                                   
+        0, 0.3, 1,  ])                                                               
+    x = torch.from_numpy(np.reshape(x_in, (2,3,5)).astype('float32'))
+    kernel = torch.from_numpy(np.reshape(kernel_in, (3,1,2)).astype('float32'))
+    print('Input Shape = {}, Kernel Shape = {}'.format(x.shape, kernel.shape))
+    
+    print('====== torch.nn.ConvTranspose1d ======')
+    with torch.no_grad():
+        m = torch.nn.ConvTranspose1d(
+            in_channels=3, 
+            out_channels=1, 
+            kernel_size=2, 
+            stride=1, 
+            # padding='valid', 
+            padding=0, 
+            bias=False
+        )
+        m.weight.copy_(kernel)
+        # o = torch.permute(m(x), (0,2,3,1))
+        o = m(x)
+        print(o)
+    print('==========================')
+    
+    print('====== dpi_nn.ConvTranspose1d (without opt) =====')
+    with torch.no_grad():
+        m = nn.ConvTranspose1d(
+            in_channels=3, 
+            out_channels=1, 
+            kernel_size=2, 
+            stride=1, 
+            # padding='valid', 
+            padding=0, 
+            bias=False
+        )
+        m.weight.copy_(kernel)
+        o = m(x)
+        print(o)
+    print('==========================')
+    
+    print('====== dpi_nn.ConvTranspose1d (with opt) =====')
+    with torch.no_grad():
+        m = nn.ConvTranspose1d(
+            in_channels=3, 
+            out_channels=1, 
+            kernel_size=2, 
+            stride=1, 
+            # padding='valid', 
+            padding=0, 
+            bias=False,
+            use_transform=True,
+            bit=4,
+            max_scale=4.0,
+        )
+        m.weight.copy_(kernel)
+        o = m(x)
+        print(o)
+    print('==========================')
+
 
 def main():
     print('====== PYTORCH Examples======')
 
     while True:
         print('1: Fixed  Float32 Input Conv2D')
         print('2: Random Float32 Input Conv2D')
         print('3: Fixed  Float32 Input Conv1D')
+        print('4: Fixed  Float32 Input Conv1DTranspose')
         print('q: Quit')
         print('>>> Select Case:')
         cmd = input()
         if cmd == '1':
             fixed_input()
         elif cmd == '2':
             random_input()
         elif cmd == '3':
             fixed_input_conv1d()
+        elif cmd == '4': 
+            fixed_input_tconv1d()
         elif cmd == 'q': 
             break
         
     return True
```

## ddesigner_api/pytorch/xwn/__init__.py

```diff
@@ -1,17 +1,36 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 
 from ddesigner_api.pytorch.xwn import torch_nn
```

## ddesigner_api/pytorch/xwn/torch_nn.py

```diff
@@ -1,7 +1,39 @@
+# Copyright 2023 The Deeper-I Authors. All Rights Reserved.
+#
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
 # -*- coding: utf-8 -*-
 import math
 import warnings
 
 import torch
 from torch import Tensor
 from torch.nn import functional as F
@@ -409,356 +441,202 @@
 #         )
 # 
 #     def forward(self, input: Tensor) -> Tensor:
 #         return self._conv_forward(input, self.weight, self.bias)
 # 
 # 
 # 
-# class _ConvTransposeNd(_ConvNd):
-#     def __init__(self, in_channels, out_channels, kernel_size, stride,
-#                  padding, dilation, transposed, output_padding,
-#                  groups, bias, padding_mode, device=None, dtype=None) -> None:
-#         if padding_mode != 'zeros':
-#             raise ValueError('Only "zeros" padding mode is supported for {}'.format(self.__class__.__name__))
-# 
-#         factory_kwargs = {'device': device, 'dtype': dtype}
-#         super(_ConvTransposeNd, self).__init__(
-#             in_channels, out_channels, kernel_size, stride,
-#             padding, dilation, transposed, output_padding,
-#             groups, bias, padding_mode, **factory_kwargs)
-# 
-#     # dilation being an optional parameter is for backwards
-#     # compatibility
-#     def _output_padding(self, input: Tensor, output_size: Optional[List[int]],
-#                         stride: List[int], padding: List[int], kernel_size: List[int],
-#                         num_spatial_dims: int, dilation: Optional[List[int]] = None) -> List[int]:
-#         if output_size is None:
-#             ret = _single(self.output_padding)  # converting to list if was not already
-#         else:
-#             has_batch_dim = input.dim() == num_spatial_dims + 2
-#             num_non_spatial_dims = 2 if has_batch_dim else 1
-#             if len(output_size) == num_non_spatial_dims + num_spatial_dims:
-#                 output_size = output_size[num_non_spatial_dims:]
-#             if len(output_size) != num_spatial_dims:
-#                 raise ValueError(
-#                     "ConvTranspose{}D: for {}D input, output_size must have {} or {} elements (got {})"
-#                     .format(num_spatial_dims, input.dim(), num_spatial_dims,
-#                             num_non_spatial_dims + num_spatial_dims, len(output_size)))
-# 
-#             min_sizes = torch.jit.annotate(List[int], [])
-#             max_sizes = torch.jit.annotate(List[int], [])
-#             for d in range(num_spatial_dims):
-#                 dim_size = ((input.size(d + num_non_spatial_dims) - 1) * stride[d] -
-#                             2 * padding[d] +
-#                             (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)
-#                 min_sizes.append(dim_size)
-#                 max_sizes.append(min_sizes[d] + stride[d] - 1)
-# 
-#             for i in range(len(output_size)):
-#                 size = output_size[i]
-#                 min_size = min_sizes[i]
-#                 max_size = max_sizes[i]
-#                 if size < min_size or size > max_size:
-#                     raise ValueError((
-#                         "requested an output size of {}, but valid sizes range "
-#                         "from {} to {} (for an input of {})").format(
-#                             output_size, min_sizes, max_sizes, input.size()[2:]))
-# 
-#             res = torch.jit.annotate(List[int], [])
-#             for d in range(num_spatial_dims):
-#                 res.append(output_size[d] - min_sizes[d])
-# 
-#             ret = res
-#         return ret
-# 
-# 
-# class ConvTranspose1d(_ConvTransposeNd):
-#     __doc__ = r"""Applies a 1D transposed convolution operator over an input image
-#     composed of several input planes.
-# 
-#     This module can be seen as the gradient of Conv1d with respect to its input.
-#     It is also known as a fractionally-strided convolution or
-#     a deconvolution (although it is not an actual deconvolution operation as it does
-#     not compute a true inverse of convolution). For more information, see the visualizations
-#     `here`_ and the `Deconvolutional Networks`_ paper.
-# 
-#     This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
-# 
-#     On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
-# 
-#     * :attr:`stride` controls the stride for the cross-correlation.
-# 
-#     * :attr:`padding` controls the amount of implicit zero padding on both
-#       sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
-#       below for details.
-# 
-#     * :attr:`output_padding` controls the additional size added to one side
-#       of the output shape. See note below for details.
-# 
-#     * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
-#       It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.
-# 
-#     {groups_note}
-# 
-#     Note:
-#         The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
-#         amount of zero padding to both sizes of the input. This is set so that
-#         when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`
-#         are initialized with same parameters, they are inverses of each other in
-#         regard to the input and output shapes. However, when ``stride > 1``,
-#         :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output
-#         shape. :attr:`output_padding` is provided to resolve this ambiguity by
-#         effectively increasing the calculated output shape on one side. Note
-#         that :attr:`output_padding` is only used to find output shape, but does
-#         not actually add zero-padding to output.
-# 
-#     Note:
-#         In some circumstances when using the CUDA backend with CuDNN, this operator
-#         may select a nondeterministic algorithm to increase performance. If this is
-#         undesirable, you can try to make the operation deterministic (potentially at
-#         a performance cost) by setting ``torch.backends.cudnn.deterministic =
-#         True``.
-#         Please see the notes on :doc:`/notes/randomness` for background.
-# 
-# 
-#     Args:
-#         in_channels (int): Number of channels in the input image
-#         out_channels (int): Number of channels produced by the convolution
-#         kernel_size (int or tuple): Size of the convolving kernel
-#         stride (int or tuple, optional): Stride of the convolution. Default: 1
-#         padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
-#             will be added to both sides of the input. Default: 0
-#         output_padding (int or tuple, optional): Additional size added to one side
-#             of the output shape. Default: 0
-#         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
-#         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
-#         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
-#     """.format(**reproducibility_notes, **convolution_notes) + r"""
-# 
-#     Shape:
-#         - Input: :math:`(N, C_{in}, L_{in})` or :math:`(C_{in}, L_{in})`
-#         - Output: :math:`(N, C_{out}, L_{out})` or :math:`(C_{out}, L_{out})`, where
-# 
-#           .. math::
-#               L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
-#                         \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1
-# 
-#     Attributes:
-#         weight (Tensor): the learnable weights of the module of shape
-#                          :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
-#                          :math:`\text{kernel\_size})`.
-#                          The values of these weights are sampled from
-#                          :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
-#                          :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
-#         bias (Tensor):   the learnable bias of the module of shape (out_channels).
-#                          If :attr:`bias` is ``True``, then the values of these weights are
-#                          sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
-#                          :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
-# 
-#     .. _`here`:
-#         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
-# 
-#     .. _`Deconvolutional Networks`:
-#         https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf
-#     """
-# 
-#     def __init__(
-#         self,
-#         in_channels: int,
-#         out_channels: int,
-#         kernel_size: _size_1_t,
-#         stride: _size_1_t = 1,
-#         padding: _size_1_t = 0,
-#         output_padding: _size_1_t = 0,
-#         groups: int = 1,
-#         bias: bool = True,
-#         dilation: _size_1_t = 1,
-#         padding_mode: str = 'zeros',
-#         device=None,
-#         dtype=None
-#     ) -> None:
-#         factory_kwargs = {'device': device, 'dtype': dtype}
-#         kernel_size = _single(kernel_size)
-#         stride = _single(stride)
-#         padding = _single(padding)
-#         dilation = _single(dilation)
-#         output_padding = _single(output_padding)
-#         super(ConvTranspose1d, self).__init__(
-#             in_channels, out_channels, kernel_size, stride, padding, dilation,
-#             True, output_padding, groups, bias, padding_mode, **factory_kwargs)
-# 
-#     def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
-#         if self.padding_mode != 'zeros':
-#             raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')
-# 
-#         assert isinstance(self.padding, tuple)
-#         # One cannot replace List by Tuple or Sequence in "_output_padding" because
-#         # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
-#         num_spatial_dims = 1
-#         output_padding = self._output_padding(
-#             input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
-#             num_spatial_dims, self.dilation)  # type: ignore[arg-type]
-#         return F.conv_transpose1d(
-#             input, self.weight, self.bias, self.stride, self.padding,
-#             output_padding, self.groups, self.dilation)
-# 
-# 
-# class ConvTranspose2d(_ConvTransposeNd):
-#     __doc__ = r"""Applies a 2D transposed convolution operator over an input image
-#     composed of several input planes.
-# 
-#     This module can be seen as the gradient of Conv2d with respect to its input.
-#     It is also known as a fractionally-strided convolution or
-#     a deconvolution (although it is not an actual deconvolution operation as it does
-#     not compute a true inverse of convolution). For more information, see the visualizations
-#     `here`_ and the `Deconvolutional Networks`_ paper.
-# 
-#     This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
-# 
-#     On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
-# 
-#     * :attr:`stride` controls the stride for the cross-correlation.
-# 
-#     * :attr:`padding` controls the amount of implicit zero padding on both
-#       sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
-#       below for details.
-# 
-#     * :attr:`output_padding` controls the additional size added to one side
-#       of the output shape. See note below for details.
-# 
-#     * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
-#       It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.
-# 
-#     {groups_note}
-# 
-#     The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
-#     can either be:
-# 
-#         - a single ``int`` -- in which case the same value is used for the height and width dimensions
-#         - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
-#           and the second `int` for the width dimension
-# 
-#     Note:
-#         The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
-#         amount of zero padding to both sizes of the input. This is set so that
-#         when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`
-#         are initialized with same parameters, they are inverses of each other in
-#         regard to the input and output shapes. However, when ``stride > 1``,
-#         :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output
-#         shape. :attr:`output_padding` is provided to resolve this ambiguity by
-#         effectively increasing the calculated output shape on one side. Note
-#         that :attr:`output_padding` is only used to find output shape, but does
-#         not actually add zero-padding to output.
-# 
-#     Note:
-#         {cudnn_reproducibility_note}
-# 
-#     Args:
-#         in_channels (int): Number of channels in the input image
-#         out_channels (int): Number of channels produced by the convolution
-#         kernel_size (int or tuple): Size of the convolving kernel
-#         stride (int or tuple, optional): Stride of the convolution. Default: 1
-#         padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
-#             will be added to both sides of each dimension in the input. Default: 0
-#         output_padding (int or tuple, optional): Additional size added to one side
-#             of each dimension in the output shape. Default: 0
-#         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
-#         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
-#         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
-#     """.format(**reproducibility_notes, **convolution_notes) + r"""
-# 
-#     Shape:
-#         - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`
-#         - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where
-# 
-#         .. math::
-#               H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
-#                         \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
-#         .. math::
-#               W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
-#                         \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
-# 
-#     Attributes:
-#         weight (Tensor): the learnable weights of the module of shape
-#                          :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
-#                          :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
-#                          The values of these weights are sampled from
-#                          :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
-#                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
-#         bias (Tensor):   the learnable bias of the module of shape (out_channels)
-#                          If :attr:`bias` is ``True``, then the values of these weights are
-#                          sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
-#                          :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
-# 
-#     Examples::
-# 
-#         >>> # With square kernels and equal stride
-#         >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)
-#         >>> # non-square kernels and unequal stride and with padding
-#         >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
-#         >>> input = torch.randn(20, 16, 50, 100)
-#         >>> output = m(input)
-#         >>> # exact output size can be also specified as an argument
-#         >>> input = torch.randn(1, 16, 12, 12)
-#         >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
-#         >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
-#         >>> h = downsample(input)
-#         >>> h.size()
-#         torch.Size([1, 16, 6, 6])
-#         >>> output = upsample(h, output_size=input.size())
-#         >>> output.size()
-#         torch.Size([1, 16, 12, 12])
-# 
-#     .. _`here`:
-#         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
-# 
-#     .. _`Deconvolutional Networks`:
-#         https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf
-#     """
-# 
-#     def __init__(
-#         self,
-#         in_channels: int,
-#         out_channels: int,
-#         kernel_size: _size_2_t,
-#         stride: _size_2_t = 1,
-#         padding: _size_2_t = 0,
-#         output_padding: _size_2_t = 0,
-#         groups: int = 1,
-#         bias: bool = True,
-#         dilation: _size_2_t = 1,
-#         padding_mode: str = 'zeros',
-#         device=None,
-#         dtype=None
-#     ) -> None:
-#         factory_kwargs = {'device': device, 'dtype': dtype}
-#         kernel_size = _pair(kernel_size)
-#         stride = _pair(stride)
-#         padding = _pair(padding)
-#         dilation = _pair(dilation)
-#         output_padding = _pair(output_padding)
-#         super(ConvTranspose2d, self).__init__(
-#             in_channels, out_channels, kernel_size, stride, padding, dilation,
-#             True, output_padding, groups, bias, padding_mode, **factory_kwargs)
-# 
-#     def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
-#         if self.padding_mode != 'zeros':
-#             raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
-# 
-#         assert isinstance(self.padding, tuple)
-#         # One cannot replace List by Tuple or Sequence in "_output_padding" because
-#         # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
-#         num_spatial_dims = 2
-#         output_padding = self._output_padding(
-#             input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
-#             num_spatial_dims, self.dilation)  # type: ignore[arg-type]
-# 
-#         return F.conv_transpose2d(
-#             input, self.weight, self.bias, self.stride, self.padding,
-#             output_padding, self.groups, self.dilation)
+class _ConvTransposeNd(_ConvNd):
+    def __init__(self, in_channels, out_channels, kernel_size, stride,
+                 padding, dilation, transposed, output_padding,
+                 groups, bias, padding_mode, device=None, dtype=None) -> None:
+        if padding_mode != 'zeros':
+            raise ValueError('Only "zeros" padding mode is supported for {}'.format(self.__class__.__name__))
+
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super(_ConvTransposeNd, self).__init__(
+            in_channels, out_channels, kernel_size, stride,
+            padding, dilation, transposed, output_padding,
+            groups, bias, padding_mode, **factory_kwargs)
+
+    # dilation being an optional parameter is for backwards
+    # compatibility
+    def _output_padding(self, input: Tensor, output_size: Optional[List[int]],
+                        stride: List[int], padding: List[int], kernel_size: List[int],
+                        num_spatial_dims: int, dilation: Optional[List[int]] = None) -> List[int]:
+        if output_size is None:
+            ret = _single(self.output_padding)  # converting to list if was not already
+        else:
+            has_batch_dim = input.dim() == num_spatial_dims + 2
+            num_non_spatial_dims = 2 if has_batch_dim else 1
+            if len(output_size) == num_non_spatial_dims + num_spatial_dims:
+                output_size = output_size[num_non_spatial_dims:]
+            if len(output_size) != num_spatial_dims:
+                raise ValueError(
+                    "ConvTranspose{}D: for {}D input, output_size must have {} or {} elements (got {})"
+                    .format(num_spatial_dims, input.dim(), num_spatial_dims,
+                            num_non_spatial_dims + num_spatial_dims, len(output_size)))
+
+            min_sizes = torch.jit.annotate(List[int], [])
+            max_sizes = torch.jit.annotate(List[int], [])
+            for d in range(num_spatial_dims):
+                dim_size = ((input.size(d + num_non_spatial_dims) - 1) * stride[d] -
+                            2 * padding[d] +
+                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)
+                min_sizes.append(dim_size)
+                max_sizes.append(min_sizes[d] + stride[d] - 1)
+
+            for i in range(len(output_size)):
+                size = output_size[i]
+                min_size = min_sizes[i]
+                max_size = max_sizes[i]
+                if size < min_size or size > max_size:
+                    raise ValueError((
+                        "requested an output size of {}, but valid sizes range "
+                        "from {} to {} (for an input of {})").format(
+                            output_size, min_sizes, max_sizes, input.size()[2:]))
+
+            res = torch.jit.annotate(List[int], [])
+            for d in range(num_spatial_dims):
+                res.append(output_size[d] - min_sizes[d])
+
+            ret = res
+        return ret
+
+
+class ConvTranspose1d(_ConvTransposeNd):
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_1_t,
+        stride: _size_1_t = 1,
+        padding: _size_1_t = 0,
+        output_padding: _size_1_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_1_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None,
+
+        use_transform=False, 
+        use_pruning=False, 
+        bit=4, 
+        max_scale=4.0,
+        prun_weight=0.5,
+
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size = _single(kernel_size)
+        stride = _single(stride)
+        padding = _single(padding)
+        dilation = _single(dilation)
+        output_padding = _single(output_padding)
+        super(ConvTranspose1d, self).__init__(
+            in_channels, out_channels, kernel_size, stride, padding, dilation,
+            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
+
+        # Add optimization kernel
+        self.opt = Optimization(
+            use_transform=use_transform, 
+            bit=bit, 
+            max_scale=max_scale,
+            use_pruning=use_pruning, 
+            prun_weight=prun_weight,
+            transpose=True,
+        )
+        self.opt.set_shape(self.weight.shape)
+
+    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
+        if self.padding_mode != 'zeros':
+            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')
+
+        assert isinstance(self.padding, tuple)
+        # One cannot replace List by Tuple or Sequence in "_output_padding" because
+        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
+
+        weight = self.opt.optimize(self.weight)
+
+        num_spatial_dims = 1
+        output_padding = self._output_padding(
+            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
+            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
+        # return F.conv_transpose1d(
+        #     input, self.weight, self.bias, self.stride, self.padding,
+        #     output_padding, self.groups, self.dilation)
+        return F.conv_transpose1d(
+            input, weight, self.bias, self.stride, self.padding,
+            output_padding, self.groups, self.dilation)
+
+
+class ConvTranspose2d(_ConvTransposeNd):
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_2_t,
+        stride: _size_2_t = 1,
+        padding: _size_2_t = 0,
+        output_padding: _size_2_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_2_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None,
+
+        use_transform=False, 
+        use_pruning=False, 
+        bit=4, 
+        max_scale=4.0,
+        prun_weight=0.5,
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size = _pair(kernel_size)
+        stride = _pair(stride)
+        padding = _pair(padding)
+        dilation = _pair(dilation)
+        output_padding = _pair(output_padding)
+        super(ConvTranspose2d, self).__init__(
+            in_channels, out_channels, kernel_size, stride, padding, dilation,
+            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
+
+        # Add optimization kernel
+        self.opt = Optimization(
+            use_transform=use_transform, 
+            bit=bit, 
+            max_scale=max_scale,
+            use_pruning=use_pruning, 
+            prun_weight=prun_weight,
+            transpose=True,
+        )
+        self.opt.set_shape(self.weight.shape)
+
+    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
+        if self.padding_mode != 'zeros':
+            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
+
+        assert isinstance(self.padding, tuple)
+        # One cannot replace List by Tuple or Sequence in "_output_padding" because
+        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
+
+        weight = self.opt.optimize(self.weight)
+
+        num_spatial_dims = 2
+        output_padding = self._output_padding(
+            input, output_size, self.stride, self.padding, self.kernel_size,  # type: ignore[arg-type]
+            num_spatial_dims, self.dilation)  # type: ignore[arg-type]
+
+        # return F.conv_transpose2d(
+        #     input, self.weight, self.bias, self.stride, self.padding,
+        #     output_padding, self.groups, self.dilation)
+        return F.conv_transpose2d(
+            input, weight, self.bias, self.stride, self.padding,
+            output_padding, self.groups, self.dilation)
 # 
 # 
 # class ConvTranspose3d(_ConvTransposeNd):
 #     __doc__ = r"""Applies a 3D transposed convolution operator over an input image composed of several input
 #     planes.
 #     The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
 #     and sums over the outputs from all input feature planes.
```

## ddesigner_api/pytorch/xwn/torch_opt.py

```diff
@@ -1,21 +1,40 @@
 # Copyright 2023 The Deeper-I Authors. All Rights Reserved.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
+# BSD 3-Clause License
+# 
+# Copyright (c) 2017, 
+# All rights reserved.
+# 
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions are met:
+# 
+# * Redistributions of source code must retain the above copyright notice, this
+#   list of conditions and the following disclaimer.
+# 
+# * Redistributions in binary form must reproduce the above copyright notice,
+#   this list of conditions and the following disclaimer in the documentation
+#   and/or other materials provided with the distribution.
+# 
+# * Neither the name of the copyright holder nor the names of its
+#   contributors may be used to endorse or promote products derived from
+#   this software without specific prior written permission.
+# 
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# -*- coding: utf-8 -*-
 
 import numpy as np
 import torch
 
 
 
 class Optimization:
```

## Comparing `DDesignerAPI-0.0.3.2.dist-info/METADATA` & `DDesignerAPI-0.0.3.4.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: DDesignerAPI
-Version: 0.0.3.2
+Version: 0.0.3.4
 Summary: Deep-learning Designer: Deep-Learning Training Optimization & Layers API(like Keras)
 Home-page: https://github.com/DPI/TrainingAPI
 Author: Deeper-I
 Author-email: dean@deeper-i.com
-License: Apache-2.0
-Keywords: deeper-i,xwn,tensorflow,keras
+License: ['Apache-2.0', 'BSD3-Clause']
+Keywords: xwn,pytorch,tensorflow,keras
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.7
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 
 [DDesigner API] Deep-learning Designer API
 ==========================================
@@ -33,53 +33,55 @@
 * Tensorflow 2.6.0
 * PyTorch 1.13.1
 
 ## 2.2. Components of Network 
 ### 2.2.1. Layers
 * Accelerator enabled layers and Custom layers that perform specific functions
 #### 2.2.1.1. Summary
-|Operation|Support Train Platform|Support TACHY Accelerator|Property|
-|:---:|:---:|:---:|:---:|
-|**Convolution**|TF/Keras/PyTorch|O|Basic|
-|**TransposeConvolution**|TF/Keras/PyTorch|O|Basic|
-|**FullyConnected**|TF/Keras/PyTorch|O|Basic|
-|**SqeezeAndExcitation**|Keras|X|Custom|
-|**SubPixelConv**|Keras|X|Custom|
+|Operation|Support Train Platform|Support TACHY Accelerator|
+|:---:|:---:|:---:|
+|**Convolution**|TF/Keras/PyTorch|O|
+|**TransposeConvolution**|TF/Keras/PyTorch|O|
+|**FullyConnected**|TF/Keras/PyTorch|O|
+|**CascadeConvolution**|TF/Keras/PyTorch|O|
+|**SqeezeAndExcitation**|Keras|X|
+|**SubPixelConv**|Keras|X|
 #### 2.2.1.2. Detail
 * Convolution           : 1D, 2D with XWN optimization
 * TransposeConvolution  : 1D, 2D with XWN optimization
 * FullyConnected        : Dense or matmul operation
+* CascadeConvolution    : A Layer that decomposes a layer with large kernel into multiple layers with smaller kernels to lighten the model 
 * SqeezeAndExcitation   : [arXiv:1709.01507], support 1D, 2D
 * SubPixelConv          : Layer with scale-up function. support 2D only. (CONV + Depth2Space)
 <br/><br/>
 ### 2.2.2. Blocks
 * A set of defined layers for user convenience
 #### 2.2.2.1. Summary
 |Platform|ConvBlock|TConvBlock|FCBlock|
 |:---:|:---:|:---:|:---:|
 |**TF-Keras**|1D/2D|2D|O|
 #### 2.2.2.2. Detail
-* ConvBlock     : Convolution N-D Block (CONV  + BN + ACT + DROPOUT), support Conv1DBlock, Conv2DBlock
+* ConvBlock     : Convolution N-D Block (CONV + BN + ACT + DROPOUT), support Conv1DBlock, Conv2DBlock
 * TConvBlock    : Transpose Convolution 2D Block (TCONV + BN + ACT + DROPOUT), support TConv2DBlock
-* FCBlock       : FullyConnected Block (FC    + BN + ACT + DROPOUT)
+* FCBlock       : FullyConnected Block (FC + BN + ACT + DROPOUT)
 <br/><br/>
 ## 2.3. XWN (**Applies only to convolution operations**)
 ### 2.3.1. Transform Configuration (data type / default value / description) 
 * transform     : bool  / False / Choose whether to use
 * bit           : int   / 4     / Quantization range (bit-1 ** 2)
 * max_scale     : float / 4.0   / Max value
 ### 2.3.2. Pruning Configuration
 * pruning       : bool  / False / Choose whether to use
 * prun_weight   : float / 0.5   / Weights for puning edge generation
 ### 2.3.3. Summary
 |Platform|Conv|TransposeConv|
 |:---:|:---:|:---:|
 |**TF**|1D/2D|1D/2D|
 |**TF-Keras**|1D/2D|1D/2D|
-|**PyTorch**|1D/2D|X|
+|**PyTorch**|1D/2D|1D/2D|
 
 <br/><br/>
 
 # 3. Command Usage
 ## 3.1. Blocks  
 ### 3.1.1. Keras
 #### 3.1.1.1. Conv1DBlock
```

## Comparing `DDesignerAPI-0.0.3.2.dist-info/RECORD` & `DDesignerAPI-0.0.3.4.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-ddesigner_api/__init__.py,sha256=jIJtTG_Ks3IfTfE-OyRC23w-MXShSO6IYwWk0bRvisA,962
-ddesigner_api/numpy/__init__.py,sha256=JRro7LhcyUvKHQH9_trioRisLxy-IMnTfJJVhkW0L9w,725
+ddesigner_api/__init__.py,sha256=7LmrgpUP02GBLwWGdysk-7rrsQ_0EblXvkqz3w-XnVg,1920
+ddesigner_api/numpy/__init__.py,sha256=K8Okb4_MRPoZzLARI_srAH3Uvz7EjUu7Amh9u6s75qU,1683
 ddesigner_api/numpy/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ddesigner_api/numpy/examples/examples_numpy.py,sha256=wjHtTJVuRkWTckFtG7hLGpW3coOgctTYpGwBg_uhahk,2583
-ddesigner_api/numpy/xwn/__init__.py,sha256=FtLZM4Yg3OqOlt2fxtmbDGLwsBMLmXMoRyGp3PUERvY,738
-ddesigner_api/numpy/xwn/optimization.py,sha256=nw9ti-9ZTD8uptZoKNMm2fxHOhGp-4gT3gXqy6KHzq4,4852
-ddesigner_api/pytorch/__init__.py,sha256=292PDiVMURserkbbtdAT_VVorOkS8NlBF6gPQEefu4w,770
+ddesigner_api/numpy/examples/examples_numpy.py,sha256=DIcdwAH7ur1gPKTTXjQC_EUg3TbZFXNYzKRZUCzXvrw,3541
+ddesigner_api/numpy/xwn/__init__.py,sha256=hgud3Z-3NRvh_tcaFrNdtJt597vHXonhSgR6ksSBcHQ,1696
+ddesigner_api/numpy/xwn/optimization.py,sha256=iBh-DOL3sYwY7QVkf3wAAjDIg1--zWW8S4vUQ1aRYwM,5810
+ddesigner_api/pytorch/__init__.py,sha256=4-E-FZak6KbF2dJnhGg62gfa11f3BWub7bcPuW1m1NQ,1728
 ddesigner_api/pytorch/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ddesigner_api/pytorch/examples/examples_pytorch.py,sha256=A5tIdMnUDQie4rrE1wJrd0oFfzZgOnq9evjaA0tLTf0,7416
-ddesigner_api/pytorch/xwn/__init__.py,sha256=RS2lQNf9O7p__G_wVfxtvBrw0TSf7YKB_LeTqzKUJ_M,736
-ddesigner_api/pytorch/xwn/torch_nn.py,sha256=Es4CL3qRmMBYMZRJBiwUwGoo49WVRlJiZHhwF3b1GH0,65337
-ddesigner_api/pytorch/xwn/torch_opt.py,sha256=zivpN7V0em7LkhaqZ078ZYTLdZMGt2t5alMohhnX5kY,5223
+ddesigner_api/pytorch/examples/examples_pytorch.py,sha256=2HaZfjOH5lGVqwdYvGMJu4_G8OIs6rStrApfj0YglT0,10918
+ddesigner_api/pytorch/xwn/__init__.py,sha256=cXUUsRuw46ruQKpC9NqN_k9p5jbZNfqfC8ytSDqE_RA,1694
+ddesigner_api/pytorch/xwn/torch_nn.py,sha256=wK6BySgaYVfGd8mebXdNurWmxoqcuS4yTzjt3WgJQCY,56599
+ddesigner_api/pytorch/xwn/torch_opt.py,sha256=sSrvVCtDIGPWxf_I7fVR4lDmKHIN3k8VahzUpwIp1a8,6181
 ddesigner_api/tensorflow/__init__.py,sha256=q4RdbPLAQVOiRpf5H4WNGeTqWu8GbeW9gav3RFLev18,873
 ddesigner_api/tensorflow/dpi_blocks.py,sha256=e2yRGXgtJXWnf1pf77DndAn6yddn9Rtc_MREeG4rCIk,15071
 ddesigner_api/tensorflow/dpi_layers.py,sha256=uG_8hyRYe8WwRzGdGGvKE97Zgc1-eZqSynEVHBO7AVU,13010
 ddesigner_api/tensorflow/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ddesigner_api/tensorflow/examples/examples_keras.py,sha256=3icVGij_UymaEYkOwO4TZNRkWCBjLLSg-Khtlus_Fu8,7883
 ddesigner_api/tensorflow/examples/examples_tensorflow.py,sha256=HIwwuTR6WVR2raIAVY9rexO0Ex0iiby_jZ3l14r3jn8,3021
 ddesigner_api/tensorflow/xwn/__init__.py,sha256=raA9SRLD8OupFBsA2ujBA5xCPTYHS-EO7PYVPIRXL_0,790
 ddesigner_api/tensorflow/xwn/base_conv.py,sha256=wFDQVLEsIiMofU7Dvd1rwc8Gne8Ze3zeF08Zh_WPP2I,16354
 ddesigner_api/tensorflow/xwn/keras_layers.py,sha256=xnlVPFhRuuOYimv6sH-egO_X5T4zCMR6kfNGe77pQ0M,29476
 ddesigner_api/tensorflow/xwn/keras_opt.py,sha256=Ke4PNQSN5DvBTVQVunSxmkhvZoF0-Y6V3huVDrJJlC8,5083
 ddesigner_api/tensorflow/xwn/tf_nn.py,sha256=oaIXhbau-gtFyndce2dF3xWCDOpNhAwCJNLDkR5b5iU,9240
 ddesigner_api/tensorflow/xwn/tf_opt.py,sha256=P45GaEMNKiSIdscX0VYrr02FLx8cbhYeTpMdGDMFOGo,4167
-DDesignerAPI-0.0.3.2.dist-info/LICENSE,sha256=BIg47KsjFz_m25s6T12eL1SAuGkt_KVW_tSqA5yga8g,9136
-DDesignerAPI-0.0.3.2.dist-info/METADATA,sha256=qBM9aEdXQ85gwRR9UqgDZn4FPJMkcXRlqxmRUMkIMcw,7251
-DDesignerAPI-0.0.3.2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-DDesignerAPI-0.0.3.2.dist-info/top_level.txt,sha256=qYDp5VPzyYyterLRCKrHqsvRLXTHSMOd-Xiwy2HJ_Ow,14
-DDesignerAPI-0.0.3.2.dist-info/RECORD,,
+DDesignerAPI-0.0.3.4.dist-info/LICENSE,sha256=S2Q1V1ou7fTRCDRl0p78yVcGS0q0K_9BqCyhIdsejec,22114
+DDesignerAPI-0.0.3.4.dist-info/METADATA,sha256=MyI1yZrlrMpLTLk4NkT34xGxEztGIrVZ_AC1TAJ_1sA,7406
+DDesignerAPI-0.0.3.4.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+DDesignerAPI-0.0.3.4.dist-info/top_level.txt,sha256=qYDp5VPzyYyterLRCKrHqsvRLXTHSMOd-Xiwy2HJ_Ow,14
+DDesignerAPI-0.0.3.4.dist-info/RECORD,,
```

