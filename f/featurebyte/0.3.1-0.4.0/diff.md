# Comparing `tmp/featurebyte-0.3.1.tar.gz` & `tmp/featurebyte-0.4.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "featurebyte-0.3.1.tar", max compression
+gzip compressed data, was "featurebyte-0.4.0.tar", max compression
```

## Comparing `featurebyte-0.3.1.tar` & `featurebyte-0.4.0.tar`

### file list

```diff
@@ -1,526 +1,613 @@
--rw-r--r--   0        0        0     3860 2023-06-08 15:53:14.865277 featurebyte-0.3.1/LICENSE
--rw-r--r--   0        0        0    19816 2023-06-08 15:53:14.865277 featurebyte-0.3.1/README.md
--rw-r--r--   0        0        0    14195 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/__init__.py
--rw-r--r--   0        0        0     2017 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/__main__.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/api/__init__.py
--rw-r--r--   0        0        0    30557 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/api/api_object.py
--rw-r--r--   0        0        0     3975 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/api/api_object_util.py
--rw-r--r--   0        0        0     4985 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/api/asat_aggregator.py
--rw-r--r--   0        0        0     5521 2023-06-08 15:53:14.865277 featurebyte-0.3.1/featurebyte/api/base_aggregator.py
--rw-r--r--   0        0        0    37589 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/base_table.py
--rw-r--r--   0        0        0     4790 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/batch_feature_table.py
--rw-r--r--   0        0        0     5019 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/batch_request_table.py
--rw-r--r--   0        0        0    35381 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/catalog.py
--rw-r--r--   0        0        0     1475 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/catalog_decorator.py
--rw-r--r--   0        0        0    10842 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/catalog_get_by_id_mixin.py
--rw-r--r--   0        0        0    11517 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/change_view.py
--rw-r--r--   0        0        0     6600 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/credential.py
--rw-r--r--   0        0        0     6540 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/data_source.py
--rw-r--r--   0        0        0    13660 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/deployment.py
--rw-r--r--   0        0        0     9699 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/dimension_table.py
--rw-r--r--   0        0        0     3107 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/dimension_view.py
--rw-r--r--   0        0        0    12242 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/entity.py
--rw-r--r--   0        0        0    22561 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/event_table.py
--rw-r--r--   0        0        0    16044 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/event_view.py
--rw-r--r--   0        0        0    46149 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature.py
--rw-r--r--   0        0        0    15414 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_group.py
--rw-r--r--   0        0        0    15001 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_job.py
--rw-r--r--   0        0        0     8770 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    62689 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_list.py
--rw-r--r--   0        0        0     4116 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_namespace.py
--rw-r--r--   0        0        0    10016 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_store.py
--rw-r--r--   0        0        0     1999 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_util.py
--rw-r--r--   0        0        0      557 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/feature_validation_util.py
--rw-r--r--   0        0        0    18206 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/groupby.py
--rw-r--r--   0        0        0     4927 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/historical_feature_table.py
--rw-r--r--   0        0        0    16661 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/item_table.py
--rw-r--r--   0        0        0    11752 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/item_view.py
--rw-r--r--   0        0        0     2909 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/lag.py
--rw-r--r--   0        0        0     4909 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/materialized_table.py
--rw-r--r--   0        0        0     4952 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/observation_table.py
--rw-r--r--   0        0        0      721 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/periodic_task.py
--rw-r--r--   0        0        0     8442 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/relationship.py
--rw-r--r--   0        0        0     3474 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/request_column.py
--rw-r--r--   0        0        0     5018 2023-06-08 15:53:14.869277 featurebyte-0.3.1/featurebyte/api/savable_api_object.py
--rw-r--r--   0        0        0    21162 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/scd_table.py
--rw-r--r--   0        0        0     6023 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/scd_view.py
--rw-r--r--   0        0        0     3119 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/simple_aggregator.py
--rw-r--r--   0        0        0    46157 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/source_table.py
--rw-r--r--   0        0        0     5522 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/table.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/templates/__init__.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/templates/online_serving/__init__.py
--rw-r--r--   0        0        0      707 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/templates/online_serving/python.tpl
--rw-r--r--   0        0        0      132 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/templates/online_serving/shell.tpl
--rw-r--r--   0        0        0    60163 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/view.py
--rw-r--r--   0        0        0     8368 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/window_aggregator.py
--rw-r--r--   0        0        0     1607 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/api/window_validator.py
--rw-r--r--   0        0        0     6870 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/app.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/__init__.py
--rw-r--r--   0        0        0     3116 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/date_util.py
--rw-r--r--   0        0        0      956 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/descriptor.py
--rw-r--r--   0        0        0      636 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/dict_util.py
--rw-r--r--   0        0        0     2090 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/doc_util.py
--rw-r--r--   0        0        0      692 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/allowed_classes.py
--rw-r--r--   0        0        0    16036 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/autodoc_processor.py
--rw-r--r--   0        0        0     1419 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/constants.py
--rw-r--r--   0        0        0     4280 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/custom_nav.py
--rw-r--r--   0        0        0     8560 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/doc_types.py
--rw-r--r--   0        0        0    43070 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/documentation_layout.py
--rw-r--r--   0        0        0     7430 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/extract_csv.py
--rw-r--r--   0        0        0     3479 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/formatters.py
--rw-r--r--   0        0        0    31657 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/gen_ref_pages_docs_builder.py
--rw-r--r--   0        0        0      877 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/markdown_extension/extension.py
--rw-r--r--   0        0        0     5660 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/pydantic_field_docs.py
--rw-r--r--   0        0        0    16371 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/resource_extractor.py
--rw-r--r--   0        0        0      519 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/resource_util.py
--rw-r--r--   0        0        0      200 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/documentation/util.py
--rw-r--r--   0        0        0     1262 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/env_util.py
--rw-r--r--   0        0        0     6740 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/formatting_util.py
--rw-r--r--   0        0        0     3925 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/join_utils.py
--rw-r--r--   0        0        0     4198 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/model_util.py
--rw-r--r--   0        0        0      815 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/path_util.py
--rw-r--r--   0        0        0     1062 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/progress.py
--rw-r--r--   0        0        0      451 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/singleton.py
--rw-r--r--   0        0        0     1084 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/tile_util.py
--rw-r--r--   0        0        0     2629 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/typing.py
--rw-r--r--   0        0        0    10491 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/utils.py
--rw-r--r--   0        0        0     4699 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/common/validator.py
--rw-r--r--   0        0        0    13279 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/config.py
--rw-r--r--   0        0        0     1366 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/conftest.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/__init__.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/accessor/__init__.py
--rw-r--r--   0        0        0    15412 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/accessor/count_dict.py
--rw-r--r--   0        0        0    23735 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/accessor/datetime.py
--rw-r--r--   0        0        0    10051 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/accessor/feature_datetime.py
--rw-r--r--   0        0        0     8454 2023-06-08 15:53:14.873277 featurebyte-0.3.1/featurebyte/core/accessor/feature_string.py
--rw-r--r--   0        0        0    15765 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/accessor/string.py
--rw-r--r--   0        0        0     8902 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/frame.py
--rw-r--r--   0        0        0    11843 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/generic.py
--rw-r--r--   0        0        0    16742 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/mixin.py
--rw-r--r--   0        0        0    37956 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/series.py
--rw-r--r--   0        0        0     1484 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/timedelta.py
--rw-r--r--   0        0        0     6393 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/core/util.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/__init__.py
--rw-r--r--   0        0        0      697 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/__main__.py
--rw-r--r--   0        0        0     5546 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/app.py
--rw-r--r--   0        0        0     3125 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/creditcard.sql
--rw-r--r--   0        0        0     1154 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/doctest_grocery.sql
--rw-r--r--   0        0        0     2200 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/grocery.sql
--rw-r--r--   0        0        0     5757 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/datasets/healthcare.sql
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/docker/__init__.py
--rw-r--r--   0        0        0     4518 2023-06-08 15:53:50.449892 featurebyte-0.3.1/featurebyte/docker/featurebyte.yml
--rw-r--r--   0        0        0    10343 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/docker/manager.py
--rw-r--r--   0        0        0     9199 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/enum.py
--rw-r--r--   0        0        0     9244 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/exception.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/feature_manager/__init__.py
--rw-r--r--   0        0        0    10392 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/feature_manager/manager.py
--rw-r--r--   0        0        0     2911 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/feature_manager/model.py
--rw-r--r--   0        0        0     3997 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/feature_manager/sql_template.py
--rw-r--r--   0        0        0     3667 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/feature_utility.py
--rw-r--r--   0        0        0     4145 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/logging.py
--rw-r--r--   0        0        0     7839 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/middleware.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/__init__.py
--rw-r--r--   0        0        0      604 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/migration_data_service.py
--rw-r--r--   0        0        0     1664 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/model.py
--rw-r--r--   0        0        0     8423 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/run.py
--rw-r--r--   0        0        0     1442 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/service/__init__.py
--rw-r--r--   0        0        0     9762 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/service/data_warehouse.py
--rw-r--r--   0        0        0     9181 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/migration/service/mixin.py
--rw-r--r--   0        0        0      646 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/__init__.py
--rw-r--r--   0        0        0    10813 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/base.py
--rw-r--r--   0        0        0      617 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/batch_feature_table.py
--rw-r--r--   0        0        0     1726 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/batch_request_table.py
--rw-r--r--   0        0        0     2377 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/catalog.py
--rw-r--r--   0        0        0     1572 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/context.py
--rw-r--r--   0        0        0     8513 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/credential.py
--rw-r--r--   0        0        0     1333 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/deployment.py
--rw-r--r--   0        0        0     2026 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/dimension_table.py
--rw-r--r--   0        0        0     3618 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/entity.py
--rw-r--r--   0        0        0     3051 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/entity_validation.py
--rw-r--r--   0        0        0     3551 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/event_table.py
--rw-r--r--   0        0        0    11926 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/feature.py
--rw-r--r--   0        0        0     3750 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    21070 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/feature_list.py
--rw-r--r--   0        0        0     7606 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/feature_store.py
--rw-r--r--   0        0        0      644 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/historical_feature_table.py
--rw-r--r--   0        0        0     2919 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/item_table.py
--rw-r--r--   0        0        0     1620 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/materialized_table.py
--rw-r--r--   0        0        0     2113 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/observation_table.py
--rw-r--r--   0        0        0     3738 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/online_store.py
--rw-r--r--   0        0        0     1433 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/parent_serving.py
--rw-r--r--   0        0        0     2891 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/periodic_task.py
--rw-r--r--   0        0        0     1492 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/persistent.py
--rw-r--r--   0        0        0     1024 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/proxy_table.py
--rw-r--r--   0        0        0     4330 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/relationship.py
--rw-r--r--   0        0        0      944 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/relationship_analysis.py
--rw-r--r--   0        0        0     7865 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/request_input.py
--rw-r--r--   0        0        0     3554 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/scd_table.py
--rw-r--r--   0        0        0     1435 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/semantic.py
--rw-r--r--   0        0        0     1085 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/task.py
--rw-r--r--   0        0        0     3269 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/models/tile.py
--rw-r--r--   0        0        0      154 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/persistent/__init__.py
--rw-r--r--   0        0        0     9396 2023-06-08 15:53:14.877277 featurebyte-0.3.1/featurebyte/persistent/audit.py
--rw-r--r--   0        0        0    21712 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/persistent/base.py
--rw-r--r--   0        0        0     9824 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/persistent/mongo.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/__init__.py
--rw-r--r--   0        0        0     2533 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/algorithm.py
--rw-r--r--   0        0        0     2988 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/enum.py
--rw-r--r--   0        0        0    16847 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/graph.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/graph_node/__init__.py
--rw-r--r--   0        0        0     4592 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/graph_node/base.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/__init__.py
--rw-r--r--   0        0        0      873 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/column_info.py
--rw-r--r--   0        0        0    12700 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/common_table.py
--rw-r--r--   0        0        0     1707 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/critical_data_info.py
--rw-r--r--   0        0        0     6699 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/feature_job_setting.py
--rw-r--r--   0        0        0    19282 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/graph.py
--rw-r--r--   0        0        0    23872 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/model/table.py
--rw-r--r--   0        0        0     1076 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/__init__.py
--rw-r--r--   0        0        0     5236 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/agg_func.py
--rw-r--r--   0        0        0    28374 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/base.py
--rw-r--r--   0        0        0     5224 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/binary.py
--rw-r--r--   0        0        0    17953 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/cleaning_operation.py
--rw-r--r--   0        0        0     7797 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/count_dict.py
--rw-r--r--   0        0        0     7293 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/date.py
--rw-r--r--   0        0        0    60575 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/generic.py
--rw-r--r--   0        0        0    17049 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/input.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/metadata/__init__.py
--rw-r--r--   0        0        0      873 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/metadata/column.py
--rw-r--r--   0        0        0    21929 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/metadata/operation.py
--rw-r--r--   0        0        0    19397 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/metadata/sdk_code.py
--rw-r--r--   0        0        0       47 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/metadata/templates/sdk_code.tpl
--rw-r--r--   0        0        0     7393 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/mixin.py
--rw-r--r--   0        0        0    21084 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/nested.py
--rw-r--r--   0        0        0     3272 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/request.py
--rw-r--r--   0        0        0     2054 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/scalar.py
--rw-r--r--   0        0        0     6696 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/schema.py
--rw-r--r--   0        0        0     5461 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/string.py
--rw-r--r--   0        0        0     4824 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/unary.py
--rw-r--r--   0        0        0     1042 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/node/validator.py
--rw-r--r--   0        0        0     1050 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/pruning_util.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/__init__.py
--rw-r--r--   0        0        0    27339 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/adapter.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/__init__.py
--rw-r--r--   0        0        0     7198 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/asat.py
--rw-r--r--   0        0        0    16281 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/base.py
--rw-r--r--   0        0        0     5739 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/item.py
--rw-r--r--   0        0        0     3801 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/latest.py
--rw-r--r--   0        0        0     9549 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/lookup.py
--rw-r--r--   0        0        0     3811 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/request_table.py
--rw-r--r--   0        0        0    26795 2023-06-08 15:53:14.881277 featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/window.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/__init__.py
--rw-r--r--   0        0        0     5560 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/aggregate.py
--rw-r--r--   0        0        0    12647 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/base.py
--rw-r--r--   0        0        0     2723 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/binary.py
--rw-r--r--   0        0        0     5740 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/count_dict.py
--rw-r--r--   0        0        0    10724 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/datetime.py
--rw-r--r--   0        0        0     7163 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/generic.py
--rw-r--r--   0        0        0     2409 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/groupby.py
--rw-r--r--   0        0        0     2593 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/input.py
--rw-r--r--   0        0        0     1449 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/is_in.py
--rw-r--r--   0        0        0     6208 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/join.py
--rw-r--r--   0        0        0     6138 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/join_feature.py
--rw-r--r--   0        0        0     2291 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/literal.py
--rw-r--r--   0        0        0      878 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/request.py
--rw-r--r--   0        0        0     8360 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/string.py
--rw-r--r--   0        0        0    11741 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/tile.py
--rw-r--r--   0        0        0     5170 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/track_changes.py
--rw-r--r--   0        0        0     5157 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/unary.py
--rw-r--r--   0        0        0     2536 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/ast/util.py
--rw-r--r--   0        0        0     7070 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/builder.py
--rw-r--r--   0        0        0     4909 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/common.py
--rw-r--r--   0        0        0     1704 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/dataframe.py
--rw-r--r--   0        0        0     1966 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/expression.py
--rw-r--r--   0        0        0    20348 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/feature_compute.py
--rw-r--r--   0        0        0    28059 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/feature_historical.py
--rw-r--r--   0        0        0     3694 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/feature_preview.py
--rw-r--r--   0        0        0     4449 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/groupby_helper.py
--rw-r--r--   0        0        0      426 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/__init__.py
--rw-r--r--   0        0        0     3344 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/base.py
--rw-r--r--   0        0        0    27934 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/preview.py
--rw-r--r--   0        0        0     6685 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/tile.py
--rw-r--r--   0        0        0     4027 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/materialisation.py
--rw-r--r--   0        0        0    17988 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/online_serving.py
--rw-r--r--   0        0        0      659 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/online_serving_util.py
--rw-r--r--   0        0        0     5042 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/parent_serving.py
--rw-r--r--   0        0        0    15948 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/scd_helper.py
--rw-r--r--   0        0        0    20783 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/specs.py
--rw-r--r--   0        0        0     2206 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/template.py
--rw-r--r--   0        0        0    13029 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/tile_compute.py
--rw-r--r--   0        0        0     3793 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/tile_util.py
--rw-r--r--   0        0        0     9354 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/sql/tiling.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/__init__.py
--rw-r--r--   0        0        0     5184 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/base.py
--rw-r--r--   0        0        0     5201 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/flattening.py
--rw-r--r--   0        0        0     6716 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/operation_structure.py
--rw-r--r--   0        0        0    20327 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/pruning.py
--rw-r--r--   0        0        0    10248 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/reconstruction.py
--rw-r--r--   0        0        0    13262 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/transform/sdk_code.py
--rw-r--r--   0        0        0     5951 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/query_graph/util.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/routes/__init__.py
--rw-r--r--   0        0        0     4367 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/routes/app_container.py
--rw-r--r--   0        0        0     4340 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/routes/app_container_config.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/routes/batch_feature_table/__init__.py
--rw-r--r--   0        0        0     4832 2023-06-08 15:53:14.885278 featurebyte-0.3.1/featurebyte/routes/batch_feature_table/api.py
--rw-r--r--   0        0        0     4616 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/batch_feature_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/batch_request_table/__init__.py
--rw-r--r--   0        0        0     4860 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/batch_request_table/api.py
--rw-r--r--   0        0        0     3137 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/batch_request_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/catalog/__init__.py
--rw-r--r--   0        0        0     4307 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/catalog/api.py
--rw-r--r--   0        0        0     2524 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/catalog/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/__init__.py
--rw-r--r--   0        0        0    10579 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/base.py
--rw-r--r--   0        0        0     3761 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/base_materialized_table.py
--rw-r--r--   0        0        0     5515 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/base_table.py
--rw-r--r--   0        0        0      886 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/schema.py
--rw-r--r--   0        0        0      400 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/common/util.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/context/__init__.py
--rw-r--r--   0        0        0     3107 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/context/api.py
--rw-r--r--   0        0        0     1596 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/context/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/credential/__init__.py
--rw-r--r--   0        0        0     4605 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/credential/api.py
--rw-r--r--   0        0        0     3071 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/credential/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/deployment/__init__.py
--rw-r--r--   0        0        0     5604 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/deployment/api.py
--rw-r--r--   0        0        0    10416 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/deployment/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/dimension_table/__init__.py
--rw-r--r--   0        0        0     4181 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/dimension_table/api.py
--rw-r--r--   0        0        0     1674 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/dimension_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/entity/__init__.py
--rw-r--r--   0        0        0     5145 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/entity/api.py
--rw-r--r--   0        0        0     2718 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/entity/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/event_table/__init__.py
--rw-r--r--   0        0        0     4689 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/event_table/api.py
--rw-r--r--   0        0        0     1818 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/event_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature/__init__.py
--rw-r--r--   0        0        0     6378 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature/api.py
--rw-r--r--   0        0        0    15348 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_job_setting_analysis/__init__.py
--rw-r--r--   0        0        0     5449 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_job_setting_analysis/api.py
--rw-r--r--   0        0        0     4703 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_job_setting_analysis/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list/__init__.py
--rw-r--r--   0        0        0     7085 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list/api.py
--rw-r--r--   0        0        0    13578 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list_namespace/__init__.py
--rw-r--r--   0        0        0     4324 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list_namespace/api.py
--rw-r--r--   0        0        0     6989 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_list_namespace/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_namespace/__init__.py
--rw-r--r--   0        0        0     3960 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_namespace/api.py
--rw-r--r--   0        0        0     7126 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_namespace/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_store/__init__.py
--rw-r--r--   0        0        0     7999 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_store/api.py
--rw-r--r--   0        0        0    12891 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/feature_store/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/historical_feature_table/__init__.py
--rw-r--r--   0        0        0     5367 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/historical_feature_table/api.py
--rw-r--r--   0        0        0     5557 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/historical_feature_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/item_table/__init__.py
--rw-r--r--   0        0        0     3832 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/item_table/api.py
--rw-r--r--   0        0        0     1514 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/item_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/observation_table/__init__.py
--rw-r--r--   0        0        0     4768 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/observation_table/api.py
--rw-r--r--   0        0        0     3157 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/observation_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/periodic_tasks/__init__.py
--rw-r--r--   0        0        0     1583 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/periodic_tasks/api.py
--rw-r--r--   0        0        0      540 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/periodic_tasks/controller.py
--rw-r--r--   0        0        0    16128 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/registry.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/relationship_info/__init__.py
--rw-r--r--   0        0        0     4202 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/relationship_info/api.py
--rw-r--r--   0        0        0     4602 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/relationship_info/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/scd_table/__init__.py
--rw-r--r--   0        0        0     3768 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/scd_table/api.py
--rw-r--r--   0        0        0     1969 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/scd_table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/semantic/__init__.py
--rw-r--r--   0        0        0     3733 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/semantic/api.py
--rw-r--r--   0        0        0     1362 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/semantic/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/table/__init__.py
--rw-r--r--   0        0        0     1469 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/table/api.py
--rw-r--r--   0        0        0      464 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/table/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/task/__init__.py
--rw-r--r--   0        0        0     1045 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/task/api.py
--rw-r--r--   0        0        0     1837 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/task/controller.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/temp_data/__init__.py
--rw-r--r--   0        0        0      581 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/temp_data/api.py
--rw-r--r--   0        0        0     1353 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/routes/temp_data/controller.py
--rw-r--r--   0        0        0      180 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/__init__.py
--rw-r--r--   0        0        0     1431 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/batch_feature_table.py
--rw-r--r--   0        0        0      842 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/batch_request_table.py
--rw-r--r--   0        0        0     1511 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/catalog.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/common/__init__.py
--rw-r--r--   0        0        0     1068 2023-06-08 15:53:14.889278 featurebyte-0.3.1/featurebyte/schema/common/base.py
--rw-r--r--   0        0        0     3662 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/common/operation.py
--rw-r--r--   0        0        0     1508 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/context.py
--rw-r--r--   0        0        0     3658 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/credential.py
--rw-r--r--   0        0        0     1686 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/deployment.py
--rw-r--r--   0        0        0      981 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/dimension_table.py
--rw-r--r--   0        0        0     1684 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/entity.py
--rw-r--r--   0        0        0     2391 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/event_table.py
--rw-r--r--   0        0        0     7337 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature.py
--rw-r--r--   0        0        0     4664 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature_job_setting_analysis.py
--rw-r--r--   0        0        0     3758 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature_list.py
--rw-r--r--   0        0        0     1460 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature_list_namespace.py
--rw-r--r--   0        0        0     1965 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature_namespace.py
--rw-r--r--   0        0        0     3589 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/feature_store.py
--rw-r--r--   0        0        0     1591 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/historical_feature_table.py
--rw-r--r--   0        0        0    10583 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/info.py
--rw-r--r--   0        0        0      991 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/item_table.py
--rw-r--r--   0        0        0      599 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/materialized_table.py
--rw-r--r--   0        0        0      909 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/observation_table.py
--rw-r--r--   0        0        0      459 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/periodic_task.py
--rw-r--r--   0        0        0     1460 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/relationship_info.py
--rw-r--r--   0        0        0     1190 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/request_table.py
--rw-r--r--   0        0        0     1376 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/scd_table.py
--rw-r--r--   0        0        0      914 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/semantic.py
--rw-r--r--   0        0        0     2415 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/table.py
--rw-r--r--   0        0        0     1012 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/task.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/__init__.py
--rw-r--r--   0        0        0      333 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/progress.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/__init__.py
--rw-r--r--   0        0        0     2829 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/base.py
--rw-r--r--   0        0        0      520 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/batch_feature_create.py
--rw-r--r--   0        0        0      610 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/batch_feature_table.py
--rw-r--r--   0        0        0      602 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/batch_request_table.py
--rw-r--r--   0        0        0     1382 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/deployment_create_update.py
--rw-r--r--   0        0        0     1328 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/feature_job_setting_analysis.py
--rw-r--r--   0        0        0      725 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/historical_feature_table.py
--rw-r--r--   0        0        0     1215 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/materialized_table_delete.py
--rw-r--r--   0        0        0      589 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/observation_table.py
--rw-r--r--   0        0        0      500 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/test.py
--rw-r--r--   0        0        0      416 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/schema/worker/task/tile.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/__init__.py
--rw-r--r--   0        0        0    28027 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/base_document.py
--rw-r--r--   0        0        0      524 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/base_service.py
--rw-r--r--   0        0        0     5195 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/base_table_document.py
--rw-r--r--   0        0        0     1846 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/batch_feature_table.py
--rw-r--r--   0        0        0     2715 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/batch_request_table.py
--rw-r--r--   0        0        0     2303 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/catalog.py
--rw-r--r--   0        0        0     5515 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/context.py
--rw-r--r--   0        0        0     5558 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/credential.py
--rw-r--r--   0        0        0     4782 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/default_version_mode.py
--rw-r--r--   0        0        0    18162 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/deploy.py
--rw-r--r--   0        0        0      478 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/deployment.py
--rw-r--r--   0        0        0      674 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/dimension_table.py
--rw-r--r--   0        0        0     2199 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/entity.py
--rw-r--r--   0        0        0     7515 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/entity_validation.py
--rw-r--r--   0        0        0      618 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/event_table.py
--rw-r--r--   0        0        0    11435 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature.py
--rw-r--r--   0        0        0     3535 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    10881 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_list.py
--rw-r--r--   0        0        0      572 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_list_namespace.py
--rw-r--r--   0        0        0     4048 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_list_status.py
--rw-r--r--   0        0        0      564 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_namespace.py
--rw-r--r--   0        0        0    15409 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_readiness.py
--rw-r--r--   0        0        0      603 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_store.py
--rw-r--r--   0        0        0    15423 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/feature_store_warehouse.py
--rw-r--r--   0        0        0     2939 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/historical_feature_table.py
--rw-r--r--   0        0        0    42750 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/info.py
--rw-r--r--   0        0        0      604 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/item_table.py
--rw-r--r--   0        0        0     4788 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/materialized_table.py
--rw-r--r--   0        0        0     4396 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/mixin.py
--rw-r--r--   0        0        0     6202 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/observation_table.py
--rw-r--r--   0        0        0     9257 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/online_enable.py
--rw-r--r--   0        0        0     4504 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/online_serving.py
--rw-r--r--   0        0        0     7266 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/parent_serving.py
--rw-r--r--   0        0        0      437 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/periodic_task.py
--rw-r--r--   0        0        0    25370 2023-06-08 15:53:14.893278 featurebyte-0.3.1/featurebyte/service/preview.py
--rw-r--r--   0        0        0     9285 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/relationship.py
--rw-r--r--   0        0        0     2303 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/relationship_info.py
--rw-r--r--   0        0        0      590 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/scd_table.py
--rw-r--r--   0        0        0      600 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/semantic.py
--rw-r--r--   0        0        0     2703 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/session_manager.py
--rw-r--r--   0        0        0     7932 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/session_validator.py
--rw-r--r--   0        0        0     1051 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/table.py
--rw-r--r--   0        0        0    16758 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/table_columns_info.py
--rw-r--r--   0        0        0     2241 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/table_status.py
--rw-r--r--   0        0        0    10862 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/task_manager.py
--rw-r--r--   0        0        0      779 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/user_service.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/validator/__init__.py
--rw-r--r--   0        0        0     3704 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/validator/materialized_table_delete.py
--rw-r--r--   0        0        0    11567 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/validator/production_ready_validator.py
--rw-r--r--   0        0        0    15798 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/version.py
--rw-r--r--   0        0        0    14878 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/view_construction.py
--rw-r--r--   0        0        0     4180 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/service/working_schema.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/__init__.py
--rw-r--r--   0        0        0    26478 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/base.py
--rw-r--r--   0        0        0    14634 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/base_spark.py
--rw-r--r--   0        0        0     5643 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/databricks.py
--rw-r--r--   0        0        0      434 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/enum.py
--rw-r--r--   0        0        0     4993 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/hive.py
--rw-r--r--   0        0        0     5107 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/manager.py
--rw-r--r--   0        0        0     7592 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/simple_storage.py
--rw-r--r--   0        0        0    14756 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/snowflake.py
--rw-r--r--   0        0        0     8393 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/spark.py
--rw-r--r--   0        0        0     3479 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/session/sqlite.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/__init__.py
--rw-r--r--   0        0        0     3198 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/base.py
--rw-r--r--   0        0        0     2743 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/common.py
--rw-r--r--   0        0        0        6 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/databricks/.gitignore
--rw-r--r--   0        0        0      957 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql
--rw-r--r--   0        0        0      476 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql
--rw-r--r--   0        0        0      171 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT.sql
--rw-r--r--   0        0        0      610 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
--rw-r--r--   0        0        0      177 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
--rw-r--r--   0        0        0      188 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_NUM_UNIQUE.sql
--rw-r--r--   0        0        0     1491 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_GET_RANK.sql
--rw-r--r--   0        0        0      397 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_GET_RELATIVE_FREQUENCY.sql
--rw-r--r--   0        0        0      559 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql
--rw-r--r--   0        0        0      559 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql
--rw-r--r--   0        0        0      653 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql
--rw-r--r--   0        0        0      292 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/T_ONLINE_STORE_MAPPING.sql
--rw-r--r--   0        0        0      299 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/T_TILE_FEATURE_MAPPING.sql
--rw-r--r--   0        0        0      212 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/T_TILE_JOB_MONITOR.sql
--rw-r--r--   0        0        0      153 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/T_TILE_MONITOR_SUMMARY.sql
--rw-r--r--   0        0        0      526 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/snowflake/T_TILE_REGISTRY.sql
--rw-r--r--   0        0        0        6 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/.gitignore
--rw-r--r--   0        0        0      304 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/T_ONLINE_STORE_MAPPING.sql
--rw-r--r--   0        0        0      311 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/T_TILE_FEATURE_MAPPING.sql
--rw-r--r--   0        0        0      228 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/T_TILE_JOB_MONITOR.sql
--rw-r--r--   0        0        0      191 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/T_TILE_MONITOR_SUMMARY.sql
--rw-r--r--   0        0        0      531 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/T_TILE_REGISTRY.sql
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/spark/__init__.py
--rw-r--r--   0        0        0    29690 2023-06-08 15:55:07.262925 featurebyte-0.3.1/featurebyte/sql/spark/featurebyte-hive-udf-1.0.3-SNAPSHOT-all.jar
--rw-r--r--   0        0        0     2001 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_common.py
--rw-r--r--   0        0        0     5878 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_generate.py
--rw-r--r--   0        0        0     3566 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_generate_entity_tracking.py
--rw-r--r--   0        0        0     9817 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_generate_schedule.py
--rw-r--r--   0        0        0     7297 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_monitor.py
--rw-r--r--   0        0        0     3491 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_registry.py
--rw-r--r--   0        0        0     6139 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/sql/tile_schedule_online_store.py
--rw-r--r--   0        0        0      239 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/storage/__init__.py
--rw-r--r--   0        0        0     4999 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/storage/base.py
--rw-r--r--   0        0        0     3640 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/storage/local.py
--rw-r--r--   0        0        0      415 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/storage/local_temp.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/tile/__init__.py
--rw-r--r--   0        0        0    14347 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/tile/manager.py
--rw-r--r--   0        0        0     2918 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/tile/scheduler.py
--rw-r--r--   0        0        0      411 2023-06-08 15:53:14.897278 featurebyte-0.3.1/featurebyte/tile/sql_template.py
--rw-r--r--   0        0        0    28505 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/tile/tile_cache.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/__init__.py
--rw-r--r--   0        0        0     1909 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/credential.py
--rw-r--r--   0        0        0     1532 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/messaging.py
--rw-r--r--   0        0        0      534 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/persistent.py
--rw-r--r--   0        0        0        0 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/snowflake/__init__.py
--rw-r--r--   0        0        0      462 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/snowflake/sql.py
--rw-r--r--   0        0        0      765 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/utils/storage.py
--rw-r--r--   0        0        0     1537 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/__init__.py
--rw-r--r--   0        0        0      170 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/enum.py
--rw-r--r--   0        0        0     3873 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/process_store.py
--rw-r--r--   0        0        0     1159 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/progress.py
--rw-r--r--   0        0        0     2502 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/schedulers.py
--rw-r--r--   0        0        0      107 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/start.py
--rw-r--r--   0        0        0      214 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/__init__.py
--rw-r--r--   0        0        0     2900 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/base.py
--rw-r--r--   0        0        0     4603 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/batch_feature_create.py
--rw-r--r--   0        0        0     3718 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/batch_feature_table.py
--rw-r--r--   0        0        0     2243 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/batch_request_table.py
--rw-r--r--   0        0        0     1795 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/deployment_create_update.py
--rw-r--r--   0        0        0     7130 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/feature_job_setting_analysis.py
--rw-r--r--   0        0        0     3954 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/historical_feature_table.py
--rw-r--r--   0        0        0     4321 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/materialized_table_delete.py
--rw-r--r--   0        0        0     2604 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/mixin.py
--rw-r--r--   0        0        0     2226 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/observation_table.py
--rw-r--r--   0        0        0      626 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/test_task.py
--rw-r--r--   0        0        0     1904 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task/tile_task.py
--rw-r--r--   0        0        0     5407 2023-06-08 15:53:14.901278 featurebyte-0.3.1/featurebyte/worker/task_executor.py
--rw-r--r--   0        0        0     7717 2023-06-08 15:53:47.181841 featurebyte-0.3.1/pyproject.toml
--rw-r--r--   0        0        0    23050 1970-01-01 00:00:00.000000 featurebyte-0.3.1/PKG-INFO
+-rw-r--r--   0        0        0     3860 2023-07-25 03:10:46.664321 featurebyte-0.4.0/LICENSE
+-rw-r--r--   0        0        0    19816 2023-07-25 03:10:46.664321 featurebyte-0.4.0/README.md
+-rw-r--r--   0        0        0    17168 2023-07-25 03:10:46.664321 featurebyte-0.4.0/featurebyte/__init__.py
+-rw-r--r--   0        0        0     2552 2023-07-25 03:10:46.664321 featurebyte-0.4.0/featurebyte/__main__.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.664321 featurebyte-0.4.0/featurebyte/api/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.664321 featurebyte-0.4.0/featurebyte/api/aggregator/__init__.py
+-rw-r--r--   0        0        0     6216 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/aggregator/base_aggregator.py
+-rw-r--r--   0        0        0     5063 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/aggregator/forward_aggregator.py
+-rw-r--r--   0        0        0     5348 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/base.py
+-rw-r--r--   0        0        0      497 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/catalog.py
+-rw-r--r--   0        0        0      531 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/feature.py
+-rw-r--r--   0        0        0      744 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0      762 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/feature_list.py
+-rw-r--r--   0        0        0      635 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/feature_namespace.py
+-rw-r--r--   0        0        0      430 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/target_namespace.py
+-rw-r--r--   0        0        0      469 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_handler/user_defined_function.py
+-rw-r--r--   0        0        0    18841 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_object.py
+-rw-r--r--   0        0        0     9984 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/api_object_util.py
+-rw-r--r--   0        0        0     4996 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/asat_aggregator.py
+-rw-r--r--   0        0        0    37664 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/base_table.py
+-rw-r--r--   0        0        0     4833 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/batch_feature_table.py
+-rw-r--r--   0        0        0     5062 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/batch_request_table.py
+-rw-r--r--   0        0        0    40434 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/catalog.py
+-rw-r--r--   0        0        0     1475 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/catalog_decorator.py
+-rw-r--r--   0        0        0    13551 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/catalog_get_by_id_mixin.py
+-rw-r--r--   0        0        0    11517 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/change_view.py
+-rw-r--r--   0        0        0     6613 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/credential.py
+-rw-r--r--   0        0        0     7615 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/data_source.py
+-rw-r--r--   0        0        0    13859 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/deployment.py
+-rw-r--r--   0        0        0     9699 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/dimension_table.py
+-rw-r--r--   0        0        0     3107 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/dimension_view.py
+-rw-r--r--   0        0        0    10127 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/entity.py
+-rw-r--r--   0        0        0    22561 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/event_table.py
+-rw-r--r--   0        0        0    15975 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/event_view.py
+-rw-r--r--   0        0        0    47504 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature.py
+-rw-r--r--   0        0        0    20227 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_group.py
+-rw-r--r--   0        0        0    15001 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_job.py
+-rw-r--r--   0        0        0     8703 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    61414 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_list.py
+-rw-r--r--   0        0        0     3765 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_namespace.py
+-rw-r--r--   0        0        0     4220 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_or_target_mixin.py
+-rw-r--r--   0        0        0     1217 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_or_target_namespace_mixin.py
+-rw-r--r--   0        0        0    10024 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_store.py
+-rw-r--r--   0        0        0     2004 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_util.py
+-rw-r--r--   0        0        0      557 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/feature_validation_util.py
+-rw-r--r--   0        0        0    20211 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/groupby.py
+-rw-r--r--   0        0        0     4970 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/historical_feature_table.py
+-rw-r--r--   0        0        0    16661 2023-07-25 03:10:46.668321 featurebyte-0.4.0/featurebyte/api/item_table.py
+-rw-r--r--   0        0        0    11917 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/item_view.py
+-rw-r--r--   0        0        0     2909 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/lag.py
+-rw-r--r--   0        0        0     4909 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/materialized_table.py
+-rw-r--r--   0        0        0     6301 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/mixin.py
+-rw-r--r--   0        0        0     4995 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/observation_table.py
+-rw-r--r--   0        0        0      721 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/periodic_task.py
+-rw-r--r--   0        0        0     8485 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/relationship.py
+-rw-r--r--   0        0        0     3474 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/request_column.py
+-rw-r--r--   0        0        0     4464 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/savable_api_object.py
+-rw-r--r--   0        0        0    21162 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/scd_table.py
+-rw-r--r--   0        0        0     6023 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/scd_view.py
+-rw-r--r--   0        0        0     3130 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/simple_aggregator.py
+-rw-r--r--   0        0        0    46865 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/source_table.py
+-rw-r--r--   0        0        0     5039 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/static_source_table.py
+-rw-r--r--   0        0        0     5522 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/table.py
+-rw-r--r--   0        0        0    13682 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/target.py
+-rw-r--r--   0        0        0     3913 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/target_namespace.py
+-rw-r--r--   0        0        0     4517 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/target_table.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/__init__.py
+-rw-r--r--   0        0        0     2054 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/doc_util.py
+-rw-r--r--   0        0        0     2611 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/feature_or_target_doc.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/online_serving/__init__.py
+-rw-r--r--   0        0        0      707 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/online_serving/python.tpl
+-rw-r--r--   0        0        0      132 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/online_serving/shell.tpl
+-rw-r--r--   0        0        0      427 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/templates/series_doc.py
+-rw-r--r--   0        0        0    18615 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/user_defined_function.py
+-rw-r--r--   0        0        0    13245 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/user_defined_function_injector.py
+-rw-r--r--   0        0        0    62136 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/view.py
+-rw-r--r--   0        0        0     8379 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/window_aggregator.py
+-rw-r--r--   0        0        0     1607 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/api/window_validator.py
+-rw-r--r--   0        0        0     7924 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/app.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/__init__.py
+-rw-r--r--   0        0        0     3116 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/date_util.py
+-rw-r--r--   0        0        0      956 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/descriptor.py
+-rw-r--r--   0        0        0      636 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/dict_util.py
+-rw-r--r--   0        0        0     2090 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/doc_util.py
+-rw-r--r--   0        0        0      692 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/allowed_classes.py
+-rw-r--r--   0        0        0    16036 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/autodoc_processor.py
+-rw-r--r--   0        0        0     1520 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/constants.py
+-rw-r--r--   0        0        0     4445 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/custom_nav.py
+-rw-r--r--   0        0        0     8560 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/doc_types.py
+-rw-r--r--   0        0        0    42329 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/documentation_layout.py
+-rw-r--r--   0        0        0     7430 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/extract_csv.py
+-rw-r--r--   0        0        0     3479 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/formatters.py
+-rw-r--r--   0        0        0    32026 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/gen_ref_pages_docs_builder.py
+-rw-r--r--   0        0        0      877 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/markdown_extension/extension.py
+-rw-r--r--   0        0        0     6198 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/pydantic_field_docs.py
+-rw-r--r--   0        0        0    16371 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/resource_extractor.py
+-rw-r--r--   0        0        0      519 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/resource_util.py
+-rw-r--r--   0        0        0      200 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/documentation/util.py
+-rw-r--r--   0        0        0     1246 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/env_util.py
+-rw-r--r--   0        0        0     6740 2023-07-25 03:10:46.672322 featurebyte-0.4.0/featurebyte/common/formatting_util.py
+-rw-r--r--   0        0        0     3925 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/join_utils.py
+-rw-r--r--   0        0        0     4198 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/model_util.py
+-rw-r--r--   0        0        0      815 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/path_util.py
+-rw-r--r--   0        0        0     1062 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/progress.py
+-rw-r--r--   0        0        0      451 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/singleton.py
+-rw-r--r--   0        0        0     2666 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/typing.py
+-rw-r--r--   0        0        0    10491 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/utils.py
+-rw-r--r--   0        0        0     5180 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/common/validator.py
+-rw-r--r--   0        0        0    13483 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/config.py
+-rw-r--r--   0        0        0     1366 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/conftest.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/__init__.py
+-rw-r--r--   0        0        0    15533 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/count_dict.py
+-rw-r--r--   0        0        0    23735 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/datetime.py
+-rw-r--r--   0        0        0    10075 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/feature_datetime.py
+-rw-r--r--   0        0        0     8510 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/feature_string.py
+-rw-r--r--   0        0        0    15765 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/string.py
+-rw-r--r--   0        0        0     9213 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/target_datetime.py
+-rw-r--r--   0        0        0     8168 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/accessor/target_string.py
+-rw-r--r--   0        0        0     8902 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/frame.py
+-rw-r--r--   0        0        0    11818 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/generic.py
+-rw-r--r--   0        0        0    16742 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/mixin.py
+-rw-r--r--   0        0        0    38755 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/series.py
+-rw-r--r--   0        0        0     1484 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/timedelta.py
+-rw-r--r--   0        0        0     6393 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/core/util.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/__init__.py
+-rw-r--r--   0        0        0      697 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/__main__.py
+-rw-r--r--   0        0        0     5546 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/app.py
+-rw-r--r--   0        0        0     3125 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/creditcard.sql
+-rw-r--r--   0        0        0     1154 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/doctest_grocery.sql
+-rw-r--r--   0        0        0     2200 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/grocery.sql
+-rw-r--r--   0        0        0     5757 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/datasets/healthcare.sql
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/docker/__init__.py
+-rw-r--r--   0        0        0     4650 2023-07-25 03:11:30.063633 featurebyte-0.4.0/featurebyte/docker/featurebyte.yml
+-rw-r--r--   0        0        0    11444 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/docker/manager.py
+-rw-r--r--   0        0        0    10597 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/enum.py
+-rw-r--r--   0        0        0     9387 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/exception.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/feature_manager/__init__.py
+-rw-r--r--   0        0        0     2065 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/feature_manager/model.py
+-rw-r--r--   0        0        0      785 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/feature_manager/sql_template.py
+-rw-r--r--   0        0        0     3667 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/feature_utility.py
+-rw-r--r--   0        0        0     4145 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/logging.py
+-rw-r--r--   0        0        0     8053 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/middleware.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/__init__.py
+-rw-r--r--   0        0        0      604 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/migration_data_service.py
+-rw-r--r--   0        0        0     1703 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/model.py
+-rw-r--r--   0        0        0     9020 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/run.py
+-rw-r--r--   0        0        0     1442 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/service/__init__.py
+-rw-r--r--   0        0        0    10260 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/service/data_warehouse.py
+-rw-r--r--   0        0        0     7689 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/migration/service/mixin.py
+-rw-r--r--   0        0        0      694 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/__init__.py
+-rw-r--r--   0        0        0    11865 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/base.py
+-rw-r--r--   0        0        0      392 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/base_feature_or_target_table.py
+-rw-r--r--   0        0        0      617 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/batch_feature_table.py
+-rw-r--r--   0        0        0     1726 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/batch_request_table.py
+-rw-r--r--   0        0        0     2377 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/catalog.py
+-rw-r--r--   0        0        0     1611 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/context.py
+-rw-r--r--   0        0        0    10169 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/credential.py
+-rw-r--r--   0        0        0     1333 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/deployment.py
+-rw-r--r--   0        0        0     2026 2023-07-25 03:10:46.676322 featurebyte-0.4.0/featurebyte/models/dimension_table.py
+-rw-r--r--   0        0        0     3618 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/entity.py
+-rw-r--r--   0        0        0     3051 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/entity_validation.py
+-rw-r--r--   0        0        0     3551 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/event_table.py
+-rw-r--r--   0        0        0    11876 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/feature.py
+-rw-r--r--   0        0        0     3750 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    22006 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/feature_list.py
+-rw-r--r--   0        0        0     4704 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/feature_namespace.py
+-rw-r--r--   0        0        0     7622 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/feature_store.py
+-rw-r--r--   0        0        0      659 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/historical_feature_table.py
+-rw-r--r--   0        0        0     2919 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/item_table.py
+-rw-r--r--   0        0        0     1620 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/materialized_table.py
+-rw-r--r--   0        0        0     2113 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/observation_table.py
+-rw-r--r--   0        0        0     3861 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/online_store.py
+-rw-r--r--   0        0        0     1845 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/online_store_compute_query.py
+-rw-r--r--   0        0        0     1295 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/online_store_table_version.py
+-rw-r--r--   0        0        0     1433 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/parent_serving.py
+-rw-r--r--   0        0        0     2891 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/periodic_task.py
+-rw-r--r--   0        0        0     1492 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/persistent.py
+-rw-r--r--   0        0        0     1024 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/proxy_table.py
+-rw-r--r--   0        0        0     4330 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/relationship.py
+-rw-r--r--   0        0        0      944 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/relationship_analysis.py
+-rw-r--r--   0        0        0     8647 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/request_input.py
+-rw-r--r--   0        0        0     3554 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/scd_table.py
+-rw-r--r--   0        0        0     1435 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/semantic.py
+-rw-r--r--   0        0        0     1428 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/static_source_table.py
+-rw-r--r--   0        0        0     2834 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/target.py
+-rw-r--r--   0        0        0     2107 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/target_namespace.py
+-rw-r--r--   0        0        0      595 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/target_table.py
+-rw-r--r--   0        0        0     1085 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/task.py
+-rw-r--r--   0        0        0     4041 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/tile.py
+-rw-r--r--   0        0        0     1259 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/tile_job_log.py
+-rw-r--r--   0        0        0     2968 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/tile_registry.py
+-rw-r--r--   0        0        0    10462 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/models/user_defined_function.py
+-rw-r--r--   0        0        0      154 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/persistent/__init__.py
+-rw-r--r--   0        0        0     9544 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/persistent/audit.py
+-rw-r--r--   0        0        0    22972 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/persistent/base.py
+-rw-r--r--   0        0        0     9824 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/persistent/mongo.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/__init__.py
+-rw-r--r--   0        0        0     2533 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/algorithm.py
+-rw-r--r--   0        0        0     3103 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/enum.py
+-rw-r--r--   0        0        0    19691 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/graph.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/graph_node/__init__.py
+-rw-r--r--   0        0        0     4592 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/graph_node/base.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/__init__.py
+-rw-r--r--   0        0        0      873 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/column_info.py
+-rw-r--r--   0        0        0    12700 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/common_table.py
+-rw-r--r--   0        0        0     1707 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/critical_data_info.py
+-rw-r--r--   0        0        0     6699 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/feature_job_setting.py
+-rw-r--r--   0        0        0    19287 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/graph.py
+-rw-r--r--   0        0        0    23872 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/model/table.py
+-rw-r--r--   0        0        0     1076 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/__init__.py
+-rw-r--r--   0        0        0     5236 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/agg_func.py
+-rw-r--r--   0        0        0    28314 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/base.py
+-rw-r--r--   0        0        0     5172 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/binary.py
+-rw-r--r--   0        0        0    17953 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/cleaning_operation.py
+-rw-r--r--   0        0        0     7797 2023-07-25 03:10:46.680322 featurebyte-0.4.0/featurebyte/query_graph/node/count_dict.py
+-rw-r--r--   0        0        0     7293 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/date.py
+-rw-r--r--   0        0        0    10182 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/function.py
+-rw-r--r--   0        0        0    64270 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/generic.py
+-rw-r--r--   0        0        0    17083 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/input.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/metadata/__init__.py
+-rw-r--r--   0        0        0      873 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/metadata/column.py
+-rw-r--r--   0        0        0    22891 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/metadata/operation.py
+-rw-r--r--   0        0        0    20687 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/metadata/sdk_code.py
+-rw-r--r--   0        0        0       47 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/metadata/templates/sdk_code.tpl
+-rw-r--r--   0        0        0     7503 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/mixin.py
+-rw-r--r--   0        0        0    21084 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/nested.py
+-rw-r--r--   0        0        0     3272 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/request.py
+-rw-r--r--   0        0        0     2054 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/scalar.py
+-rw-r--r--   0        0        0     6696 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/schema.py
+-rw-r--r--   0        0        0     5461 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/string.py
+-rw-r--r--   0        0        0     4824 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/unary.py
+-rw-r--r--   0        0        0     1042 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/node/validator.py
+-rw-r--r--   0        0        0     1050 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/pruning_util.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/__init__.py
+-rw-r--r--   0        0        0     1060 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/adapter/__init__.py
+-rw-r--r--   0        0        0    18300 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/adapter/base.py
+-rw-r--r--   0        0        0     7716 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/adapter/databricks.py
+-rw-r--r--   0        0        0    10999 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/adapter/snowflake.py
+-rw-r--r--   0        0        0     1738 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/adapter/spark.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/__init__.py
+-rw-r--r--   0        0        0     7198 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/asat.py
+-rw-r--r--   0        0        0    22320 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/base.py
+-rw-r--r--   0        0        0     6446 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/forward.py
+-rw-r--r--   0        0        0     5739 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/item.py
+-rw-r--r--   0        0        0     3801 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/latest.py
+-rw-r--r--   0        0        0     9549 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/lookup.py
+-rw-r--r--   0        0        0     3811 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/request_table.py
+-rw-r--r--   0        0        0    26795 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/window.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/__init__.py
+-rw-r--r--   0        0        0     6184 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/aggregate.py
+-rw-r--r--   0        0        0    12647 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/base.py
+-rw-r--r--   0        0        0     2723 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/binary.py
+-rw-r--r--   0        0        0     5740 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/count_dict.py
+-rw-r--r--   0        0        0    10402 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/datetime.py
+-rw-r--r--   0        0        0     1907 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/function.py
+-rw-r--r--   0        0        0     7163 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/generic.py
+-rw-r--r--   0        0        0     2409 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/groupby.py
+-rw-r--r--   0        0        0     2593 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/input.py
+-rw-r--r--   0        0        0     1449 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/is_in.py
+-rw-r--r--   0        0        0     6208 2023-07-25 03:10:46.684322 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/join.py
+-rw-r--r--   0        0        0     6138 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/join_feature.py
+-rw-r--r--   0        0        0     2291 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/literal.py
+-rw-r--r--   0        0        0      878 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/request.py
+-rw-r--r--   0        0        0     8251 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/string.py
+-rw-r--r--   0        0        0    11756 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/tile.py
+-rw-r--r--   0        0        0     5170 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/track_changes.py
+-rw-r--r--   0        0        0     5157 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/unary.py
+-rw-r--r--   0        0        0     2536 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/ast/util.py
+-rw-r--r--   0        0        0     7070 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/builder.py
+-rw-r--r--   0        0        0     4909 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/common.py
+-rw-r--r--   0        0        0     1704 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/dataframe.py
+-rw-r--r--   0        0        0     1865 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/expression.py
+-rw-r--r--   0        0        0    21590 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/feature_compute.py
+-rw-r--r--   0        0        0    20358 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/feature_historical.py
+-rw-r--r--   0        0        0     4642 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/feature_preview.py
+-rw-r--r--   0        0        0     4449 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/groupby_helper.py
+-rw-r--r--   0        0        0      426 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/__init__.py
+-rw-r--r--   0        0        0     3512 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/base.py
+-rw-r--r--   0        0        0    27742 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/preview.py
+-rw-r--r--   0        0        0     6685 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/tile.py
+-rw-r--r--   0        0        0     4027 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/materialisation.py
+-rw-r--r--   0        0        0     9430 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/online_serving.py
+-rw-r--r--   0        0        0     1305 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/online_serving_util.py
+-rw-r--r--   0        0        0    10730 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/online_store_compute_query.py
+-rw-r--r--   0        0        0     5771 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/parent_serving.py
+-rw-r--r--   0        0        0    15948 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/scd_helper.py
+-rw-r--r--   0        0        0    24706 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/specs.py
+-rw-r--r--   0        0        0     2206 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/template.py
+-rw-r--r--   0        0        0    12508 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/tile_compute.py
+-rw-r--r--   0        0        0     8103 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/tile_util.py
+-rw-r--r--   0        0        0     9354 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/sql/tiling.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/__init__.py
+-rw-r--r--   0        0        0     5184 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/base.py
+-rw-r--r--   0        0        0     2482 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/entity_extractor.py
+-rw-r--r--   0        0        0     4925 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/flattening.py
+-rw-r--r--   0        0        0     6715 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/operation_structure.py
+-rw-r--r--   0        0        0    19339 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/pruning.py
+-rw-r--r--   0        0        0     3225 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/quick_pruning.py
+-rw-r--r--   0        0        0    10147 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/reconstruction.py
+-rw-r--r--   0        0        0    13262 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/transform/sdk_code.py
+-rw-r--r--   0        0        0     5951 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/query_graph/util.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/__init__.py
+-rw-r--r--   0        0        0     9852 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/app_container_config.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_feature_table/__init__.py
+-rw-r--r--   0        0        0     5506 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_feature_table/api.py
+-rw-r--r--   0        0        0     5197 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_feature_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_request_table/__init__.py
+-rw-r--r--   0        0        0     5534 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_request_table/api.py
+-rw-r--r--   0        0        0     3753 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/batch_request_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/catalog/__init__.py
+-rw-r--r--   0        0        0     4855 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/catalog/api.py
+-rw-r--r--   0        0        0     1374 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/catalog/catalog_name_injector.py
+-rw-r--r--   0        0        0     2429 2023-07-25 03:10:46.688323 featurebyte-0.4.0/featurebyte/routes/catalog/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/__init__.py
+-rw-r--r--   0        0        0    11843 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/base.py
+-rw-r--r--   0        0        0     4141 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/base_materialized_table.py
+-rw-r--r--   0        0        0     5390 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/base_table.py
+-rw-r--r--   0        0        0     4900 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/feature_metadata_extractor.py
+-rw-r--r--   0        0        0     8746 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/feature_or_target_table.py
+-rw-r--r--   0        0        0      886 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/schema.py
+-rw-r--r--   0        0        0      400 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/common/util.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/context/__init__.py
+-rw-r--r--   0        0        0     3655 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/context/api.py
+-rw-r--r--   0        0        0     1596 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/context/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/credential/__init__.py
+-rw-r--r--   0        0        0     5141 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/credential/api.py
+-rw-r--r--   0        0        0     3766 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/credential/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/deployment/__init__.py
+-rw-r--r--   0        0        0     6193 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/deployment/api.py
+-rw-r--r--   0        0        0    11712 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/deployment/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/dimension_table/__init__.py
+-rw-r--r--   0        0        0     4814 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/dimension_table/api.py
+-rw-r--r--   0        0        0     2670 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/dimension_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/entity/__init__.py
+-rw-r--r--   0        0        0     4769 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/entity/api.py
+-rw-r--r--   0        0        0     2949 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/entity/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/event_table/__init__.py
+-rw-r--r--   0        0        0     5278 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/event_table/api.py
+-rw-r--r--   0        0        0     2901 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/event_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature/__init__.py
+-rw-r--r--   0        0        0     6905 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature/api.py
+-rw-r--r--   0        0        0    22334 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_job_setting_analysis/__init__.py
+-rw-r--r--   0        0        0     6253 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_job_setting_analysis/api.py
+-rw-r--r--   0        0        0     6016 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_job_setting_analysis/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list/__init__.py
+-rw-r--r--   0        0        0     8251 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list/api.py
+-rw-r--r--   0        0        0    16099 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list_namespace/__init__.py
+-rw-r--r--   0        0        0     5061 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list_namespace/api.py
+-rw-r--r--   0        0        0     7212 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_list_namespace/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_namespace/__init__.py
+-rw-r--r--   0        0        0     4639 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_namespace/api.py
+-rw-r--r--   0        0        0    10655 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_namespace/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_store/__init__.py
+-rw-r--r--   0        0        0     8551 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_store/api.py
+-rw-r--r--   0        0        0    12839 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/feature_store/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/historical_feature_table/__init__.py
+-rw-r--r--   0        0        0     6028 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/historical_feature_table/api.py
+-rw-r--r--   0        0        0     3963 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/historical_feature_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/item_table/__init__.py
+-rw-r--r--   0        0        0     4410 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/item_table/api.py
+-rw-r--r--   0        0        0     2833 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/item_table/controller.py
+-rw-r--r--   0        0        0     5888 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/lazy_app_container.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/observation_table/__init__.py
+-rw-r--r--   0        0        0     5423 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/observation_table/api.py
+-rw-r--r--   0        0        0     3589 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/observation_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/periodic_tasks/__init__.py
+-rw-r--r--   0        0        0     2179 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/periodic_tasks/api.py
+-rw-r--r--   0        0        0      540 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/periodic_tasks/controller.py
+-rw-r--r--   0        0        0    16659 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/registry.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/relationship_info/__init__.py
+-rw-r--r--   0        0        0     4348 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/relationship_info/api.py
+-rw-r--r--   0        0        0     5644 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/relationship_info/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/scd_table/__init__.py
+-rw-r--r--   0        0        0     4335 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/scd_table/api.py
+-rw-r--r--   0        0        0     3146 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/scd_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/semantic/__init__.py
+-rw-r--r--   0        0        0     4292 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/semantic/api.py
+-rw-r--r--   0        0        0     1380 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/semantic/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/static_source_table/__init__.py
+-rw-r--r--   0        0        0     5536 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/static_source_table/api.py
+-rw-r--r--   0        0        0     3664 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/static_source_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/table/__init__.py
+-rw-r--r--   0        0        0     1469 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/table/api.py
+-rw-r--r--   0        0        0      464 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/target/__init__.py
+-rw-r--r--   0        0        0     4554 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/target/api.py
+-rw-r--r--   0        0        0     6373 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/target/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.692323 featurebyte-0.4.0/featurebyte/routes/target_namespace/__init__.py
+-rw-r--r--   0        0        0     5024 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/target_namespace/api.py
+-rw-r--r--   0        0        0     1825 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/target_namespace/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/target_table/__init__.py
+-rw-r--r--   0        0        0     5186 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/target_table/api.py
+-rw-r--r--   0        0        0     3142 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/target_table/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/task/__init__.py
+-rw-r--r--   0        0        0     1045 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/task/api.py
+-rw-r--r--   0        0        0     1892 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/task/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/temp_data/__init__.py
+-rw-r--r--   0        0        0      582 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/temp_data/api.py
+-rw-r--r--   0        0        0     1353 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/temp_data/controller.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/user_defined_function/__init__.py
+-rw-r--r--   0        0        0     5845 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/user_defined_function/api.py
+-rw-r--r--   0        0        0    10715 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/routes/user_defined_function/controller.py
+-rw-r--r--   0        0        0      180 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/__init__.py
+-rw-r--r--   0        0        0     1431 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/batch_feature_table.py
+-rw-r--r--   0        0        0      842 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/batch_request_table.py
+-rw-r--r--   0        0        0     1517 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/catalog.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/common/__init__.py
+-rw-r--r--   0        0        0     1226 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/common/base.py
+-rw-r--r--   0        0        0      643 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/common/feature_or_target.py
+-rw-r--r--   0        0        0     3662 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/common/operation.py
+-rw-r--r--   0        0        0     1508 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/context.py
+-rw-r--r--   0        0        0     3658 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/credential.py
+-rw-r--r--   0        0        0     1686 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/deployment.py
+-rw-r--r--   0        0        0      981 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/dimension_table.py
+-rw-r--r--   0        0        0     1684 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/entity.py
+-rw-r--r--   0        0        0     2391 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/event_table.py
+-rw-r--r--   0        0        0     6127 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature.py
+-rw-r--r--   0        0        0     4664 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0     4836 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature_list.py
+-rw-r--r--   0        0        0     1470 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature_list_namespace.py
+-rw-r--r--   0        0        0     2062 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature_namespace.py
+-rw-r--r--   0        0        0     3589 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/feature_store.py
+-rw-r--r--   0        0        0     1410 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/historical_feature_table.py
+-rw-r--r--   0        0        0    11647 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/info.py
+-rw-r--r--   0        0        0      991 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/item_table.py
+-rw-r--r--   0        0        0      599 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/materialized_table.py
+-rw-r--r--   0        0        0      909 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/observation_table.py
+-rw-r--r--   0        0        0      459 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/periodic_task.py
+-rw-r--r--   0        0        0      478 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/preview.py
+-rw-r--r--   0        0        0     1460 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/relationship_info.py
+-rw-r--r--   0        0        0     1190 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/request_table.py
+-rw-r--r--   0        0        0     1376 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/scd_table.py
+-rw-r--r--   0        0        0      914 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/semantic.py
+-rw-r--r--   0        0        0     1152 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/static_source_table.py
+-rw-r--r--   0        0        0     2415 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/table.py
+-rw-r--r--   0        0        0     2974 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/target.py
+-rw-r--r--   0        0        0     1852 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/target_namespace.py
+-rw-r--r--   0        0        0     1644 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/target_table.py
+-rw-r--r--   0        0        0     1012 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/task.py
+-rw-r--r--   0        0        0     1916 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/user_defined_function.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/__init__.py
+-rw-r--r--   0        0        0      333 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/progress.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/__init__.py
+-rw-r--r--   0        0        0     2829 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/base.py
+-rw-r--r--   0        0        0      739 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/batch_feature_create.py
+-rw-r--r--   0        0        0      610 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/batch_feature_table.py
+-rw-r--r--   0        0        0      602 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/batch_request_table.py
+-rw-r--r--   0        0        0     1382 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/deployment_create_update.py
+-rw-r--r--   0        0        0     1328 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0      677 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/feature_list_batch_feature_create.py
+-rw-r--r--   0        0        0      725 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/historical_feature_table.py
+-rw-r--r--   0        0        0     1459 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/materialized_table_delete.py
+-rw-r--r--   0        0        0      589 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/observation_table.py
+-rw-r--r--   0        0        0      602 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/static_source_table.py
+-rw-r--r--   0        0        0      576 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/target_table.py
+-rw-r--r--   0        0        0      544 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/test.py
+-rw-r--r--   0        0        0      459 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/schema/worker/task/tile.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/__init__.py
+-rw-r--r--   0        0        0    34638 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/base_document.py
+-rw-r--r--   0        0        0     1090 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/base_namespace_service.py
+-rw-r--r--   0        0        0     5215 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/base_table_document.py
+-rw-r--r--   0        0        0     1929 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/batch_feature_table.py
+-rw-r--r--   0        0        0     2831 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/batch_request_table.py
+-rw-r--r--   0        0        0      596 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/catalog.py
+-rw-r--r--   0        0        0     5638 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/context.py
+-rw-r--r--   0        0        0     5665 2023-07-25 03:10:46.696323 featurebyte-0.4.0/featurebyte/service/credential.py
+-rw-r--r--   0        0        0     4606 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/default_version_mode.py
+-rw-r--r--   0        0        0    18089 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/deploy.py
+-rw-r--r--   0        0        0      653 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/deployment.py
+-rw-r--r--   0        0        0      674 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/dimension_table.py
+-rw-r--r--   0        0        0     4246 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/entity.py
+-rw-r--r--   0        0        0     7231 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/entity_validation.py
+-rw-r--r--   0        0        0      618 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/event_table.py
+-rw-r--r--   0        0        0     6732 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature.py
+-rw-r--r--   0        0        0     3742 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    16639 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_list.py
+-rw-r--r--   0        0        0     4268 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_list_namespace.py
+-rw-r--r--   0        0        0     4251 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_list_status.py
+-rw-r--r--   0        0        0    12724 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_manager.py
+-rw-r--r--   0        0        0      574 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_namespace.py
+-rw-r--r--   0        0        0    12595 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_preview.py
+-rw-r--r--   0        0        0    15619 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_readiness.py
+-rw-r--r--   0        0        0     1468 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_store.py
+-rw-r--r--   0        0        0     8851 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/feature_store_warehouse.py
+-rw-r--r--   0        0        0     3392 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/historical_feature_table.py
+-rw-r--r--   0        0        0    14351 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/historical_features.py
+-rw-r--r--   0        0        0      604 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/item_table.py
+-rw-r--r--   0        0        0     4808 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/materialized_table.py
+-rw-r--r--   0        0        0     4446 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/mixin.py
+-rw-r--r--   0        0        0     5383 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/namespace_handler.py
+-rw-r--r--   0        0        0     6290 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/observation_table.py
+-rw-r--r--   0        0        0     9227 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/online_enable.py
+-rw-r--r--   0        0        0     4540 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/online_serving.py
+-rw-r--r--   0        0        0     2637 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/online_store_compute_query_service.py
+-rw-r--r--   0        0        0     3016 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/online_store_table_version.py
+-rw-r--r--   0        0        0     7009 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/parent_serving.py
+-rw-r--r--   0        0        0      437 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/periodic_task.py
+-rw-r--r--   0        0        0    10079 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/preview.py
+-rw-r--r--   0        0        0     9258 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/relationship.py
+-rw-r--r--   0        0        0     2311 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/relationship_info.py
+-rw-r--r--   0        0        0      991 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/sanitizer.py
+-rw-r--r--   0        0        0      590 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/scd_table.py
+-rw-r--r--   0        0        0      600 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/semantic.py
+-rw-r--r--   0        0        0     2830 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/session_manager.py
+-rw-r--r--   0        0        0     6617 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/session_validator.py
+-rw-r--r--   0        0        0     2930 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/static_source_table.py
+-rw-r--r--   0        0        0     1051 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/table.py
+-rw-r--r--   0        0        0    16479 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/table_columns_info.py
+-rw-r--r--   0        0        0     3275 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/table_info.py
+-rw-r--r--   0        0        0     2081 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/table_status.py
+-rw-r--r--   0        0        0     6967 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/target.py
+-rw-r--r--   0        0        0     6520 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/target_helper/base_feature_or_target_computer.py
+-rw-r--r--   0        0        0     5195 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/target_helper/compute_target.py
+-rw-r--r--   0        0        0      535 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/target_namespace.py
+-rw-r--r--   0        0        0     3085 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/target_table.py
+-rw-r--r--   0        0        0     9795 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/task_manager.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile/__init__.py
+-rw-r--r--   0        0        0    11222 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile/tile_task_executor.py
+-rw-r--r--   0        0        0     2362 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile_cache.py
+-rw-r--r--   0        0        0     7045 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile_job_log.py
+-rw-r--r--   0        0        0    14700 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile_manager.py
+-rw-r--r--   0        0        0     2681 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile_registry_service.py
+-rw-r--r--   0        0        0     2519 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/tile_scheduler.py
+-rw-r--r--   0        0        0     2662 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/user_defined_function.py
+-rw-r--r--   0        0        0      709 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/user_service.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/validator/__init__.py
+-rw-r--r--   0        0        0     6291 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/validator/materialized_table_delete.py
+-rw-r--r--   0        0        0    11655 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/validator/production_ready_validator.py
+-rw-r--r--   0        0        0    15448 2023-07-25 03:10:46.700324 featurebyte-0.4.0/featurebyte/service/version.py
+-rw-r--r--   0        0        0    13077 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/service/view_construction.py
+-rw-r--r--   0        0        0     4591 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/service/working_schema.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/__init__.py
+-rw-r--r--   0        0        0    29242 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/base.py
+-rw-r--r--   0        0        0    16306 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/base_spark.py
+-rw-r--r--   0        0        0     5643 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/databricks.py
+-rw-r--r--   0        0        0      434 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/enum.py
+-rw-r--r--   0        0        0     4993 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/hive.py
+-rw-r--r--   0        0        0     5107 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/manager.py
+-rw-r--r--   0        0        0     9179 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/simple_storage.py
+-rw-r--r--   0        0        0    13216 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/snowflake.py
+-rw-r--r--   0        0        0    11998 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/spark.py
+-rw-r--r--   0        0        0     3479 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/sqlite.py
+-rw-r--r--   0        0        0     6615 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/session/webhdfs.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/__init__.py
+-rw-r--r--   0        0        0     2569 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/base.py
+-rw-r--r--   0        0        0     2743 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/common.py
+-rw-r--r--   0        0        0        6 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/databricks/.gitignore
+-rw-r--r--   0        0        0      957 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql
+-rw-r--r--   0        0        0      476 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql
+-rw-r--r--   0        0        0      171 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT.sql
+-rw-r--r--   0        0        0      610 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
+-rw-r--r--   0        0        0      177 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
+-rw-r--r--   0        0        0      188 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_NUM_UNIQUE.sql
+-rw-r--r--   0        0        0     1491 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_GET_RANK.sql
+-rw-r--r--   0        0        0      449 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_GET_RELATIVE_FREQUENCY.sql
+-rw-r--r--   0        0        0      559 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql
+-rw-r--r--   0        0        0      559 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql
+-rw-r--r--   0        0        0      653 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql
+-rw-r--r--   0        0        0      153 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/snowflake/T_TILE_MONITOR_SUMMARY.sql
+-rw-r--r--   0        0        0        6 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/spark/.gitignore
+-rw-r--r--   0        0        0      191 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/spark/T_TILE_MONITOR_SUMMARY.sql
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/spark/__init__.py
+-rw-r--r--   0        0        0    29714 2023-07-25 03:12:56.191175 featurebyte-0.4.0/featurebyte/sql/spark/featurebyte-hive-udf-1.0.3-SNAPSHOT-all.jar
+-rw-r--r--   0        0        0     1923 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_common.py
+-rw-r--r--   0        0        0     5635 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_generate.py
+-rw-r--r--   0        0        0     3564 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_generate_entity_tracking.py
+-rw-r--r--   0        0        0     8611 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_monitor.py
+-rw-r--r--   0        0        0     2833 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_registry.py
+-rw-r--r--   0        0        0     6829 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/sql/tile_schedule_online_store.py
+-rw-r--r--   0        0        0      297 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/storage/__init__.py
+-rw-r--r--   0        0        0     4999 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/storage/base.py
+-rw-r--r--   0        0        0     3640 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/storage/local.py
+-rw-r--r--   0        0        0      415 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/storage/local_temp.py
+-rw-r--r--   0        0        0     5575 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/storage/s3.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/tile/__init__.py
+-rw-r--r--   0        0        0      411 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/tile/sql_template.py
+-rw-r--r--   0        0        0    26885 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/tile/tile_cache.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/__init__.py
+-rw-r--r--   0        0        0     1909 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/credential.py
+-rw-r--r--   0        0        0     1532 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/messaging.py
+-rw-r--r--   0        0        0      534 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/persistent.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/snowflake/__init__.py
+-rw-r--r--   0        0        0      462 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/snowflake/sql.py
+-rw-r--r--   0        0        0     2225 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/utils/storage.py
+-rw-r--r--   0        0        0     1990 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/__init__.py
+-rw-r--r--   0        0        0      170 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/enum.py
+-rw-r--r--   0        0        0     1159 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/progress.py
+-rw-r--r--   0        0        0     2502 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/schedulers.py
+-rw-r--r--   0        0        0      230 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/start.py
+-rw-r--r--   0        0        0      214 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/__init__.py
+-rw-r--r--   0        0        0     2926 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/base.py
+-rw-r--r--   0        0        0    11492 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/batch_feature_create.py
+-rw-r--r--   0        0        0     3718 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/batch_feature_table.py
+-rw-r--r--   0        0        0     2243 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/batch_request_table.py
+-rw-r--r--   0        0        0     1795 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/deployment_create_update.py
+-rw-r--r--   0        0        0     6763 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0     1463 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/feature_list_batch_feature_create.py
+-rw-r--r--   0        0        0     3502 2023-07-25 03:10:46.704324 featurebyte-0.4.0/featurebyte/worker/task/historical_feature_table.py
+-rw-r--r--   0        0        0     5405 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/materialized_table_delete.py
+-rw-r--r--   0        0        0     2604 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/mixin.py
+-rw-r--r--   0        0        0     2226 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/observation_table.py
+-rw-r--r--   0        0        0     2197 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/static_source_table.py
+-rw-r--r--   0        0        0     3287 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/target_table.py
+-rw-r--r--   0        0        0      626 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/test_task.py
+-rw-r--r--   0        0        0     1541 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task/tile_task.py
+-rw-r--r--   0        0        0     6279 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/task_executor.py
+-rw-r--r--   0        0        0        0 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/util/__init__.py
+-rw-r--r--   0        0        0     1773 2023-07-25 03:10:46.708324 featurebyte-0.4.0/featurebyte/worker/util/observation_set_helper.py
+-rw-r--r--   0        0        0     7915 2023-07-25 03:11:27.323424 featurebyte-0.4.0/pyproject.toml
+-rw-r--r--   0        0        0    23156 1970-01-01 00:00:00.000000 featurebyte-0.4.0/PKG-INFO
```

### Comparing `featurebyte-0.3.1/LICENSE` & `featurebyte-0.4.0/LICENSE`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/README.md` & `featurebyte-0.4.0/README.md`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/__init__.py` & `featurebyte-0.4.0/featurebyte/api/deployment.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,507 +1,392 @@
-"""Python Library for FeatureOps"""
-from typing import Any, List, Optional
+"""
+Deployment module
+"""
+from __future__ import annotations
 
-import sys
-from http import HTTPStatus
+from typing import Literal, Optional
+
+import json
+import os
 
 import pandas as pd
+from bson import ObjectId
+from jinja2 import Template
+from typeguard import typechecked
 
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.batch_feature_table import BatchFeatureTable
 from featurebyte.api.batch_request_table import BatchRequestTable
-from featurebyte.api.catalog import Catalog
-from featurebyte.api.change_view import ChangeView
-from featurebyte.api.credential import Credential
-from featurebyte.api.data_source import DataSource
-from featurebyte.api.deployment import Deployment
-from featurebyte.api.dimension_table import DimensionTable
-from featurebyte.api.dimension_view import DimensionView
 from featurebyte.api.entity import Entity
-from featurebyte.api.event_table import EventTable
-from featurebyte.api.event_view import EventView
-from featurebyte.api.feature import Feature
-from featurebyte.api.feature_group import BaseFeatureGroup, FeatureGroup
-from featurebyte.api.feature_job_setting_analysis import FeatureJobSettingAnalysis
+from featurebyte.api.feature_job import FeatureJobStatusResult
 from featurebyte.api.feature_list import FeatureList
-from featurebyte.api.feature_store import FeatureStore
-from featurebyte.api.historical_feature_table import HistoricalFeatureTable
-from featurebyte.api.item_table import ItemTable
-from featurebyte.api.item_view import ItemView
-from featurebyte.api.observation_table import ObservationTable
-from featurebyte.api.periodic_task import PeriodicTask
-from featurebyte.api.relationship import Relationship
-from featurebyte.api.request_column import RequestColumn
-from featurebyte.api.scd_table import SCDTable
-from featurebyte.api.scd_view import SCDView
-from featurebyte.api.source_table import SourceTable
 from featurebyte.api.table import Table
-from featurebyte.common.env_util import is_notebook
-from featurebyte.common.utils import get_version
-from featurebyte.config import Configurations, Profile
-from featurebyte.core.series import Series
-from featurebyte.core.timedelta import to_timedelta
-from featurebyte.datasets.app import import_dataset
-from featurebyte.docker.manager import ApplicationName
-from featurebyte.docker.manager import start_app as _start_app
-from featurebyte.docker.manager import start_playground as _start_playground
-from featurebyte.docker.manager import stop_app as _stop_app
-from featurebyte.enum import AggFunc, SourceType, StorageType
-from featurebyte.exception import (
-    FeatureByteException,
-    InvalidSettingsError,
-    RecordRetrievalException,
-)
-from featurebyte.feature_utility import list_unsaved_features
-from featurebyte.logging import get_logger
-from featurebyte.models.credential import (
-    AccessTokenCredential,
-    AzureBlobStorageCredential,
-    GCSStorageCredential,
-    S3StorageCredential,
-    UsernamePasswordCredential,
-)
-from featurebyte.models.feature import DefaultVersionMode
-from featurebyte.models.feature_list import FeatureListStatus
-from featurebyte.models.feature_store import TableStatus
-from featurebyte.query_graph.model.feature_job_setting import (
-    FeatureJobSetting,
-    TableFeatureJobSetting,
-)
-from featurebyte.query_graph.node.cleaning_operation import (
-    ColumnCleaningOperation,
-    DisguisedValueImputation,
-    MissingValueImputation,
-    StringValueImputation,
-    TableCleaningOperation,
-    UnexpectedValueImputation,
-    ValueBeyondEndpointImputation,
-)
-from featurebyte.query_graph.node.schema import DatabricksDetails, SnowflakeDetails, SparkDetails
-from featurebyte.schema.deployment import DeploymentSummary
-from featurebyte.schema.feature_list import FeatureVersionInfo
-
-version: str = get_version()
-
-logger = get_logger(__name__)
-
-
-def list_profiles() -> pd.DataFrame:
-    """
-    List all service profiles
-
-    Returns
-    -------
-    pd.DataFrame
-        List of service profiles
-
-    Examples
-    --------
-    >>> fb.list_profiles()
-        name                api_url api_token
-    0  local  http://127.0.0.1:8088      None
-    """
-    profiles = Configurations().profiles
-    return pd.DataFrame([profile.dict() for profile in profiles] if profiles else [])
-
-
-def get_active_profile() -> Profile:
-    """
-    Get active profile from configuration file.
-
-    Returns
-    -------
-    Profile
-
-    Raises
-    ------
-    InvalidSettingsError
-        If no profile is found in configuration file
-    """
-    conf = Configurations()
-    if not conf.profile:
-        logger.error(
-            "No profile found. Please update your configuration file at {conf.config_file_path}"
+from featurebyte.common.doc_util import FBAutoDoc
+from featurebyte.common.formatting_util import CodeStr
+from featurebyte.config import Configurations
+from featurebyte.exception import FeatureListNotOnlineEnabledError
+from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.deployment import DeploymentModel
+from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
+from featurebyte.schema.deployment import DeploymentUpdate
+
+
+class Deployment(ApiObject):
+    """
+    A FeatureByte Catalog serves as a centralized repository for storing metadata about FeatureByte objects such as
+    tables, entities, features, and feature lists associated with a specific domain. It functions as an effective tool
+    for facilitating collaboration among team members working on similar use cases or utilizing the same data source
+    within a data warehouse.
+
+    By employing a catalog, team members can effortlessly search, retrieve, and reuse the necessary tables,
+    entities, features, and feature lists while obtaining comprehensive information about their properties.
+    This information includes their type, creation date, related versions, status, and other descriptive details.
+
+    For data warehouses covering multiple domains, creating multiple catalogs can help maintain organization and
+    simplify management of the data and features.
+    """
+
+    __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.Deployment")
+
+    # class variables
+    _route = "/deployment"
+    _list_schema = DeploymentModel
+    _get_schema = DeploymentModel
+    _update_schema_class = DeploymentUpdate
+    _list_fields = [
+        "name",
+        "feature_list_name",
+        "feature_list_version",
+        "num_feature",
+        "enabled",
+    ]
+    _list_foreign_keys = [
+        ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_name", "name", True),
+        ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_version", "version", True),
+        ForeignKeyMapping("feature_list_id", FeatureList, "num_feature", "num_feature", True),
+    ]
+
+    @property
+    def enabled(self) -> bool:
+        """
+        Deployment enabled status
+
+        Returns
+        -------
+        bool
+        """
+        return self.cached_model.enabled
+
+    @property
+    def feature_list_id(self) -> PydanticObjectId:
+        """
+        Feature list ID associated with this deployment.
+
+        Returns
+        -------
+        PydanticObjectId
+        """
+        return self.cached_model.feature_list_id
+
+    def enable(self) -> None:
+        """
+        Enable the deployment.
+
+        Examples
+        --------
+        >>> deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
+        >>> deployment.enable()  # doctest: +SKIP
+        """
+        self.patch_async_task(route=f"{self._route}/{self.id}", payload={"enabled": True})
+        # call get to update the object cache
+        self.get_by_id(self.id)
+
+    def disable(self) -> None:
+        """
+        Disable the deployment.
+
+        Examples
+        --------
+        >>> deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
+        >>> deployment.disable()  # doctest: +SKIP
+        """
+        self.patch_async_task(route=f"{self._route}/{self.id}", payload={"enabled": False})
+        # call get to update the object cache
+        self.get_by_id(self.id)
+
+    @typechecked
+    def compute_batch_feature_table(
+        self,
+        batch_request_table: BatchRequestTable,
+        batch_feature_table_name: str,
+    ) -> BatchFeatureTable:
+        """
+        Get batch features asynchronously using a batch request table. The batch request features
+        will be materialized into a batch feature table.
+
+        Parameters
+        ----------
+        batch_request_table: BatchRequestTable
+            Batch request table contains required serving names columns
+        batch_feature_table_name: str
+            Name of the batch feature table to be created
+
+        Returns
+        -------
+        BatchFeatureTable
+
+        Examples
+        --------
+        >>. deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
+        >>> batch_features = deployment.compute_batch_feature_table(  # doctest: +SKIP
+        ...   batch_request_table=batch_request_table,
+        ...   batch_feature_table_name = <batch_feature_table_name>
+        ... )
+        """
+        payload = BatchFeatureTableCreate(
+            name=batch_feature_table_name,
+            feature_store_id=batch_request_table.location.feature_store_id,
+            batch_request_table_id=batch_request_table.id,
+            deployment_id=self.id,
         )
-        raise InvalidSettingsError("No profile found")
-
-    logger.info(f"Active profile: {conf.profile.name} ({conf.profile.api_url})")
-    versions = Configurations().check_sdk_versions()
-    if versions["remote sdk"] != versions["local sdk"]:
-        logger.warning(
-            f"Remote SDK version ({versions['remote sdk']}) is different from local ({versions['local sdk']}). "
-            "Update local SDK to avoid unexpected behavior."
+        batch_feature_table_doc = self.post_async_task(
+            route="/batch_feature_table", payload=payload.json_dict()
         )
-    else:
-        logger.info(f"SDK version: {versions['local sdk']}")
-    return conf.profile
-
-
-def use_profile(profile: str) -> None:
-    """
-    Use service profile specified in configuration file.
-
-    Parameters
-    ----------
-    profile: str
-        Profile name
-
-    Examples
-    --------
-    Use the local profile
-    >>> fb.use_profile("local")
-    """
-    try:
-        logger.info(f"Using profile: {profile}")
-        Configurations().use_profile(profile)
-    finally:
-        # report active profile
-        try:
-            get_active_profile()
-        except InvalidSettingsError:
-            pass
-
-
-def list_credentials(
-    include_id: Optional[bool] = False,
-) -> pd.DataFrame:
-    """
-    List all credentials
-
-    Parameters
-    ----------
-    include_id: Optional[bool]
-        Whether to include id in the list
-
-    Returns
-    -------
-    pd.DataFrame
-        List of credentials
-
-    Examples
-    --------
-    >>> fb.list_credentials()[["feature_store", "database_credential_type", "storage_credential_type"]]
-      feature_store database_credential_type storage_credential_type
-    0    playground                     None                    None
-    """
-    return Credential.list(include_id=include_id)
-
-
-def list_feature_stores(include_id: Optional[bool] = False) -> pd.DataFrame:
-    """
-    List all feature stores
-
-    Parameters
-    ----------
-    include_id: Optional[bool]
-        Whether to include id in the list
-
-    Returns
-    -------
-    pd.DataFrame
-        List of feature stores
-
-    Examples
-    --------
-    >>> fb.list_feature_stores()[["name", "type"]]
-             name   type
-    0  playground  spark
-    """
-    return FeatureStore.list(include_id=include_id)
-
-
-def get_feature_store(name: str) -> FeatureStore:
-    """
-    Get feature store by name
-
-    Parameters
-    ----------
-    name : str
-        Feature store name
-
-    Returns
-    -------
-    FeatureStore
-        Feature store
-
-    Examples
-    --------
-    >>> feature_store = fb.get_feature_store("playground")
-    """
-    return FeatureStore.get(name)
-
-
-def list_catalogs(include_id: Optional[bool] = False) -> pd.DataFrame:
-    """
-    List all catalogs
-
-    Parameters
-    ----------
-    include_id: Optional[bool]
-        Whether to include id in the list
-
-
-    Returns
-    -------
-    pd.DataFrame
-        List of catalogs
-
-    Examples
-    --------
-    >>> fb.list_catalogs()[["name", "active"]]
-              name  active
-        0  grocery    True
-        1  default   False
-    """
-    return Catalog.list(include_id=include_id)
-
-
-def activate_and_get_catalog(name: str) -> Catalog:
-    """
-    Activate catalog by name
-
-    Parameters
-    ----------
-    name : str
-        Catalog name
-
-    Returns
-    -------
-    Catalog
-        Catalog
-
-    Examples
-    --------
-    >>> catalog = fb.activate_and_get_catalog("grocery")
-    """
-    return Catalog.activate(name)
-
-
-def start() -> None:
-    """
-    Start featurebyte application
-    """
-    _start_app(ApplicationName.FEATUREBYTE, verbose=False)
-
-
-def start_spark() -> None:
-    """
-    Start local spark application
-    """
-    _start_app(ApplicationName.SPARK, verbose=False)
-
-
-def stop(clean: bool = False) -> None:
-    """
-    Stop all applications
-
-    Parameters
-    ----------
-    clean : bool
-        Whether to clean up all data, by default False
-    """
-    _stop_app(clean=clean, verbose=False)
-
-
-def playground(
-    datasets: Optional[List[str]] = None,
-    force_import: bool = False,
-) -> None:
-    """
-    Start playground environment
-
-    Parameters
-    ----------
-    datasets : Optional[List[str]]
-        List of datasets to import, by default None (import all datasets)
-    force_import: bool
-        Import datasets even if they are already imported, by default False
-    """
-    _start_playground(
-        datasets=datasets,
-        force_import=force_import,
-        verbose=False,
-    )
-
-
-def list_deployments(
-    include_id: Optional[bool] = True,
-) -> pd.DataFrame:
-    """
-    List all deployments across all catalogs.
-    Deployed features are updated regularly based on their job settings and consume recurring compute resources
-    in the data warehouse.
-    It is recommended to delete deployments when they are no longer needed to avoid unnecessary costs.
-
-    Parameters
-    ----------
-    include_id: Optional[bool]
-        Whether to include id in the list
-
-    Returns
-    -------
-    pd.DataFrame
-        List of deployments
-
-    Examples
-    --------
-    >>> fb.list_deployments()
-    Empty DataFrame
-    Columns: [id, name, catalog_name, feature_list_name, feature_list_version, num_feature]
-    Index: []
-
-    See Also
-    --------
-    - [FeatureList.deploy](/reference/featurebyte.api.feature_list.FeatureList.deploy/) Deploy / Undeploy a feature list
-    """
-    output = []
-    for item_dict in Deployment.iterate_api_object_using_paginated_routes(
-        route="/deployment/all/", params={"enabled": True}
-    ):
-        output.append(item_dict)
-    columns = ["name", "catalog_name", "feature_list_name", "feature_list_version", "num_feature"]
-    output_df = pd.DataFrame(
-        output,
-        columns=["_id"] + columns,
-    ).rename(columns={"_id": "id"})
-    if include_id:
-        return output_df
-    return output_df.drop(columns=["id"])
-
-
-__all__ = [
-    # API objects
-    "BatchFeatureTable",
-    "BatchRequestTable",
-    "Catalog",
-    "ChangeView",
-    "DatabricksDetails",
-    "DataSource",
-    "Deployment",
-    "DimensionTable",
-    "DimensionView",
-    "Entity",
-    "EventTable",
-    "EventView",
-    "Feature",
-    "FeatureGroup",
-    "FeatureJobSetting",
-    "FeatureJobSettingAnalysis",
-    "FeatureList",
-    "FeatureStore",
-    "HistoricalFeatureTable",
-    "ItemTable",
-    "ItemView",
-    "ObservationTable",
-    "Relationship",
-    "RequestColumn",
-    "SCDTable",
-    "SCDView",
-    "SourceTable",
-    "SnowflakeDetails",
-    "SparkDetails",
-    "to_timedelta",
-    "Table",
-    "TableFeatureJobSetting",
-    # credentials
-    "AzureBlobStorageCredential",
-    "AccessTokenCredential",
-    "Credential",
-    "GCSStorageCredential",
-    "S3StorageCredential",
-    "UsernamePasswordCredential",
-    # enums
-    "AggFunc",
-    "FeatureListStatus",
-    "SourceType",
-    "StorageType",
-    "TableStatus",
-    # imputation related classes
-    "MissingValueImputation",
-    "DisguisedValueImputation",
-    "UnexpectedValueImputation",
-    "ValueBeyondEndpointImputation",
-    "StringValueImputation",
-    # feature & feature list version specific classes
-    "DefaultVersionMode",
-    "FeatureVersionInfo",
-    # others
-    "ColumnCleaningOperation",
-    "TableCleaningOperation",
-    "PeriodicTask",
-    # services
-    "start",
-    "stop",
-    "playground",
-    # utility
-    "list_unsaved_features",
-]
+        return BatchFeatureTable.get_by_id(batch_feature_table_doc["_id"])
 
+    def get_online_serving_code(self, language: Literal["python", "sh"] = "python") -> str:
+        """
+        Retrieves either Python or shell script template for serving online features from a deployed featurelist,
+        defaulted to python.
+
+        Parameters
+        ----------
+        language: Literal["python", "sh"]
+            Language for which to get code template
+
+        Returns
+        -------
+        str
+
+        Raises
+        ------
+        FeatureListNotOnlineEnabledError
+            Feature list not deployed
+        NotImplementedError
+            Serving code not available
+
+        Examples
+        --------
+        Retrieve python code template when "language" is set to "python"
+
+        >>> feature_list = catalog.get_feature_list("invoice_feature_list")
+        >>> deployment = feature_list.deploy()  # doctest: +SKIP
+        >>> deployment.enable()  # doctest: +SKIP
+        >>> deployment.get_online_serving_code(language="python")  # doctest: +SKIP
+            from typing import Any, Dict
+            import pandas as pd
+            import requests
+            def request_features(entity_serving_names: Dict[str, Any]) -> pd.DataFrame:
+                "
+                Send POST request to online serving endpoint
+                Parameters
+                ----------
+                entity_serving_names: Dict[str, Any]
+                    Entity serving name values to used for serving request
+                Returns
+                -------
+                pd.DataFrame
+                "
+                response = requests.post(
+                    url="http://localhost:8080/deployment/{deployment.id}/online_features",
+                    headers={{"Content-Type": "application/json", "active-catalog-id": "{catalog.id}"}},
+                    json={{"entity_serving_names": entity_serving_names}},
+                )
+                assert response.status_code == 200, response.json()
+                return pd.DataFrame.from_dict(response.json()["features"])
+            request_features([{{"cust_id": "sample_cust_id"}}])
+
+        Retrieve shell script template when "language" is set to "sh"
+
+        >>> feature_list = catalog.get_feature_list("invoice_feature_list")
+        >>> deployment = feature_list.deploy()  # doctest: +SKIP
+        >>> deployment.enable()  # doctest: +SKIP
+        >>> deployment.get_online_serving_code(language="sh")  # doctest: +SKIP
+            \\#!/bin/sh
+            curl -X POST
+                -H 'Content-Type: application/json' \\
+                -H 'Authorization: Bearer token' \\
+                -H 'active-catalog-id: {catalog.id}' \\
+                -d '{{"entity_serving_names": [{{"cust_id": "sample_cust_id"}}]}}' \\
+                http://localhost:8080/deployment/{deployment.id}/online_features
+
+        See Also
+        --------
+        - [FeatureList.deploy](/reference/featurebyte.api.feature_list.FeatureList.deploy/)
+        - [Deployment.enable](/reference/featurebyte.api.deployment.Deployment.enable/)
+        """
+        # pylint: disable=too-many-locals
+        if not self.enabled:
+            raise FeatureListNotOnlineEnabledError("Deployment is not enabled.")
+
+        templates = {"python": "python.tpl", "sh": "shell.tpl"}
+        template_file = templates.get(language)
+        if not template_file:
+            raise NotImplementedError(f"Supported languages: {list(templates.keys())}")
+
+        # get entities and tables used for the feature list
+        num_rows = 1
+        feature_list = FeatureList.get_by_id(self.feature_list_id)
+        feature_list_info = feature_list.info()
+        entities = {
+            Entity.get(entity["name"]).id: {"serving_name": entity["serving_names"]}
+            for entity in feature_list_info["primary_entity"]
+        }
+        for tabular_source in feature_list_info["tables"]:
+            data = Table.get(tabular_source["name"])
+            entity_columns = [
+                column for column in data.columns_info if column.entity_id in entities
+            ]
+            if entity_columns:
+                sample_data = data.preview(num_rows)
+                for column in entity_columns:
+                    entities[column.entity_id]["sample_value"] = sample_data[column.name].to_list()
+
+        entity_serving_names = json.dumps(
+            [
+                {
+                    entity["serving_name"][0]: entity["sample_value"][row_idx]
+                    for entity in entities.values()
+                }
+                for row_idx in range(num_rows)
+            ]
+        )
 
-def log_env_summary() -> None:
-    """
-    Print environment summary.
+        # construct serving url
+        current_profile = Configurations().profile
+        assert current_profile
+        info = self.info()
+        serving_endpoint = info["serving_endpoint"]
+        headers = {
+            "Content-Type": "application/json",
+            "active-catalog-id": str(feature_list.catalog_id),
+        }
+        if current_profile.api_token:
+            headers["Authorization"] = f"Bearer {current_profile.api_token}"
+        header_params = " \\\n    ".join([f"-H '{key}: {value}'" for key, value in headers.items()])
+        serving_url = f"{current_profile.api_url}{serving_endpoint}"
+
+        # populate template
+        with open(
+            file=os.path.join(
+                os.path.dirname(__file__), f"templates/online_serving/{template_file}"
+            ),
+            mode="r",
+            encoding="utf-8",
+        ) as file_object:
+            template = Template(file_object.read())
+
+        return CodeStr(
+            template.render(
+                headers=json.dumps(headers),
+                header_params=header_params,
+                serving_url=serving_url,
+                entity_serving_names=entity_serving_names,
+            )
+        )
 
-    Raises
-    ------
-    RecordRetrievalException
-        Failed to fetch deployment summary.
-    """
+    def get_feature_jobs_status(
+        self,
+        job_history_window: int = 1,
+        job_duration_tolerance: int = 60,
+    ) -> FeatureJobStatusResult:
+        """
+        Get the status of feature jobs in the associated feature list used for the deployment.
+
+        Parameters
+        ----------
+        job_history_window: int
+            History window in hours.
+        job_duration_tolerance: int
+            Maximum duration before job is considered later, in seconds.
+
+        Returns
+        -------
+        FeatureJobStatusResult
+        """
+        return FeatureList.get_by_id(self.feature_list_id).get_feature_jobs_status(
+            job_history_window=job_history_window,
+            job_duration_tolerance=job_duration_tolerance,
+        )
 
-    # configuration informaton
-    conf = Configurations()
-    logger.info(f"Using configuration file at: {conf.config_file_path}")
-
-    # report active profile
-    get_active_profile()
-
-    # catalog informaton
-    current_catalog = Catalog.get_active()
-    logger.info(f"Active catalog: {current_catalog.name}")
-
-    # list deployments
-    client = conf.get_client()
-    response = client.get("/deployment/summary/")
-    if response.status_code != HTTPStatus.OK:
-        raise RecordRetrievalException(response, "Failed to fetch deployment summary")
-    summary = DeploymentSummary(**response.json())
-    logger.info(
-        f"{summary.num_feature_list} feature list{'s' if summary.num_feature_list else ''}, "
-        f"{summary.num_feature} feature{'s' if summary.num_feature else ''} deployed"
-    )
-
-
-if is_notebook():
-    # Custom exception handler for notebook environment to make error messages more readable.
-    # Overrides the default IPython exception handler to repackage featurebyte exceptions
-    # to keeps only the last stack frame (the one that invoked the featurebyte api).
-    #
-    # This keeps two key pieces of information in the stack trace:
-    # 1. The exception class and message
-    # 2. The line number of the code that invoked the featurebyte api
-
-    # pylint: disable=import-outside-toplevel
-    import IPython  # pylint: disable=import-error
-
-    shell = IPython.core.interactiveshell.InteractiveShell
-    default_showtraceback = shell.showtraceback
-
-    def _showtraceback(cls: Any, *args: Any, **kwargs: Any) -> None:
-        exc_cls, exc_obj, tb_obj = sys.exc_info()
-        if isinstance(exc_obj, FeatureByteException) and not exc_obj.repackaged:
-            logger.error(exc_obj)
-            assert exc_cls
-            assert issubclass(exc_cls, FeatureByteException)
-            if tb_obj and tb_obj.tb_next:
-                invoke_frame = tb_obj.tb_next
-                invoke_frame.tb_next = None
-                exc_kwargs = {**exc_obj.__dict__, "repackaged": True}
-                raise exc_cls(**exc_kwargs).with_traceback(invoke_frame) from None
-        default_showtraceback(cls, *args, **kwargs)
-
-    IPython.core.interactiveshell.InteractiveShell.showtraceback = _showtraceback
-
-    # log environment summary
-    try:
-        log_env_summary()
-    except InvalidSettingsError as exc:
-        # import should not fail - warn and continue
-        logger.warning(exc)
+    @classmethod
+    def get(cls, name: str) -> Deployment:
+        """
+        Gets a Deployment object by its name.
+
+        Parameters
+        ----------
+        name: str
+            Name of the deployment to retrieve.
+
+        Returns
+        -------
+        Deployment
+            Deployment object.
+
+        Examples
+        --------
+        Get a Deployment object that is already saved.
+
+        >>> deployment = fb.Deployment.get(<deployment_name>)  # doctest: +SKIP
+        """
+        return super().get(name)
+
+    @classmethod
+    def get_by_id(
+        cls, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
+    ) -> Deployment:
+        """
+        Returns a Deployment object by its unique identifier (ID).
+
+        Parameters
+        ----------
+        id: ObjectId
+            Deployment unique identifier ID.
+
+        Returns
+        -------
+        Deployment
+            Deployment object.
+
+        Examples
+        --------
+        Get a Deployment object that is already saved.
+
+        >>> fb.Deployment.get_by_id(<deployment_id>)  # doctest: +SKIP
+        """
+        return cls._get_by_id(id=id)
+
+    @classmethod
+    def list(cls, include_id: Optional[bool] = True) -> pd.DataFrame:
+        """
+        Returns a DataFrame that lists the deployments by their names, feature list names, feature list versions,
+        number of features, and whether the features are enabled.
+
+        Parameters
+        ----------
+        include_id: Optional[bool]
+            Whether to include id in the list.
+
+        Returns
+        -------
+        DataFrame
+            Table of objects.
+
+        Examples
+        --------
+        List all deployments.
+
+        >>> deployments = fb.Deployment.list()
+        """
+        return super().list(include_id=include_id)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/asat_aggregator.py` & `featurebyte-0.4.0/featurebyte/api/asat_aggregator.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 """
 from __future__ import annotations
 
 from typing import List, Optional, Type, cast
 
 from typeguard import typechecked
 
-from featurebyte.api.base_aggregator import BaseAggregator
+from featurebyte.api.aggregator.base_aggregator import BaseAggregator
 from featurebyte.api.feature import Feature
 from featurebyte.api.scd_view import SCDView
 from featurebyte.api.view import View
 from featurebyte.common.model_util import validate_offset_string
 from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
```

### Comparing `featurebyte-0.3.1/featurebyte/api/base_aggregator.py` & `featurebyte-0.4.0/featurebyte/api/aggregator/base_aggregator.py`

 * *Files 15% similar despite different names*

```diff
@@ -92,40 +92,65 @@
     ) -> None:
         if fill_value is not None and skip_fill_na:
             raise ValueError(
                 "Specifying both fill_value and skip_fill_na is not allowed;"
                 " try setting fill_value to None or skip_fill_na to False"
             )
 
+    def get_output_var_type(
+        self, agg_method: AggFuncType, method: str, value_column: str
+    ) -> DBVarType:
+        """
+        Get output variable type for aggregation method.
+
+        Parameters
+        ----------
+        agg_method: AggFuncType
+            Aggregation method
+        method: str
+            Aggregation method name
+        value_column: str
+            Value column name
+
+        Returns
+        -------
+        DBVarType
+
+        Raises
+        ------
+        ValueError
+            If aggregation method does not support input variable type
+        """
+        # value_column is None for count-like aggregation method
+        input_var_type = self.view.column_var_type_map.get(value_column, DBVarType.FLOAT)
+        if not agg_method.is_var_type_supported(input_var_type):
+            raise ValueError(
+                f'Aggregation method "{method}" does not support "{input_var_type}" input variable'
+            )
+        return agg_method.derive_output_var_type(
+            input_var_type=input_var_type, category=self.category
+        )
+
     def _project_feature_from_groupby_node(
         self,
         agg_method: AggFuncType,
         feature_name: str,
         groupby_node: Node,
         method: str,
         value_column: Optional[str],
         fill_value: OptionalScalar,
         skip_fill_na: bool,
     ) -> Feature:
         # value_column is None for count-like aggregation method
-        input_var_type = self.view.column_var_type_map.get(value_column, DBVarType.FLOAT)  # type: ignore
-        if not agg_method.is_var_type_supported(input_var_type):
-            raise ValueError(
-                f'Aggregation method "{method}" does not support "{input_var_type}" input variable'
-            )
-
-        var_type = agg_method.derive_output_var_type(
-            input_var_type=input_var_type, category=self.category
-        )
+        var_type = self.get_output_var_type(agg_method, method, value_column)  # type: ignore[arg-type]
 
         feature = self.view._project_feature_from_node(  # pylint: disable=protected-access
             node=groupby_node,
             feature_name=feature_name,
             feature_dtype=var_type,
-            entity_ids=self.entity_ids,
         )
         if not skip_fill_na:
             self._fill_feature(feature, method, feature_name, fill_value)
         return feature
 
     def _fill_feature(
         self,
```

### Comparing `featurebyte-0.3.1/featurebyte/api/base_table.py` & `featurebyte-0.4.0/featurebyte/api/base_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,16 @@
 
 import pandas as pd
 from bson.objectid import ObjectId
 from pandas import DataFrame
 from pydantic import Field
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.entity import Entity
 from featurebyte.api.savable_api_object import SavableApiObject
 from featurebyte.api.source_table import AbstractTableData, SourceTable
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.config import Configurations
 from featurebyte.core.mixin import GetAttrMixin, ParentMixin
 from featurebyte.enum import TableDataType, ViewMode
@@ -545,17 +546,17 @@
 
         Returns
         -------
         BaseTableData
             Table data object used for SQL query construction.
         """
         try:
-            return self._table_data_class(**self.cached_model.json_dict())
+            return self._table_data_class(**self.cached_model.dict(by_alias=True))
         except RecordRetrievalException:
-            return self._table_data_class(**self.json_dict())
+            return self._table_data_class(**self.dict(by_alias=True))
 
     @property
     def columns_info(self) -> List[ColumnInfo]:
         """
         Provides information about the columns in the table such as column name, column type, entity ID associated
         with the column, semantic ID associated with the column, and the critical data information associated with
         the column.
@@ -693,15 +694,15 @@
             )
             for col in self.columns_info
             if col.critical_data_info is not None and col.critical_data_info.cleaning_operations
         ]
 
     def _get_create_payload(self) -> dict[str, Any]:
         assert self._create_schema_class is not None
-        data = self._create_schema_class(**self.json_dict())  # pylint: disable=not-callable
+        data = self._create_schema_class(**self.dict(by_alias=True))  # pylint: disable=not-callable
         return data.json_dict()
 
     @classmethod
     @typechecked
     def create(
         cls: Type[SourceTableApiObjectT],
         source_table: SourceTable,
@@ -751,15 +752,15 @@
 
         client = Configurations().get_client()
         response = client.get(url=cls._route, params={"name": name})
         if response.status_code == HTTPStatus.OK:
             response_dict = response.json()
             if not response_dict["data"]:
                 table = cls(
-                    **data.json_dict(),
+                    **data.dict(by_alias=True),
                     feature_store=source_table.feature_store,
                     _validate_schema=True,
                 )
                 table.save()
                 return table
             existing_record = response_dict["data"][0]
             raise DuplicatedRecordException(
```

### Comparing `featurebyte-0.3.1/featurebyte/api/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/api/batch_feature_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.batch_request_table import BatchRequestTable
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.schema.batch_feature_table import BatchFeatureTableListRecord
```

### Comparing `featurebyte-0.3.1/featurebyte/api/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/api/batch_request_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.schema.batch_request_table import BatchRequestTableListRecord
```

### Comparing `featurebyte-0.3.1/featurebyte/api/catalog.py` & `featurebyte-0.4.0/featurebyte/api/catalog.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 from typing import Any, Dict, List, Literal, Optional, Union
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
 
+from featurebyte.api.api_handler.base import ListHandler
+from featurebyte.api.api_handler.catalog import CatalogListHandler
 from featurebyte.api.api_object_util import NameAttributeUpdatableMixin
 from featurebyte.api.batch_feature_table import BatchFeatureTable
 from featurebyte.api.batch_request_table import BatchRequestTable
 from featurebyte.api.catalog_decorator import update_and_reset_catalog
 from featurebyte.api.catalog_get_by_id_mixin import CatalogGetByIdMixin
 from featurebyte.api.data_source import DataSource
 from featurebyte.api.deployment import Deployment
@@ -24,15 +26,19 @@
 from featurebyte.api.feature_list import FeatureList
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.historical_feature_table import HistoricalFeatureTable
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.api.periodic_task import PeriodicTask
 from featurebyte.api.relationship import Relationship
 from featurebyte.api.savable_api_object import SavableApiObject
+from featurebyte.api.static_source_table import StaticSourceTable
 from featurebyte.api.table import Table
+from featurebyte.api.target import Target
+from featurebyte.api.target_table import TargetTable
+from featurebyte.api.user_defined_function import UserDefinedFunction
 from featurebyte.api.view import View
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId, activate_catalog, get_active_catalog_id
 from featurebyte.models.catalog import CatalogModel
 from featurebyte.models.relationship import RelationshipType
@@ -86,15 +92,15 @@
         """
         try:
             return self.cached_model.default_feature_store_ids
         except RecordRetrievalException:
             return self.internal_default_feature_store_ids
 
     def _get_create_payload(self) -> Dict[str, Any]:
-        data = CatalogCreate(**self.json_dict())
+        data = CatalogCreate(**self.dict(by_alias=True))
         return data.json_dict()
 
     def info(self, verbose: bool = False) -> Dict[str, Any]:
         """
         Returns a dictionary that summarizes the essential information of a Catalog object. The dictionary includes
         the following keys:
 
@@ -219,53 +225,55 @@
         --------
         Create a new catalog
 
         >>> catalog = fb.Catalog.get_or_create("grocery", "playground")
         >>> fb.Catalog.list()[["name", "active"]]
               name  active
         0  grocery    True
-        1  default   False
 
         See Also
         --------
         - [Catalog.create](/reference/featurebyte.api.catalog.Catalog.create/): Create Catalog
         """
         try:
             catalog = Catalog.get(name=name)
             activate_catalog(catalog.id)
             return catalog
         except RecordRetrievalException:
             return Catalog.create(name=name, feature_store_name=feature_store_name)
 
     @classmethod
-    def get_active(cls) -> Catalog:
+    def get_active(cls) -> Optional[Catalog]:
         """
         Gets the currently active catalog.
 
         Returns
         -------
-        Catalog
-            The currently active catalog.
+        Optional[Catalog]
+            The currently active catalog or None if no catalog is active.
 
         Examples
         --------
         >>> catalog = fb.Catalog.get_active()
         >>> catalog.name
         'grocery'
         """
-        return cls.get_by_id(get_active_catalog_id())
+        active_catalog_id = get_active_catalog_id()
+        if not active_catalog_id:
+            return None
+        return cls.get_by_id(active_catalog_id)
 
     @classmethod
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        item_list = super()._post_process_list(item_list)
-
-        # add column to indicate whether catalog is active
-        item_list["active"] = item_list.id == get_active_catalog_id()
-
-        return item_list
+    def _list_handler(cls) -> ListHandler:
+        return CatalogListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
+        )
 
     @typechecked
     def update_name(self, name: str) -> None:
         """
         Updates the catalog name.
 
         Parameters
@@ -517,14 +525,35 @@
         Examples
         --------
         >>> tables = catalog.list_tables()
         """
         return Table.list(include_id=include_id, entity=entity)
 
     @update_and_reset_catalog
+    def list_targets(self, include_id: Optional[bool] = True) -> pd.DataFrame:
+        """
+        Returns a DataFrame that contains various attributes of the registered targets in the catalog
+
+        Parameters
+        ----------
+        include_id: Optional[bool]
+            Whether to include id in the list.
+
+        Returns
+        -------
+        pd.DataFrame
+            Dataframe of targets
+
+        Examples
+        --------
+        >>> targets = catalog.list_targets()
+        """
+        return Target.list(include_id=include_id)
+
+    @update_and_reset_catalog
     def list_relationships(
         self, include_id: Optional[bool] = True, relationship_type: Optional[Literal[tuple(RelationshipType)]] = None  # type: ignore
     ) -> pd.DataFrame:
         """
         List all relationships that exist in your FeatureByte instance, or filtered by relationship type.
 
         This provides a dataframe with:
@@ -752,14 +781,37 @@
         List saved batch feature tables.
 
         >>> batch_feature_tables = catalog.list_batch_feature_tables()
         """
         return BatchFeatureTable.list(include_id=include_id)
 
     @update_and_reset_catalog
+    def list_static_source_tables(self, include_id: Optional[bool] = True) -> pd.DataFrame:
+        """
+        List saved static source tables.
+
+        Parameters
+        ----------
+        include_id: Optional[bool]
+            Whether to include id in the list.
+
+        Returns
+        -------
+        pd.DataFrame
+            Table of static source tables.
+
+        Examples
+        --------
+        List saved static source tables.
+
+        >>> static_source_tables = catalog.list_static_source_tables()
+        """
+        return StaticSourceTable.list(include_id=include_id)
+
+    @update_and_reset_catalog
     def list_deployments(self, include_id: Optional[bool] = True) -> pd.DataFrame:
         """
         List saved deployments.
 
         Parameters
         ----------
         include_id: Optional[bool]
@@ -775,14 +827,60 @@
         List saved deployments.
 
         >>> deployments = catalog.list_deployments()
         """
         return Deployment.list(include_id=include_id)
 
     @update_and_reset_catalog
+    def list_user_defined_functions(self, include_id: Optional[bool] = True) -> pd.DataFrame:
+        """
+        List saved user defined functions.
+
+        Parameters
+        ----------
+        include_id: Optional[bool]
+            Whether to include id in the list.
+
+        Returns
+        -------
+        pd.DataFrame
+            Table of user defined functions.
+
+        Examples
+        --------
+        List saved user defined functions.
+
+        >>> user_defined_functions = catalog.list_user_defined_functions()
+        """
+        return UserDefinedFunction.list(include_id=include_id)
+
+    @update_and_reset_catalog
+    def list_target_tables(self, include_id: Optional[bool] = True) -> pd.DataFrame:
+        """
+        List saved target tables.
+
+        Parameters
+        ----------
+        include_id: Optional[bool]
+            Whether to include id in the list.
+
+        Returns
+        -------
+        pd.DataFrame
+            Table of target tables.
+
+        Examples
+        --------
+        List saved target tables.
+
+        >>> target_tables = catalog.list_target_tables()
+        """
+        return TargetTable.list(include_id=include_id)
+
+    @update_and_reset_catalog
     def get_data_source(self) -> DataSource:
         """
         Gets the data source from the catalog to access source tables from the data warehouse.
 
         Returns
         -------
         DataSource
@@ -905,14 +1003,35 @@
         Examples
         --------
         >>> item_table = catalog.get_table("INVOICEITEMS")
         """
         return Table.get(name=name)
 
     @update_and_reset_catalog
+    def get_target(self, name: str) -> Any:
+        """
+        Gets a Target object from the catalog based on its name.
+
+        Parameters
+        ----------
+        name: str
+            Target name.
+
+        Returns
+        -------
+        Any
+            Retrieved target.
+
+        Examples
+        --------
+        >>> target = catalog.get_target("target_name")  # doctest: +SKIP
+        """
+        return Target.get(name=name)
+
+    @update_and_reset_catalog
     def get_relationship(self, name: str) -> Relationship:
         """
         Gets a Relationship object by name.
 
         Parameters
         ----------
         name: str
@@ -1132,7 +1251,76 @@
         Examples
         --------
         Get a saved batch feature table.
 
         >>> batch_feature_table = catalog.get_batch_feature_table("batch_feature_table_name") # doctest: +SKIP
         """
         return BatchFeatureTable.get(name=name)
+
+    @update_and_reset_catalog
+    def get_static_source_table(self, name: str) -> StaticSourceTable:
+        """
+        Get static source table by name.
+
+        Parameters
+        ----------
+        name: str
+            Static source table name.
+
+        Returns
+        -------
+        StaticSourceTable
+            Static source table object.
+
+        Examples
+        --------
+        Get a saved static source table.
+
+        >>> static_source_table = catalog.get_static_source_table("static_source_table_name")  # doctest: +SKIP
+        """
+        return StaticSourceTable.get(name=name)
+
+    @update_and_reset_catalog
+    def get_user_defined_function(self, name: str) -> UserDefinedFunction:
+        """
+        Get user defined function by name.
+
+        Parameters
+        ----------
+        name: str
+            User defined function name.
+
+        Returns
+        -------
+        UserDefinedFunction
+            User defined function object.
+
+        Examples
+        --------
+        Get a saved user defined function.
+
+        >>> user_defined_function = catalog.get_user_defined_function("user_defined_function_name")  # doctest: +SKIP
+        """
+        return UserDefinedFunction.get(name=name)
+
+    @update_and_reset_catalog
+    def get_target_table(self, name: str) -> TargetTable:
+        """
+        Get target table by name.
+
+        Parameters
+        ----------
+        name: str
+            Target table name.
+
+        Returns
+        -------
+        TargetTable
+            Target table object.
+
+        Examples
+        --------
+        Get a saved target table.
+
+        >>> target_table = catalog.get_target_table("target_table_name")  # doctest: +SKIP
+        """
+        return TargetTable.get(name=name)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/catalog_decorator.py` & `featurebyte-0.4.0/featurebyte/api/catalog_decorator.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/catalog_get_by_id_mixin.py` & `featurebyte-0.4.0/featurebyte/api/catalog_get_by_id_mixin.py`

 * *Files 12% similar despite different names*

```diff
@@ -17,15 +17,19 @@
 from featurebyte.api.feature_job_setting_analysis import FeatureJobSettingAnalysis
 from featurebyte.api.feature_list import FeatureList
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.historical_feature_table import HistoricalFeatureTable
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.api.periodic_task import PeriodicTask
 from featurebyte.api.relationship import Relationship
+from featurebyte.api.static_source_table import StaticSourceTable
 from featurebyte.api.table import Table
+from featurebyte.api.target import Target
+from featurebyte.api.target_table import TargetTable
+from featurebyte.api.user_defined_function import UserDefinedFunction
 from featurebyte.api.view import View
 
 
 class CatalogGetByIdMixin:
     """
     Mixin to add get_by_id functionality into the catalog.
     """
@@ -379,14 +383,39 @@
         Get a saved batch feature table.
 
         >>> batch_feature_table = catalog.get_batch_feature_table_by_id(ObjectId())  # doctest: +SKIP
         """
         return BatchFeatureTable.get_by_id(id=id)
 
     @update_and_reset_catalog
+    def get_static_source_table_by_id(
+        self, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
+    ) -> StaticSourceTable:
+        """
+        Get static source table by id.
+
+        Parameters
+        ----------
+        id: ObjectId
+            Static source table id.
+
+        Returns
+        -------
+        StaticSourceTable
+            Static source table object.
+
+        Examples
+        --------
+        Get a saved static source table.
+
+        >>> static_source_table = catalog.get_static_source_table_by_id(ObjectId())  # doctest: +SKIP
+        """
+        return StaticSourceTable.get_by_id(id=id)
+
+    @update_and_reset_catalog
     def get_deployment_by_id(
         self, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
     ) -> Deployment:
         """
         Get deployment by id.
 
         Parameters
@@ -402,7 +431,82 @@
         Examples
         --------
         Get a saved deployment.
 
         >>> deployment = catalog.get_deployment_by_id(ObjectId())  # doctest: +SKIP
         """
         return Deployment.get_by_id(id=id)
+
+    @update_and_reset_catalog
+    def get_user_defined_function_by_id(
+        self, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
+    ) -> UserDefinedFunction:
+        """
+        Get user defined function by id.
+
+        Parameters
+        ----------
+        id: ObjectId
+            User defined function id.
+
+        Returns
+        -------
+        UserDefinedFunction
+            User defined function object.
+
+        Examples
+        --------
+        Get a saved user defined function.
+
+        >>> user_defined_function = catalog.get_user_defined_function_by_id(ObjectId())  # doctest: +SKIP
+        """
+        return UserDefinedFunction.get_by_id(id=id)
+
+    @update_and_reset_catalog
+    def get_target_by_id(
+        self, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
+    ) -> Target:
+        """
+        Get target by id.
+
+        Parameters
+        ----------
+        id: ObjectId
+            Target id.
+
+        Returns
+        -------
+        Target
+            Target object.
+
+        Examples
+        --------
+        Get a saved target .
+
+        >>> target = catalog.get_target_by_id(ObjectId())  # doctest: +SKIP
+        """
+        return Target.get_by_id(id=id)
+
+    @update_and_reset_catalog
+    def get_target_table_by_id(
+        self, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
+    ) -> TargetTable:
+        """
+        Get target table by id.
+
+        Parameters
+        ----------
+        id: ObjectId
+            Target table id.
+
+        Returns
+        -------
+        TargetTable
+            Target table object.
+
+        Examples
+        --------
+        Get a saved target table.
+
+        >>> target_table = catalog.get_target_table_by_id(ObjectId())  # doctest: +SKIP
+        """
+        return TargetTable.get_by_id(id=id)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/change_view.py` & `featurebyte-0.4.0/featurebyte/api/change_view.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/credential.py` & `featurebyte-0.4.0/featurebyte/api/credential.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import Field
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ForeignKeyMapping
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.savable_api_object import DeletableApiObject, SavableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.credential import (
     DatabaseCredential,
     DatabaseCredentialType,
@@ -66,15 +66,15 @@
     # pydantic instance variable (public)
     feature_store_id: PydanticObjectId = Field(
         allow_mutation=False,
         description="Id of the feature store that the credential is associated with.",
     )
 
     def _get_create_payload(self) -> Dict[str, Any]:
-        data = CredentialCreate(**self.json_dict())
+        data = CredentialCreate(**self.dict(by_alias=True))
         return data.json_dict()
 
     @property
     def database_credential_type(self) -> Optional[DatabaseCredentialType]:
         """
         Get the database credential type.
```

### Comparing `featurebyte-0.3.1/featurebyte/api/data_source.py` & `featurebyte-0.4.0/featurebyte/api/data_source.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
 from featurebyte.api.source_table import SourceTable
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.config import Configurations
 from featurebyte.enum import SourceType
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.models.feature_store import FeatureStoreModel
+from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.node.schema import TableDetails
 
 
 class DataSource:
     """
     DataSource class to represent a data source in FeatureByte.
@@ -158,14 +159,37 @@
             url=f"/feature_store/table?database_name={database_name}&schema_name={schema_name}",
             json=self._feature_store.json_dict(),
         )
         if response.status_code == HTTPStatus.OK:
             return cast(List[str], response.json())
         raise RecordRetrievalException(response)
 
+    def _construct_columns_info(
+        self,
+        table_name: str,
+        database_name: Optional[str] = None,
+        schema_name: Optional[str] = None,
+    ) -> List[ColumnInfo]:
+        client = Configurations().get_client()
+        response = client.post(
+            url=(
+                f"/feature_store/column?"
+                f"database_name={database_name}&"
+                f"schema_name={schema_name}&"
+                f"table_name={table_name}"
+            ),
+            json=self._feature_store.json_dict(),
+        )
+        if response.status_code != HTTPStatus.OK:
+            raise RecordRetrievalException(response)
+
+        # parse response & return column info
+        column_specs = response.json()
+        return [ColumnInfo(**dict(col)) for col in column_specs]
+
     @typechecked
     def get_source_table(
         self,
         table_name: str,
         database_name: Optional[str] = None,
         schema_name: Optional[str] = None,
     ) -> SourceTable:
@@ -198,18 +222,24 @@
         >>> source_table.columns
         ['GroceryInvoiceGuid', 'GroceryCustomerGuid', 'Timestamp', 'record_available_at', 'Amount']
 
         See Also
         --------
         - [SourceTable](/reference/featurebyte.api.source_table.SourceTable/): SourceTable
         """
+        columns_info = self._construct_columns_info(
+            table_name=table_name,
+            database_name=database_name,
+            schema_name=schema_name,
+        )
         return SourceTable(
             feature_store=self._feature_store,
             tabular_source=TabularSource(
                 feature_store_id=self._feature_store.id,
                 table_details=TableDetails(
                     database_name=database_name,
                     schema_name=schema_name,
                     table_name=table_name,
                 ),
             ),
+            columns_info=columns_info,
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/api/deployment.py` & `featurebyte-0.4.0/featurebyte/service/historical_features.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,387 +1,360 @@
 """
-Deployment module
+HistoricalFeaturesService
 """
 from __future__ import annotations
 
-from typing import Literal, Optional
+from typing import Callable, Optional, Union
 
-import json
-import os
+import time
+from dataclasses import dataclass
 
 import pandas as pd
 from bson import ObjectId
-from jinja2 import Template
-from typeguard import typechecked
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
-from featurebyte.api.batch_feature_table import BatchFeatureTable
-from featurebyte.api.batch_request_table import BatchRequestTable
-from featurebyte.api.entity import Entity
-from featurebyte.api.feature_job import FeatureJobStatusResult
-from featurebyte.api.feature_list import FeatureList
-from featurebyte.api.table import Table
-from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.formatting_util import CodeStr
-from featurebyte.config import Configurations
-from featurebyte.exception import FeatureListNotOnlineEnabledError
-from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.deployment import DeploymentModel
-from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
-from featurebyte.schema.deployment import DeploymentUpdate
-
-
-class Deployment(ApiObject):
+from featurebyte.common.progress import get_ranged_progress_callback
+from featurebyte.exception import DocumentNotFoundError
+from featurebyte.logging import get_logger
+from featurebyte.models.feature_store import FeatureStoreModel
+from featurebyte.models.observation_table import ObservationTableModel
+from featurebyte.models.parent_serving import ParentServingPreparation
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.node.schema import TableDetails
+from featurebyte.query_graph.sql.common import REQUEST_TABLE_NAME, sql_to_string
+from featurebyte.query_graph.sql.feature_historical import (
+    NUM_FEATURES_PER_QUERY,
+    PROGRESS_MESSAGE_COMPUTING_FEATURES,
+    TILE_COMPUTE_PROGRESS_MAX_PERCENT,
+    get_feature_names,
+    get_historical_features_query_set,
+    get_internal_observation_set,
+    split_nodes,
+    validate_historical_requests_point_in_time,
+    validate_request_schema,
+)
+from featurebyte.query_graph.sql.parent_serving import construct_request_table_with_parent_entities
+from featurebyte.routes.common.feature_or_target_table import ValidationParameters
+from featurebyte.schema.feature_list import FeatureListGetHistoricalFeatures
+from featurebyte.service.entity_validation import EntityValidationService
+from featurebyte.service.feature_list import FeatureListService
+from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.service.target_helper.base_feature_or_target_computer import (
+    BasicExecutorParams,
+    Computer,
+    ExecutorParams,
+    QueryExecutor,
+)
+from featurebyte.service.tile_cache import TileCacheService
+from featurebyte.session.base import BaseSession
+
+logger = get_logger(__name__)
+
+
+async def compute_tiles_on_demand(  # pylint: disable=too-many-arguments
+    session: BaseSession,
+    tile_cache_service: TileCacheService,
+    graph: QueryGraph,
+    nodes: list[Node],
+    request_id: str,
+    request_table_name: str,
+    request_table_columns: list[str],
+    feature_store_id: ObjectId,
+    serving_names_mapping: Optional[dict[str, str]],
+    parent_serving_preparation: Optional[ParentServingPreparation] = None,
+    progress_callback: Optional[Callable[[int, str], None]] = None,
+) -> None:
     """
-    A FeatureByte Catalog serves as a centralized repository for storing metadata about FeatureByte objects such as
-    tables, entities, features, and feature lists associated with a specific domain. It functions as an effective tool
-    for facilitating collaboration among team members working on similar use cases or utilizing the same data source
-    within a data warehouse.
-
-    By employing a catalog, team members can effortlessly search, retrieve, and reuse the necessary tables,
-    entities, features, and feature lists while obtaining comprehensive information about their properties.
-    This information includes their type, creation date, related versions, status, and other descriptive details.
+    Compute tiles on demand
 
-    For data warehouses covering multiple domains, creating multiple catalogs can help maintain organization and
-    simplify management of the data and features.
+    Parameters
+    ----------
+    session: BaseSession
+        Session to use to make queries
+    tile_cache_service: TileCacheService
+        Tile cache service
+    graph: QueryGraph
+        Query graph
+    nodes: list[Node]
+        List of query graph node
+    request_id: str
+        Request ID to be used as suffix of table names when creating temporary tables
+    request_table_name: str
+        Name of request table
+    feature_store_id: ObjectId
+        Feature store id
+    request_table_columns: list[str]
+        List of column names in the observations set
+    serving_names_mapping : dict[str, str] | None
+        Optional serving names mapping if the training events data has different serving name
+        columns than those defined in Entities
+    parent_serving_preparation: Optional[ParentServingPreparation]
+        Preparation required for serving parent features
+    progress_callback: Optional[Callable[[int, str], None]]
+        Optional progress callback function
     """
+    if parent_serving_preparation is None:
+        effective_request_table_name = request_table_name
+    else:
+        # Lookup parent entities and join them with the request table since tile computation
+        # requires these entity columns to be present in the request table.
+        parent_serving_result = construct_request_table_with_parent_entities(
+            request_table_name=request_table_name,
+            request_table_columns=request_table_columns,
+            join_steps=parent_serving_preparation.join_steps,
+            feature_store_details=parent_serving_preparation.feature_store_details,
+        )
+        request_table_query = sql_to_string(parent_serving_result.table_expr, session.source_type)
+        effective_request_table_name = parent_serving_result.new_request_table_name
+        await session.register_table_with_query(
+            effective_request_table_name,
+            request_table_query,
+        )
 
-    __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.Deployment")
+    await tile_cache_service.compute_tiles_on_demand(
+        session=session,
+        graph=graph,
+        nodes=nodes,
+        request_id=request_id,
+        request_table_name=effective_request_table_name,
+        feature_store_id=feature_store_id,
+        serving_names_mapping=serving_names_mapping,
+        progress_callback=progress_callback,
+    )
+
+
+async def get_historical_features(  # pylint: disable=too-many-locals, too-many-arguments
+    session: BaseSession,
+    tile_cache_service: TileCacheService,
+    graph: QueryGraph,
+    nodes: list[Node],
+    observation_set: Union[pd.DataFrame, ObservationTableModel],
+    feature_store: FeatureStoreModel,
+    output_table_details: TableDetails,
+    serving_names_mapping: dict[str, str] | None = None,
+    is_feature_list_deployed: bool = False,
+    parent_serving_preparation: Optional[ParentServingPreparation] = None,
+    progress_callback: Optional[Callable[[int, str], None]] = None,
+) -> None:
+    """Get historical features
+
+    Parameters
+    ----------
+    session: BaseSession
+        Session to use to make queries
+    tile_cache_service: TileCacheService
+        Tile cache service
+    graph : QueryGraph
+        Query graph
+    nodes : list[Node]
+        List of query graph node
+    observation_set : Union[pd.DataFrame, ObservationTableModel]
+        Observation set
+    feature_store: FeatureStoreModel
+        Feature store. We need the feature store id and source type information.
+    serving_names_mapping : dict[str, str] | None
+        Optional serving names mapping if the observations set has different serving name columns
+        than those defined in Entities
+    is_feature_list_deployed : bool
+        Whether the feature list that triggered this historical request is deployed. If so, tile
+        tables would have already been back-filled and there is no need to check and calculate tiles
+        on demand.
+    parent_serving_preparation: Optional[ParentServingPreparation]
+        Preparation required for serving parent features
+    output_table_details: TableDetails
+        Output table details to write the results to
+    progress_callback: Optional[Callable[[int, str], None]]
+        Optional progress callback function
+    """
+    tic_ = time.time()
 
-    # class variables
-    _route = "/deployment"
-    _list_schema = DeploymentModel
-    _get_schema = DeploymentModel
-    _update_schema_class = DeploymentUpdate
-    _list_fields = [
-        "name",
-        "feature_list_name",
-        "feature_list_version",
-        "num_feature",
-        "enabled",
-    ]
-    _list_foreign_keys = [
-        ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_name", "name", True),
-        ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_version", "version", True),
-        ForeignKeyMapping("feature_list_id", FeatureList, "num_feature", "num_feature", True),
-    ]
-
-    @property
-    def enabled(self) -> bool:
-        """
-        Deployment enabled status
-
-        Returns
-        -------
-        bool
-        """
-        return self.cached_model.enabled
-
-    @property
-    def feature_list_id(self) -> PydanticObjectId:
-        """
-        Feature list ID associated with this deployment.
-
-        Returns
-        -------
-        PydanticObjectId
-        """
-        return self.cached_model.feature_list_id
-
-    def enable(self) -> None:
-        """
-        Enable the deployment.
-
-        Examples
-        --------
-        >>> deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
-        >>> deployment.enable()  # doctest: +SKIP
-        """
-        self.patch_async_task(route=f"{self._route}/{self.id}", payload={"enabled": True})
-
-    def disable(self) -> None:
-        """
-        Disable the deployment.
-
-        Examples
-        --------
-        >>> deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
-        >>> deployment.disable()  # doctest: +SKIP
-        """
-        self.patch_async_task(route=f"{self._route}/{self.id}", payload={"enabled": False})
+    observation_set = get_internal_observation_set(observation_set)
 
-    @typechecked
-    def compute_batch_feature_table(
-        self,
-        batch_request_table: BatchRequestTable,
-        batch_feature_table_name: str,
-    ) -> BatchFeatureTable:
-        """
-        Get batch features asynchronously using a batch request table. The batch request features
-        will be materialized into a batch feature table.
-
-        Parameters
-        ----------
-        batch_request_table: BatchRequestTable
-            Batch request table contains required serving names columns
-        batch_feature_table_name: str
-            Name of the batch feature table to be created
-
-        Returns
-        -------
-        BatchFeatureTable
-
-        Examples
-        --------
-        >>. deployment = catalog.get_deployment(<deployment_name>)  # doctest: +SKIP
-        >>> batch_features = deployment.compute_batch_feature_table(  # doctest: +SKIP
-        ...   batch_request_table=batch_request_table,
-        ...   batch_feature_table_name = <batch_feature_table_name>
-        ... )
-        """
-        payload = BatchFeatureTableCreate(
-            name=batch_feature_table_name,
-            feature_store_id=batch_request_table.location.feature_store_id,
-            batch_request_table_id=batch_request_table.id,
-            deployment_id=self.id,
+    # Validate request
+    validate_request_schema(observation_set)
+    validate_historical_requests_point_in_time(observation_set)
+
+    # use a unique request table name
+    request_id = session.generate_session_unique_id()
+    request_table_name = f"{REQUEST_TABLE_NAME}_{request_id}"
+    request_table_columns = observation_set.columns
+
+    # Execute feature SQL code
+    await observation_set.register_as_request_table(
+        session, request_table_name, add_row_index=len(nodes) > NUM_FEATURES_PER_QUERY
+    )
+
+    # Compute tiles on demand if required
+    if not is_feature_list_deployed:
+        tile_cache_progress_callback = (
+            get_ranged_progress_callback(
+                progress_callback,
+                0,
+                TILE_COMPUTE_PROGRESS_MAX_PERCENT,
+            )
+            if progress_callback
+            else None
         )
-        batch_feature_table_doc = self.post_async_task(
-            route="/batch_feature_table", payload=payload.json_dict()
+        tic = time.time()
+        # Process nodes in batches
+        tile_cache_node_groups = split_nodes(
+            graph, nodes, NUM_FEATURES_PER_QUERY, is_tile_cache=True
         )
-        return BatchFeatureTable.get_by_id(batch_feature_table_doc["_id"])
-
-    def get_online_serving_code(self, language: Literal["python", "sh"] = "python") -> str:
-        """
-        Retrieves either Python or shell script template for serving online features from a deployed featurelist,
-        defaulted to python.
-
-        Parameters
-        ----------
-        language: Literal["python", "sh"]
-            Language for which to get code template
-
-        Returns
-        -------
-        str
-
-        Raises
-        ------
-        FeatureListNotOnlineEnabledError
-            Feature list not deployed
-        NotImplementedError
-            Serving code not available
-
-        Examples
-        --------
-        Retrieve python code template when "language" is set to "python"
-
-        >>> feature_list = catalog.get_feature_list("invoice_feature_list")
-        >>> deployment = feature_list.deploy()  # doctest: +SKIP
-        >>> deployment.enable()  # doctest: +SKIP
-        >>> deployment.get_online_serving_code(language="python")  # doctest: +SKIP
-            from typing import Any, Dict
-            import pandas as pd
-            import requests
-            def request_features(entity_serving_names: Dict[str, Any]) -> pd.DataFrame:
-                "
-                Send POST request to online serving endpoint
-                Parameters
-                ----------
-                entity_serving_names: Dict[str, Any]
-                    Entity serving name values to used for serving request
-                Returns
-                -------
-                pd.DataFrame
-                "
-                response = requests.post(
-                    url="http://localhost:8080/deployment/{deployment.id}/online_features",
-                    headers={{"Content-Type": "application/json", "active-catalog-id": "{catalog.id}"}},
-                    json={{"entity_serving_names": entity_serving_names}},
+        for i, _nodes in enumerate(tile_cache_node_groups):
+            logger.debug("Checking and computing tiles on demand for %d nodes", len(_nodes))
+            await compute_tiles_on_demand(
+                session=session,
+                tile_cache_service=tile_cache_service,
+                graph=graph,
+                nodes=_nodes,
+                request_id=request_id,
+                request_table_name=request_table_name,
+                request_table_columns=request_table_columns,
+                feature_store_id=feature_store.id,
+                serving_names_mapping=serving_names_mapping,
+                parent_serving_preparation=parent_serving_preparation,
+                progress_callback=get_ranged_progress_callback(
+                    tile_cache_progress_callback,
+                    100 * i / len(tile_cache_node_groups),
+                    100 * (i + 1) / len(tile_cache_node_groups),
                 )
-                assert response.status_code == 200, response.json()
-                return pd.DataFrame.from_dict(response.json()["features"])
-            request_features([{{"cust_id": "sample_cust_id"}}])
-
-        Retrieve shell script template when "language" is set to "sh"
-
-        >>> feature_list = catalog.get_feature_list("invoice_feature_list")
-        >>> deployment = feature_list.deploy()  # doctest: +SKIP
-        >>> deployment.enable()  # doctest: +SKIP
-        >>> deployment.get_online_serving_code(language="sh")  # doctest: +SKIP
-            \\#!/bin/sh
-            curl -X POST
-                -H 'Content-Type: application/json' \\
-                -H 'Authorization: Bearer token' \\
-                -H 'active-catalog-id: {catalog.id}' \\
-                -d '{{"entity_serving_names": [{{"cust_id": "sample_cust_id"}}]}}' \\
-                http://localhost:8080/deployment/{deployment.id}/online_features
-
-        See Also
-        --------
-        - [FeatureList.deploy](/reference/featurebyte.api.feature_list.FeatureList.deploy/)
-        - [Deployment.enable](/reference/featurebyte.api.deployment.Deployment.enable/)
-        """
-        # pylint: disable=too-many-locals
-        if not self.enabled:
-            raise FeatureListNotOnlineEnabledError("Deployment is not enabled.")
-
-        templates = {"python": "python.tpl", "sh": "shell.tpl"}
-        template_file = templates.get(language)
-        if not template_file:
-            raise NotImplementedError(f"Supported languages: {list(templates.keys())}")
-
-        # get entities and tables used for the feature list
-        num_rows = 1
-        feature_list = FeatureList.get_by_id(self.feature_list_id)
-        feature_list_info = feature_list.info()
-        entities = {
-            Entity.get(entity["name"]).id: {"serving_name": entity["serving_names"]}
-            for entity in feature_list_info["primary_entity"]
-        }
-        for tabular_source in feature_list_info["tables"]:
-            data = Table.get(tabular_source["name"])
-            entity_columns = [
-                column for column in data.columns_info if column.entity_id in entities
-            ]
-            if entity_columns:
-                sample_data = data.preview(num_rows)
-                for column in entity_columns:
-                    entities[column.entity_id]["sample_value"] = sample_data[column.name].to_list()
-
-        entity_serving_names = json.dumps(
-            [
-                {
-                    entity["serving_name"][0]: entity["sample_value"][row_idx]
-                    for entity in entities.values()
-                }
-                for row_idx in range(num_rows)
-            ]
+                if tile_cache_progress_callback
+                else None,
+            )
+
+        elapsed = time.time() - tic
+        logger.debug("Done checking and computing tiles on demand", extra={"duration": elapsed})
+
+    if progress_callback:
+        progress_callback(TILE_COMPUTE_PROGRESS_MAX_PERCENT, PROGRESS_MESSAGE_COMPUTING_FEATURES)
+
+    # Generate SQL code that computes the features
+    historical_feature_query_set = get_historical_features_query_set(
+        graph=graph,
+        nodes=nodes,
+        request_table_columns=request_table_columns,
+        serving_names_mapping=serving_names_mapping,
+        source_type=feature_store.type,
+        output_table_details=output_table_details,
+        output_feature_names=get_feature_names(graph, nodes),
+        request_table_name=request_table_name,
+        parent_serving_preparation=parent_serving_preparation,
+    )
+    await historical_feature_query_set.execute(
+        session,
+        get_ranged_progress_callback(
+            progress_callback,
+            TILE_COMPUTE_PROGRESS_MAX_PERCENT,
+            100,
         )
+        if progress_callback
+        else None,
+    )
+    logger.debug(f"compute_historical_features in total took {time.time() - tic_:.2f}s")
 
-        # construct serving url
-        current_profile = Configurations().profile
-        assert current_profile
-        info = self.info()
-        serving_endpoint = info["serving_endpoint"]
-        headers = {
-            "Content-Type": "application/json",
-            "active-catalog-id": str(feature_list.catalog_id),
-        }
-        if current_profile.api_token:
-            headers["Authorization"] = f"Bearer {current_profile.api_token}"
-        header_params = " \\\n    ".join([f"-H '{key}: {value}'" for key, value in headers.items()])
-        serving_url = f"{current_profile.api_url}{serving_endpoint}"
-
-        # populate template
-        with open(
-            file=os.path.join(
-                os.path.dirname(__file__), f"templates/online_serving/{template_file}"
-            ),
-            mode="r",
-            encoding="utf-8",
-        ) as file_object:
-            template = Template(file_object.read())
-
-        return CodeStr(
-            template.render(
-                headers=json.dumps(headers),
-                header_params=header_params,
-                serving_url=serving_url,
-                entity_serving_names=entity_serving_names,
-            )
+
+@dataclass
+class HistoricalFeatureExecutorParams(ExecutorParams):
+    """
+    Historical feature executor params
+    """
+
+    # Whether the feature list that triggered this historical request is deployed. If so, tile
+    # tables would have already been back-filled and there is no need to check and calculate tiles
+    # on demand.
+    is_feature_list_deployed: bool = False
+
+
+class HistoricalFeatureExecutor(QueryExecutor[HistoricalFeatureExecutorParams]):
+    """
+    Historical feature Executor
+    """
+
+    def __init__(self, tile_cache_service: TileCacheService):
+        self.tile_cache_service = tile_cache_service
+
+    async def execute(self, executor_params: HistoricalFeatureExecutorParams) -> None:
+        await get_historical_features(
+            session=executor_params.session,
+            tile_cache_service=self.tile_cache_service,
+            graph=executor_params.graph,
+            nodes=executor_params.nodes,
+            observation_set=executor_params.observation_set,
+            serving_names_mapping=executor_params.serving_names_mapping,
+            feature_store=executor_params.feature_store,
+            is_feature_list_deployed=executor_params.is_feature_list_deployed,
+            parent_serving_preparation=executor_params.parent_serving_preparation,
+            output_table_details=executor_params.output_table_details,
+            progress_callback=executor_params.progress_callback,
         )
 
-    def get_feature_jobs_status(
+
+class HistoricalFeaturesService(
+    Computer[FeatureListGetHistoricalFeatures, HistoricalFeatureExecutorParams]
+):
+    """
+    HistoricalFeaturesService is responsible for requesting for historical features for a Feature List.
+    """
+
+    def __init__(
         self,
-        job_history_window: int = 1,
-        job_duration_tolerance: int = 60,
-    ) -> FeatureJobStatusResult:
-        """
-        Get the status of feature jobs in the associated feature list used for the deployment.
-
-        Parameters
-        ----------
-        job_history_window: int
-            History window in hours.
-        job_duration_tolerance: int
-            Maximum duration before job is considered later, in seconds.
-
-        Returns
-        -------
-        FeatureJobStatusResult
-        """
-        return FeatureList.get_by_id(self.feature_list_id).get_feature_jobs_status(
-            job_history_window=job_history_window,
-            job_duration_tolerance=job_duration_tolerance,
+        feature_store_service: FeatureStoreService,
+        entity_validation_service: EntityValidationService,
+        session_manager_service: SessionManagerService,
+        query_executor: QueryExecutor[HistoricalFeatureExecutorParams],
+        feature_list_service: FeatureListService,
+    ):
+        super().__init__(
+            feature_store_service,
+            entity_validation_service,
+            session_manager_service,
+            query_executor,
+        )
+        self.feature_list_service = feature_list_service
+
+    async def get_validation_parameters(
+        self, request: FeatureListGetHistoricalFeatures
+    ) -> ValidationParameters:
+        # multiple feature stores not supported
+        feature_clusters = request.feature_clusters
+        assert len(feature_clusters) == 1
+
+        feature_cluster = feature_clusters[0]
+        feature_store = await self.feature_store_service.get_document(
+            document_id=feature_cluster.feature_store_id
+        )
+        return ValidationParameters(
+            graph=feature_cluster.graph,
+            nodes=feature_cluster.nodes,
+            feature_store=feature_store,
+            serving_names_mapping=request.serving_names_mapping,
         )
 
-    @classmethod
-    def get(cls, name: str) -> Deployment:
-        """
-        Gets a Deployment object by its name.
-
-        Parameters
-        ----------
-        name: str
-            Name of the deployment to retrieve.
-
-        Returns
-        -------
-        Deployment
-            Deployment object.
-
-        Examples
-        --------
-        Get a Deployment object that is already saved.
-
-        >>> deployment = fb.Deployment.get(<deployment_name>)  # doctest: +SKIP
-        """
-        return super().get(name)
-
-    @classmethod
-    def get_by_id(
-        cls, id: ObjectId  # pylint: disable=redefined-builtin,invalid-name
-    ) -> Deployment:
-        """
-        Returns a Deployment object by its unique identifier (ID).
-
-        Parameters
-        ----------
-        id: ObjectId
-            Deployment unique identifier ID.
-
-        Returns
-        -------
-        Deployment
-            Deployment object.
-
-        Examples
-        --------
-        Get a Deployment object that is already saved.
-
-        >>> fb.Deployment.get_by_id(<deployment_id>)  # doctest: +SKIP
-        """
-        return cls._get_by_id(id=id)
-
-    @classmethod
-    def list(cls, include_id: Optional[bool] = True) -> pd.DataFrame:
-        """
-        Returns a DataFrame that lists the deployments by their names, feature list names, feature list versions,
-        number of features, and whether the features are enabled.
-
-        Parameters
-        ----------
-        include_id: Optional[bool]
-            Whether to include id in the list.
-
-        Returns
-        -------
-        DataFrame
-            Table of objects.
-
-        Examples
-        --------
-        List all deployments.
-
-        >>> deployments = fb.Deployment.list()
-        """
-        return super().list(include_id=include_id)
+    async def get_executor_params(
+        self,
+        request: FeatureListGetHistoricalFeatures,
+        basic_executor_params: BasicExecutorParams,
+        validation_parameters: ValidationParameters,
+    ) -> HistoricalFeatureExecutorParams:
+        feature_list_id = request.feature_list_id
+        try:
+            if feature_list_id is None:
+                is_feature_list_deployed = False
+            else:
+                feature_list = await self.feature_list_service.get_document(feature_list_id)
+                is_feature_list_deployed = feature_list.deployed
+        except DocumentNotFoundError:
+            is_feature_list_deployed = False
+
+        return HistoricalFeatureExecutorParams(
+            session=basic_executor_params.session,
+            output_table_details=basic_executor_params.output_table_details,
+            parent_serving_preparation=basic_executor_params.parent_serving_preparation,
+            progress_callback=basic_executor_params.progress_callback,
+            observation_set=basic_executor_params.observation_set,
+            graph=validation_parameters.graph,
+            nodes=validation_parameters.nodes,
+            serving_names_mapping=validation_parameters.serving_names_mapping,
+            feature_store=validation_parameters.feature_store,
+            is_feature_list_deployed=is_feature_list_deployed,
+        )
```

### Comparing `featurebyte-0.3.1/featurebyte/api/dimension_table.py` & `featurebyte-0.4.0/featurebyte/api/dimension_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/dimension_view.py` & `featurebyte-0.4.0/featurebyte/api/dimension_view.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/entity.py` & `featurebyte-0.4.0/featurebyte/api/entity.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,25 +1,22 @@
 """
 Entity class
 """
 from __future__ import annotations
 
 from typing import Any, Dict, List
 
-from http import HTTPStatus
-
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
 
 from featurebyte.api.api_object_util import NameAttributeUpdatableMixin
 from featurebyte.api.savable_api_object import SavableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.config import Configurations
-from featurebyte.exception import RecordRetrievalException, RecordUpdateException
+from featurebyte.exception import RecordRetrievalException
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.entity import EntityModel, ParentEntity
 from featurebyte.schema.entity import EntityCreate, EntityUpdate
 
 
 class Entity(NameAttributeUpdatableMixin, SavableApiObject):
     """
@@ -40,15 +37,15 @@
     _get_schema = EntityModel
     _list_fields = ["name", "serving_names", "created_at"]
 
     # pydantic instance variable (internal use)
     internal_serving_names: List[str] = Field(alias="serving_names")
 
     def _get_create_payload(self) -> dict[str, Any]:
-        data = EntityCreate(serving_name=self.serving_name, **self.json_dict())
+        data = EntityCreate(serving_name=self.serving_name, **self.dict(by_alias=True))
         return data.json_dict()
 
     @property
     def serving_names(self) -> List[str]:
         """
         Lists the serving names of an Entity object.
 
@@ -273,77 +270,14 @@
         - [Catalog.create_entity](/reference/featurebyte.api.catalog.Catalog.create_entity/): Catalog.create_entity
         """
         try:
             return Entity.get(name=name)
         except RecordRetrievalException:
             return Entity.create(name=name, serving_names=serving_names)
 
-    @typechecked
-    def add_parent(self, parent_entity_name: str, relation_dataset_name: str) -> None:
-        """
-        Adds other entity as the parent of this current entity.
-
-        Parameters
-        ----------
-        parent_entity_name: str
-            the entity that will become the parent of this entity.
-        relation_dataset_name: str
-            the name of the dataset that the parent is from
-
-        Raises
-        ------
-        RecordUpdateException
-            error updating record
-        """
-
-        client = Configurations().get_client()
-        response = client.get("/table", params={"name": relation_dataset_name})
-        assert response.status_code == HTTPStatus.OK
-        json_response = response.json()
-        data_response = json_response["data"]
-        assert len(data_response) == 1
-
-        parent_entity = Entity.get(parent_entity_name)
-        data = ParentEntity(
-            table_type=data_response[0]["type"],
-            table_id=data_response[0]["_id"],
-            id=parent_entity.id,
-        )
-
-        post_response = client.post(
-            f"{self._route}/{self.id}/parent",
-            json=data.json_dict(),
-        )
-        if post_response.status_code != HTTPStatus.CREATED:
-            raise RecordUpdateException(post_response)
-
-    @typechecked
-    def remove_parent(self, parent_entity_name: str) -> None:
-        """
-        Removes other entity as the parent of this current entity.
-
-        Parameters
-        ----------
-        parent_entity_name: str
-            the other entity that we want to remove as a parent.
-
-        Raises
-        ------
-        RecordUpdateException
-            error updating record
-        """
-
-        client = Configurations().get_client()
-        parent_entity = Entity.get(parent_entity_name)
-        post_response = client.delete(
-            f"{self._route}/{self.id}/parent/{parent_entity.id}",
-        )
-        if post_response.status_code != HTTPStatus.OK:
-            raise RecordUpdateException(post_response)
-
     def info(self, verbose: bool = False) -> Dict[str, Any]:
         """
         Returns a dictionary that summarizes the essential information of an Entity object. The dictionary contains
         the following keys:
 
         - `name`: The name of the Entity object.
         - `created_at`: The timestamp indicating when the Entity object was created.
```

### Comparing `featurebyte-0.3.1/featurebyte/api/event_table.py` & `featurebyte-0.4.0/featurebyte/api/event_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/event_view.py` & `featurebyte-0.4.0/featurebyte/api/event_view.py`

 * *Files 0% similar despite different names*

```diff
@@ -432,15 +432,14 @@
 
         # Construct new columns_info
         updated_columns_info = copy.deepcopy(self.columns_info)
         updated_columns_info.append(
             ColumnInfo(
                 name=new_column_name,
                 dtype=feature.dtype,
-                entity_id=EventView._get_feature_entity_id(feature),
             )
         )
 
         # create a new view and return it
         return self._create_joined_view(
             new_node_name=node.name, joined_columns_info=updated_columns_info
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature.py` & `featurebyte-0.4.0/featurebyte/api/feature.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,68 +2,83 @@
 Feature and FeatureList classes
 """
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import Any, ClassVar, Dict, List, Literal, Optional, Sequence, Tuple, Type, Union, cast
 
-import time
 from http import HTTPStatus
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field, root_validator
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ConflictResolution
+from featurebyte.api.api_handler.base import ListHandler
+from featurebyte.api.api_handler.feature import FeatureListHandler
+from featurebyte.api.api_object_util import is_server_mode
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature_job import FeatureJobMixin
 from featurebyte.api.feature_namespace import FeatureNamespace
+from featurebyte.api.feature_or_target_mixin import FeatureOrTargetMixin
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.feature_util import FEATURE_COMMON_LIST_FIELDS, FEATURE_LIST_FOREIGN_KEYS
 from featurebyte.api.feature_validation_util import assert_is_lookup_feature
 from featurebyte.api.savable_api_object import DeletableApiObject, SavableApiObject
+from featurebyte.api.templates.doc_util import substitute_docstring
+from featurebyte.api.templates.feature_or_target_doc import (
+    CATALOG_ID_DOC,
+    DEFINITION_DOC,
+    ENTITY_IDS_DOC,
+    PREVIEW_DOC,
+    PRIMARY_ENTITY_DOC,
+    TABLE_IDS_DOC,
+    VERSION_DOC,
+)
+from featurebyte.api.templates.series_doc import ISNULL_DOC, NOTNULL_DOC
 from featurebyte.common.descriptor import ClassInstanceMethodDescriptor
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.formatting_util import CodeStr
 from featurebyte.common.typing import Scalar, ScalarSequence
-from featurebyte.common.utils import dataframe_from_json, enforce_observation_set_row_order
+from featurebyte.common.utils import enforce_observation_set_row_order
 from featurebyte.config import Configurations
 from featurebyte.core.accessor.count_dict import CdAccessorMixin
 from featurebyte.core.accessor.feature_datetime import FeatureDtAccessorMixin
 from featurebyte.core.accessor.feature_string import FeatureStrAccessorMixin
 from featurebyte.core.series import FrozenSeries, FrozenSeriesT, Series
+from featurebyte.enum import ConflictResolution
 from featurebyte.exception import RecordCreationException, RecordRetrievalException
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.logging import get_logger
-from featurebyte.models.base import PydanticObjectId, VersionIdentifier, get_active_catalog_id
-from featurebyte.models.feature import DefaultVersionMode, FeatureModel, FeatureReadiness
+from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
 from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.relationship_analysis import derive_primary_entity
 from featurebyte.models.tile import TileSpec
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
+from featurebyte.query_graph.graph import GlobalQueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.feature_job_setting import TableFeatureJobSetting
 from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
 from featurebyte.query_graph.node.generic import AliasNode, ProjectNode
 from featurebyte.schema.feature import (
-    FeatureCreate,
+    BatchFeatureCreatePayload,
+    BatchFeatureItem,
     FeatureModelResponse,
-    FeaturePreview,
     FeatureSQL,
     FeatureUpdate,
 )
 
 logger = get_logger(__name__)
 
 
 class Feature(
     Series,
     DeletableApiObject,
     SavableApiObject,
+    FeatureOrTargetMixin,
     CdAccessorMixin,
     FeatureJobMixin,
     FeatureDtAccessorMixin,
     FeatureStrAccessorMixin,
 ):  # pylint: disable=too-many-public-methods
     """
     A feature is input data that is used to train Machine Learning models and compute predictions.
@@ -93,118 +108,95 @@
     _update_schema_class = FeatureUpdate
     _list_schema = FeatureModelResponse
     _get_schema = FeatureModelResponse
     _list_fields = [
         "name",
         "version",
         *FEATURE_COMMON_LIST_FIELDS,
+        "is_default",
     ]
     _list_foreign_keys = FEATURE_LIST_FOREIGN_KEYS
 
-    # pydantic instance variable (internal use)
-    internal_catalog_id: PydanticObjectId = Field(
-        default_factory=get_active_catalog_id, alias="catalog_id"
-    )
-
     def _get_init_params_from_object(self) -> dict[str, Any]:
         return {"feature_store": self.feature_store}
 
-    def _get_create_payload(self) -> dict[str, Any]:
-        data = FeatureCreate(**self.json_dict())
-        return data.json_dict()
-
     def _get_feature_tiles_specs(self) -> List[Tuple[str, List[TileSpec]]]:
-        tile_specs = ExtendedFeatureModel(**self.dict()).tile_specs
+        tile_specs = ExtendedFeatureModel(**self.dict(by_alias=True)).tile_specs
         return [(str(self.name), tile_specs)] if tile_specs else []
 
     @root_validator(pre=True)
     @classmethod
     def _set_feature_store(cls, values: dict[str, Any]) -> dict[str, Any]:
         if "feature_store" not in values:
             tabular_source = values.get("tabular_source")
             if isinstance(tabular_source, dict):
                 feature_store_id = TabularSource(**tabular_source).feature_store_id
                 values["feature_store"] = FeatureStore.get_by_id(id=feature_store_id)
         return values
 
-    @property
-    def version(self) -> str:
-        """
-        Returns the version identifier of a Feature object.
-
-        Returns
-        -------
-        str
-
-        Examples
-        --------
-        >>> feature = catalog.get_feature("CustomerProductGroupCounts_7d")
-        >>> feature.version  # doctest: +SKIP
-        'V230323'
-        """
-        return cast(FeatureModel, self.cached_model).version.to_str()
-
-    @property
-    def catalog_id(self) -> ObjectId:
-        """
-        Returns the catalog ID that is associated with the Feature object.
-
-        Returns
-        -------
-        ObjectId
-            Catalog ID of the table.
-
-        See Also
-        --------
-        - [Catalog](/reference/featurebyte.api.catalog.Catalog)
-        """
-        try:
-            return cast(FeatureModel, self.cached_model).catalog_id
-        except RecordRetrievalException:
-            return self.internal_catalog_id
-
-    @property
-    def entity_ids(self) -> Sequence[ObjectId]:
-        """
-        Returns the entity IDs associated with the Feature object.
-
-        Returns
-        -------
-        Sequence[ObjectId]
-        """
-        try:
-            return cast(FeatureModel, self.cached_model).entity_ids
-        except RecordRetrievalException:
-            return self.graph.get_entity_ids(node_name=self.node_name)
-
-    @property
-    def table_ids(self) -> Sequence[ObjectId]:
-        """
-        Returns the table IDs used by the Feature object.
+    @property  # type: ignore
+    @substitute_docstring(
+        doc_template=VERSION_DOC,
+        examples=(
+            """
+            >>> feature = catalog.get_feature("CustomerProductGroupCounts_7d")
+            >>> feature.version  # doctest: +SKIP
+            'V230323'
+            """
+        ),
+        format_kwargs={"class_name": "Feature"},
+    )
+    def version(self) -> str:  # pylint: disable=missing-function-docstring
+        return self._get_version()
 
-        Returns
-        -------
-        Sequence[ObjectId]
-        """
-        try:
-            return cast(FeatureModel, self.cached_model).table_ids
-        except RecordRetrievalException:
-            return self.graph.get_table_ids(node_name=self.node_name)
+    @property  # type: ignore
+    @substitute_docstring(doc_template=CATALOG_ID_DOC, format_kwargs={"class_name": "Feature"})
+    def catalog_id(self) -> ObjectId:  # pylint: disable=missing-function-docstring
+        return self._get_catalog_id()
+
+    @property  # type: ignore
+    @substitute_docstring(doc_template=ENTITY_IDS_DOC, format_kwargs={"class_name": "Feature"})
+    def entity_ids(self) -> Sequence[ObjectId]:  # pylint: disable=missing-function-docstring
+        return self._get_entity_ids()
+
+    @property  # type: ignore
+    @substitute_docstring(doc_template=TABLE_IDS_DOC, format_kwargs={"class_name": "Feature"})
+    def table_ids(self) -> Sequence[ObjectId]:  # pylint: disable=missing-function-docstring
+        return self._get_table_ids()
+
+    @property  # type: ignore
+    @substitute_docstring(doc_template=PRIMARY_ENTITY_DOC, format_kwargs={"class_name": "Feature"})
+    def primary_entity(self) -> List[Entity]:  # pylint: disable=missing-function-docstring
+        return self._primary_entity()
 
     @property
     def feature_list_ids(self) -> Sequence[ObjectId]:
         """
         Returns the feature list IDs that use the Feature object.
 
         Returns
         -------
         Sequence[ObjectId]
         """
         return cast(FeatureModel, self.cached_model).feature_list_ids
 
+    @property  # type: ignore
+    @substitute_docstring(
+        doc_template=DEFINITION_DOC,
+        examples=(
+            """
+            >>> feature = catalog.get_feature("InvoiceCount_60days")
+            >>> feature_definition = feature.definition
+            """
+        ),
+        format_kwargs={"object_type": "feature"},
+    )
+    def definition(self) -> str:  # pylint: disable=missing-function-docstring
+        return self._generate_definition()
+
     @typechecked
     def isin(self: FrozenSeriesT, other: Union[FrozenSeries, ScalarSequence]) -> FrozenSeriesT:
         """
         Identifies if each element is contained in a sequence of values represented by the `other` parameter.
 
         Parameters
         ----------
@@ -335,28 +327,29 @@
             Table of features
         """
         return FeatureNamespace.list(
             include_id=include_id, primary_entity=primary_entity, primary_table=primary_table
         )
 
     @classmethod
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        features = super()._post_process_list(item_list)
-        # convert version strings
-        features["version"] = features["version"].apply(
-            lambda version: VersionIdentifier(**version).to_str()
+    def _list_handler(cls) -> ListHandler:
+        return FeatureListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
         )
-        return features
 
     @classmethod
     def _list_versions(
         cls,
         include_id: Optional[bool] = True,
         feature_list_id: Optional[ObjectId] = None,
     ) -> pd.DataFrame:
+        # pylint: disable=line-too-long
         """
         Returns a DataFrame that presents a summary of the feature versions belonging to the namespace of the
         Feature object. The DataFrame contains multiple attributes of the feature versions, such as their version
         names, readiness states, online availability, and creation dates.
 
         Parameters
         ----------
@@ -371,34 +364,39 @@
         pd.DataFrame
             Table of features
 
         Examples
         --------
         List saved Feature versions
 
-        >>> Feature.list_versions()  # doctest: +SKIP
-            name        version  dtype readiness  online_enabled             table    entities              created_at
-        0  new_feat2  V230323  FLOAT     DRAFT           False      [sf_event_table]  [customer] 2023-03-23 07:16:21.244
-        1  new_feat1  V230323  FLOAT     DRAFT           False      [sf_event_table]  [customer] 2023-03-23 07:16:21.166
-        2     sum_1d  V230323  FLOAT     DRAFT           False      [sf_event_table]  [customer] 2023-03-23 07:16:21.009
+        >>> Feature.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                                     name  version      dtype         readiness  online_enabled                                          tables    primary_tables           entities   primary_entities  created_at  is_default
+        0  CustomerLatestInvoiceTimestamp      ...  TIMESTAMP  PRODUCTION_READY           False                                [GROCERYINVOICE]  [GROCERYINVOICE]  [grocerycustomer]  [grocerycustomer]         ...  True
+        1                    InvoiceCount      ...      FLOAT             DRAFT           False                  [GROCERYINVOICE, INVOICEITEMS]    [INVOICEITEMS]   [groceryinvoice]   [groceryinvoice]         ...  True
+        2              ProductGroupLookup      ...    VARCHAR             DRAFT           False                                [GROCERYPRODUCT]  [GROCERYPRODUCT]   [groceryproduct]   [groceryproduct]         ...  True
+        3  CustomerProductGroupCounts_90d      ...     OBJECT             DRAFT           False  [GROCERYINVOICE, INVOICEITEMS, GROCERYPRODUCT]    [INVOICEITEMS]  [grocerycustomer]  [grocerycustomer]         ...  True
+        4   CustomerProductGroupCounts_7d      ...     OBJECT             DRAFT           False  [GROCERYINVOICE, INVOICEITEMS, GROCERYPRODUCT]    [INVOICEITEMS]  [grocerycustomer]  [grocerycustomer]         ...  True
+        5         InvoiceAmountAvg_60days      ...      FLOAT  PRODUCTION_READY           False                                [GROCERYINVOICE]  [GROCERYINVOICE]  [grocerycustomer]  [grocerycustomer]         ...  True
+        6             InvoiceCount_60days      ...      FLOAT  PRODUCTION_READY           False                                [GROCERYINVOICE]  [GROCERYINVOICE]  [grocerycustomer]  [grocerycustomer]         ...  True
 
         List Feature versions with the same name
 
         >>> feature = catalog.get_feature("InvoiceCount_60days")
-        >>> feature.list_versions()  # doctest: +SKIP
-                name  version  dtype readiness  online_enabled             table    entities              created_at
-            0  sum_1d  V230323  FLOAT     DRAFT           False  [sf_event_table]  [customer] 2023-03-23 06:19:35.838
+        >>> feature.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                              name  version  dtype         readiness  online_enabled            tables    primary_tables           entities   primary_entities  created_at  is_default
+            0  InvoiceCount_60days      ...  FLOAT  PRODUCTION_READY           False  [GROCERYINVOICE]  [GROCERYINVOICE]  [grocerycustomer]  [grocerycustomer]         ...  True
         """
         params = {}
         if feature_list_id:
             params = {"feature_list_id": str(feature_list_id)}
 
         return cls._list(include_id=include_id, params=params)
 
     def _list_versions_with_same_name(self, include_id: bool = True) -> pd.DataFrame:
+        # pylint: disable=line-too-long
         """
         List feature versions with the same name
 
         Parameters
         ----------
         include_id: bool
             Whether to include id in the list.
@@ -407,17 +405,17 @@
         -------
         pd.DataFrame
             Table of features with the same name
 
         Examples
         --------
         >>> feature = catalog.get_feature("InvoiceCount_60days")
-        >>> feature.list_versions()  # doctest: +SKIP
-                 name  version  dtype readiness  online_enabled             table    entities              created_at  is_default
-            0  sum_1d  V230323  FLOAT     DRAFT           False  [sf_event_table]  [customer] 2023-03-23 06:19:35.838        True
+        >>> feature.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                              name  version  dtype         readiness  online_enabled            tables    primary_tables           entities   primary_entities  created_at  is_default
+            0  InvoiceCount_60days      ...  FLOAT  PRODUCTION_READY           False  [GROCERYINVOICE]  [GROCERYINVOICE]  [grocerycustomer]  [grocerycustomer]         ...        True
         """
         output = self._list(include_id=True, params={"name": self.name})
         default_feature_id = self.feature_namespace.default_feature_id
         output["is_default"] = output["id"] == default_feature_id
         columns = output.columns
         if not include_id:
             columns = [column for column in columns if column != "id"]
@@ -594,15 +592,17 @@
         We check for this by looking to see by looking at the operation structure to see if it's time-based.
 
         Returns
         -------
         bool
             True if the feature is time based, False otherwise.
         """
-        operation_structure = self.graph.extract_operation_structure(self.node)
+        operation_structure = self.graph.extract_operation_structure(
+            self.node, keep_all_source_columns=True
+        )
         return operation_structure.is_time_based
 
     @property
     def is_datetime(self) -> bool:
         """
         Returns whether the feature has a datetime data type.
 
@@ -630,61 +630,14 @@
 
         Returns
         -------
         bool
         """
         return super().saved
 
-    @property
-    def definition(self) -> str:
-        """
-        Displays the feature definition file of the feature.
-
-        The file is the single source of truth for a feature version. The file is generated automatically after a
-        feature is declared in the SDK and is stored in the FeatureByte Service.
-
-        This file uses the same SDK syntax as the feature declaration and provides an explicit outline of the intended
-        operations of the feature declaration, including those that are inherited but not explicitly declared by the
-        user. These operations may include feature job settings and cleaning operations inherited from tables metadata.
-
-        The feature definition file serves as the basis for generating the final logical execution graph, which is
-        then transpiled into platform-specific SQL (e.g. SnowSQL, SparkSQL) for feature materialization.
-
-        Returns
-        -------
-        str
-
-        Examples
-        --------
-        >>> feature = catalog.get_feature("InvoiceCount_60days")
-        >>> feature_definition = feature.definition
-        """
-        try:
-            definition = self.cached_model.definition
-            assert definition is not None, "Saved feature's definition should not be None."
-        except RecordRetrievalException:
-            definition = self._generate_code(to_format=True, to_use_saved_data=True)
-        return CodeStr(definition)
-
-    @property
-    def primary_entity(self) -> List[Entity]:
-        """
-        Returns the primary entity of the Feature object.
-
-        Returns
-        -------
-        list[Entity]
-            Primary entity
-        """
-        entities = []
-        for entity_id in self.entity_ids:
-            entities.append(Entity.get_by_id(entity_id))
-        primary_entity = derive_primary_entity(entities)  # type: ignore
-        return primary_entity
-
     @typechecked
     def save(
         self, conflict_resolution: ConflictResolution = "raise", _id: Optional[ObjectId] = None
     ) -> None:
         """
         Adds a Feature object to the catalog.
 
@@ -696,26 +649,71 @@
         ----------
         conflict_resolution: ConflictResolution
             "raise" will raise an error when we encounter a conflict error.
             "retrieve" will handle the conflict error by retrieving the object with the same name.
         _id: Optional[ObjectId]
             The object ID to be used when saving the object. If not provided, a new object ID will be generated.
 
+        Raises
+        ------
+        RecordCreationException
+            When the feature object cannot be saved using feature definition.
+
         Examples
         --------
         >>> grocery_invoice_view = catalog.get_view("GROCERYINVOICE")
         >>> invoice_amount_avg_60days = grocery_invoice_view.groupby("GroceryCustomerGuid").aggregate_over(
         ...   value_column="Amount",
         ...   method="avg",
         ...   feature_names=["InvoiceAmountAvg_60days"],
         ...   windows=["60d"],
         ... )["InvoiceAmountAvg_60days"]
         >>> invoice_amount_avg_60days.save()  # doctest: +SKIP
         """
-        super().save(conflict_resolution=conflict_resolution, _id=_id)
+        if is_server_mode():
+            # server mode save a feature by POST /feature/ endpoint directly without running the feature definition.
+            super().save(conflict_resolution=conflict_resolution, _id=_id)
+        else:
+            # For non-server mode, a feature is saved by POST /feature/batch endpoint. A task is created to run the
+            # feature definition and save the feature. The task is executed asynchronously. The feature definition is
+            # validated before saving the feature.
+            self._check_object_not_been_saved(conflict_resolution=conflict_resolution)
+            pruned_graph, node_name_map = GlobalQueryGraph().quick_prune(
+                target_node_names=[self.node_name]
+            )
+            feature_item = BatchFeatureItem(
+                id=self.id,
+                name=self.name,
+                node_name=node_name_map[self.node_name],
+                tabular_source=self.tabular_source,
+            )
+            try:
+                self.post_async_task(
+                    route="/feature/batch",
+                    payload=BatchFeatureCreatePayload(
+                        graph=pruned_graph,
+                        features=[feature_item],
+                    ).json_dict(),
+                    retrieve_result=False,
+                    has_output_url=False,
+                )
+                object_dict = self._get_object_dict_by_id(id_value=feature_item.id)
+            except RecordCreationException as exc:
+                traceback_message = exc.response.json()["traceback"]
+                has_dup_exception = False
+                if traceback_message:
+                    has_dup_exception = (
+                        "featurebyte.exception.DuplicatedRecordException" in traceback_message
+                    )
+                if conflict_resolution == "retrieve" and has_dup_exception:
+                    object_dict = self._get_object_dict_by_name(name=feature_item.name)
+                else:
+                    raise exc
+
+            type(self).__init__(self, **object_dict, **self._get_init_params_from_object())
 
     @typechecked
     def astype(
         self: FrozenSeriesT,
         new_type: Union[Type[int], Type[float], Type[str], Literal["int", "float", "str"]],
     ) -> FrozenSeriesT:
         """
@@ -760,107 +758,63 @@
         if isinstance(other, FrozenSeries):
             entity_ids = entity_ids.union(getattr(other, "entity_ids", []))
         return {"entity_ids": sorted(entity_ids)}
 
     def unary_op_series_params(self) -> dict[str, Any]:
         return {"entity_ids": self.entity_ids}
 
-    def _get_pruned_feature_model(self) -> FeatureModel:
-        """
-        Get pruned model of feature
-
-        Returns
-        -------
-        FeatureModel
-        """
-        pruned_graph, mapped_node = self.extract_pruned_graph_and_node()
-        feature_dict = self.dict()
-        feature_dict["graph"] = pruned_graph
-        feature_dict["node_name"] = mapped_node.name
-        return FeatureModel(**feature_dict)
-
+    @substitute_docstring(
+        doc_template=PREVIEW_DOC,
+        description=(
+            """
+            Materializes a Feature object using a small observation set of up to 50 rows. Unlike compute_historical_features,
+            this method does not store partial aggregations (tiles) to speed up future computation. Instead, it computes
+            the feature values on the fly, and should be used only for small observation sets for debugging or prototyping
+            unsaved features.
+            """
+        ),
+        examples=(
+            """
+            Examples
+            --------
+            Preview feature with a small observation set.
+
+            >>> catalog.get_feature("InvoiceCount_60days").preview(
+            ...     observation_set=pd.DataFrame({
+            ...         "POINT_IN_TIME": ["2022-06-01 00:00:00", "2022-06-02 00:00:00"],
+            ...         "GROCERYCUSTOMERGUID": [
+            ...             "a2828c3b-036c-4e2e-9bd6-30c9ee9a20e3",
+            ...             "ac479f28-e0ff-41a4-8e60-8678e670e80b",
+            ...         ],
+            ...     })
+            ... )
+              POINT_IN_TIME                   GROCERYCUSTOMERGUID  InvoiceCount_60days
+            0    2022-06-01  a2828c3b-036c-4e2e-9bd6-30c9ee9a20e3                 10.0
+            1    2022-06-02  ac479f28-e0ff-41a4-8e60-8678e670e80b                  6.0
+            """
+        ),
+        see_also=(
+            """
+            See Also
+            --------
+            - [FeatureGroup.preview](/reference/featurebyte.api.feature_group.FeatureGroup.preview/):
+              Preview feature group.
+            - [FeatureList.compute_historical_features](/reference/featurebyte.api.feature_list.FeatureList.compute_historical_features/):
+              Get historical features from a feature list.
+            """
+        ),
+        format_kwargs={"object_type": "feature"},
+    )
     @enforce_observation_set_row_order
     @typechecked
-    def preview(
+    def preview(  # pylint: disable=missing-function-docstring
         self,
         observation_set: pd.DataFrame,
     ) -> pd.DataFrame:
-        """
-        Materializes a Feature object using a small observation set of up to 50 rows. Unlike compute_historical_features,
-        this method does not store partial aggregations (tiles) to speed up future computation. Instead, it computes
-        the feature values on the fly, and should be used only for small observation sets for debugging or prototyping
-        unsaved features.
-
-        The small observation set should combine historical points-in-time and key values of the primary entity from
-        the feature. Associated serving entities can also be utilized.
-
-        Parameters
-        ----------
-        observation_set : pd.DataFrame
-            Observation set DataFrame which combines historical points-in-time and values of the feature primary entity
-            or its descendant (serving entities). The column containing the point-in-time values should be named
-            `POINT_IN_TIME`, while the columns representing entity values should be named using accepted serving
-            names for the entity.
-
-        Returns
-        -------
-        pd.DataFrame
-            Materialized feature values.
-            The returned DataFrame will have the same number of rows, and include all columns from the observation set.
-
-            **Note**: `POINT_IN_TIME` values will be converted to UTC time.
-
-        Raises
-        ------
-        RecordRetrievalException
-            Failed to materialize feature preview.
-
-        Examples
-        --------
-        Preview feature with a small observation set.
-
-        >>> catalog.get_feature("InvoiceCount_60days").preview(
-        ...     observation_set=pd.DataFrame({
-        ...         "POINT_IN_TIME": ["2022-06-01 00:00:00", "2022-06-02 00:00:00"],
-        ...         "GROCERYCUSTOMERGUID": [
-        ...             "a2828c3b-036c-4e2e-9bd6-30c9ee9a20e3",
-        ...             "ac479f28-e0ff-41a4-8e60-8678e670e80b",
-        ...         ],
-        ...     })
-        ... )
-          POINT_IN_TIME                   GROCERYCUSTOMERGUID  InvoiceCount_60days
-        0    2022-06-01  a2828c3b-036c-4e2e-9bd6-30c9ee9a20e3                 10.0
-        1    2022-06-02  ac479f28-e0ff-41a4-8e60-8678e670e80b                  6.0
-
-        See Also
-        --------
-        - [FeatureGroup.preview](/reference/featurebyte.api.feature_group.FeatureGroup.preview/):
-          Preview feature group.
-        - [FeatureList.compute_historical_features](/reference/featurebyte.api.feature_list.FeatureList.compute_historical_features/):
-          Get historical features from a feature list.
-        """
-        tic = time.time()
-
-        feature = self._get_pruned_feature_model()
-        payload = FeaturePreview(
-            feature_store_name=self.feature_store.name,
-            graph=feature.graph,
-            node_name=feature.node_name,
-            point_in_time_and_serving_name_list=observation_set.to_dict(orient="records"),
-        )
-
-        client = Configurations().get_client()
-        response = client.post(url="/feature/preview", json=payload.json_dict())
-        if response.status_code != HTTPStatus.OK:
-            raise RecordRetrievalException(response)
-        result = response.json()
-
-        elapsed = time.time() - tic
-        logger.debug(f"Preview took {elapsed:.2f}s")
-        return dataframe_from_json(result)  # pylint: disable=no-member
+        return self._preview(observation_set=observation_set, url="/feature/preview")
 
     @typechecked
     def create_new_version(
         self,
         table_feature_job_settings: Optional[List[TableFeatureJobSetting]] = None,
         table_cleaning_operations: Optional[List[TableCleaningOperation]] = None,
     ) -> Feature:
@@ -1097,15 +1051,17 @@
             update_payload={"default_version_mode": DefaultVersionMode(default_version_mode).value},
             allow_update_local=False,
         )
 
     def as_default_version(self) -> None:
         """
         When a feature has its default version mode set to manual, this method designates the Feature object as the
-        default version for that specific feature.
+        default version for that specific feature. Note that the specified feature must be a version with the highest
+        level of readiness. This method is used when there are other version that share the same level of readiness
+        as the default version and the user wants to manually set the default version.
 
         Each feature is recognized by its name and can possess numerous versions, though only a single default
         version is allowed.
 
         The default version streamlines feature reuse by supplying the most suitable version when none is explicitly
         indicated. By default, the feature's default version mode is automatic, selecting the version with the highest
         readiness level as the default. If several versions share the same readiness level, the most recent one
@@ -1135,19 +1091,18 @@
             Retrieved Feature SQL string.
 
         Raises
         ------
         RecordRetrievalException
             Failed to get feature SQL.
         """
-        feature = self._get_pruned_feature_model()
-
+        pruned_graph, mapped_node = self.extract_pruned_graph_and_node()
         payload = FeatureSQL(
-            graph=feature.graph,
-            node_name=feature.node_name,
+            graph=pruned_graph,
+            node_name=mapped_node.name,
         )
 
         client = Configurations().get_client()
         response = client.post("/feature/sql", json=payload.json_dict())
         if response.status_code != HTTPStatus.OK:
             raise RecordRetrievalException(response)
 
@@ -1165,44 +1120,50 @@
         Parameters
         ----------
         other: Union[FrozenSeries, Sequence[Union[bool, int, float, str]]]
             other
         """
         assert_is_lookup_feature(self.node_types_lineage)
 
-    def isnull(self) -> Feature:
-        """
-        Returns a boolean Feature indicating whether each element is missing.
-
-        Returns
-        -------
-        Feature
-            Feature with boolean values
-
-        Examples
-        --------
-        >>> feature = catalog.get_feature("InvoiceCount_60days")
-        >>> new_feature = feature.isnull()
-        """
+    @substitute_docstring(
+        doc_template=ISNULL_DOC,
+        format_kwargs={"class_name": "Feature"},
+        examples=(
+            """
+            >>> feature = catalog.get_feature("InvoiceCount_60days")
+            >>> new_feature = feature.isnull()
+            """
+        ),
+    )
+    def isnull(self) -> Feature:  # pylint: disable=missing-function-docstring
         return super().isnull()
 
-    def notnull(self) -> Feature:
-        """
-        Returns a boolean Feature indicating whether each element is not null.
-
-        Returns
-        -------
-        Feature
-            Feature with boolean values
-
-        Examples
-        --------
-        >>> feature = catalog.get_feature("InvoiceCount_60days")
-        >>> new_feature = feature.notnull()
-        """
+    @substitute_docstring(
+        doc_template=NOTNULL_DOC,
+        format_kwargs={"class_name": "Feature"},
+        examples=(
+            """
+            >>> feature = catalog.get_feature("InvoiceCount_60days")
+            >>> new_feature = feature.notnull()
+            """
+        ),
+    )
+    def notnull(self) -> Feature:  # pylint: disable=missing-function-docstring
         return super().notnull()
 
     # descriptors
     list_versions: ClassVar[ClassInstanceMethodDescriptor] = ClassInstanceMethodDescriptor(
         class_method=_list_versions,
         instance_method=_list_versions_with_same_name,
     )
+
+    @typechecked
+    def update_namespace_description(self, description: Optional[str]) -> None:
+        """
+        Update description of object namespace
+
+        Parameters
+        ----------
+        description: Optional[str]
+            Description of the object namespace
+        """
+        self.feature_namespace.update_description(description=description)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_group.py` & `featurebyte-0.4.0/featurebyte/api/feature_group.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,66 +1,160 @@
 """
 Feature group module.
 """
 
 from __future__ import annotations
 
-from typing import Any, List, Optional, OrderedDict, Sequence, Set, Tuple, Union, cast
+from typing import Any, Dict, List, Optional, OrderedDict, Sequence, Set, Tuple, Union, cast
 
 import collections
 import time
+from datetime import datetime
 from http import HTTPStatus
 
 import pandas as pd
+from alive_progress import alive_bar
 from bson import ObjectId
-from pydantic import Field, parse_obj_as, root_validator
+from pydantic import Field, parse_obj_as
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ConflictResolution
+from featurebyte.api.api_object_util import (
+    PAGINATED_CALL_PAGE_SIZE,
+    delete_api_object_by_id,
+    get_api_object_by_id,
+    iterate_api_object_using_paginated_routes,
+)
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature import Feature
+from featurebyte.api.feature_store import FeatureStore
+from featurebyte.api.mixin import AsyncMixin
 from featurebyte.common.doc_util import FBAutoDoc
+from featurebyte.common.env_util import get_alive_bar_additional_params
 from featurebyte.common.typing import Scalar
 from featurebyte.common.utils import dataframe_from_json, enforce_observation_set_row_order
 from featurebyte.config import Configurations
 from featurebyte.core.mixin import ParentMixin
 from featurebyte.core.series import Series
+from featurebyte.enum import ConflictResolution
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.logging import get_logger
-from featurebyte.models.base import FeatureByteBaseModel
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureCluster, FeatureListModel
 from featurebyte.models.relationship_analysis import derive_primary_entity
-from featurebyte.schema.feature_list import FeatureListPreview, FeatureListSQL
+from featurebyte.query_graph.graph import GlobalQueryGraph
+from featurebyte.query_graph.model.common_table import TabularSource
+from featurebyte.schema.feature import BatchFeatureItem
+from featurebyte.schema.feature_list import (
+    FeatureListCreateWithBatchFeatureCreationPayload,
+    FeatureListPreview,
+    FeatureListSQL,
+)
 
 logger = get_logger(__name__)
 
 
-class BaseFeatureGroup(FeatureByteBaseModel):
+Item = Union[Feature, "BaseFeatureGroup"]
+FeatureObjects = OrderedDict[str, Feature]
+
+
+class BaseFeatureGroup(AsyncMixin):
     """
     BaseFeatureGroup class
 
     This class represents a collection of Feature's that users create.
 
     Parameters
     ----------
-    items: Sequence[Union[Feature, BaseFeatureGroup]]
+    items: Sequence[Item]
         List of feature like objects to be used to create the FeatureList
-    feature_objects: OrderedDict[str, Feature]
+    feature_objects: FeatureObjects
         Dictionary of feature name to feature object
     """
 
-    items: Sequence[Union[Feature, BaseFeatureGroup]] = Field(
+    items: Sequence[Item] = Field(
         exclude=True,
         description="A sequence that consists of Feature, FeatureList, and FeatureGroup objects. This sequence is used "
         "to create a new FeatureGroup that contains the Feature objects found within the provided items.",
     )
-    feature_objects: OrderedDict[str, Feature] = Field(
-        exclude=True, default_factory=collections.OrderedDict
-    )
+    feature_objects: FeatureObjects = Field(exclude=True, default_factory=collections.OrderedDict)
+
+    @classmethod
+    def _flatten_items(cls, items: Sequence[Item]) -> FeatureObjects:
+        feature_objects = collections.OrderedDict()
+        feature_ids = set()
+        for item in items:
+            if isinstance(item, Feature):
+                if item.name is None:
+                    raise ValueError(f'Feature (feature.id: "{item.id}") name must not be None!')
+                if item.name in feature_objects:
+                    raise ValueError(f'Duplicated feature name (feature.name: "{item.name}")!')
+                if item.id in feature_ids:
+                    raise ValueError(f'Duplicated feature id (feature.id: "{item.id}")!')
+                feature_objects[item.name] = item
+                feature_ids.add(item.id)
+            else:
+                for name, feature in item.feature_objects.items():
+                    if feature.name in feature_objects:
+                        raise ValueError(
+                            f'Duplicated feature name (feature.name: "{feature.name}")!'
+                        )
+                    if feature.id in feature_ids:
+                        raise ValueError(f'Duplicated feature id (feature.id: "{feature.id}")!')
+                    feature_objects[name] = feature
+        return feature_objects
+
+    @staticmethod
+    def _initialize_items_and_feature_objects_from_persistent(
+        feature_list_id: ObjectId, feature_ids: List[str]
+    ) -> Tuple[Sequence[Item], FeatureObjects]:
+        feature_id_to_object = {}
+        feature_store_map: Dict[ObjectId, FeatureStore] = {}
+        with alive_bar(
+            total=len(feature_ids),
+            title="Loading Feature(s)",
+            **get_alive_bar_additional_params(),
+        ) as progress_bar:
+            for feature_dict in iterate_api_object_using_paginated_routes(
+                route="/feature",
+                params={"feature_list_id": feature_list_id, "page_size": PAGINATED_CALL_PAGE_SIZE},
+            ):
+                # store the feature store retrieve result to reuse it if same feature store are called again
+                feature_store_id = TabularSource(**feature_dict["tabular_source"]).feature_store_id
+                if feature_store_id not in feature_store_map:
+                    feature_store_map[feature_store_id] = FeatureStore.get_by_id(feature_store_id)
+                feature_dict["feature_store"] = feature_store_map[feature_store_id]
+
+                # deserialize feature record into feature object
+                feature = Feature.from_persistent_object_dict(object_dict=feature_dict)
+                feature_id_to_object[str(feature.id)] = feature
+                progress_bar.text = feature.name
+                progress_bar()  # pylint: disable=not-callable
+
+            # preserve the order of features
+            items = []
+            feature_objects = collections.OrderedDict()
+            for feature_id in feature_ids:
+                feature = feature_id_to_object[str(feature_id)]
+                assert feature.name is not None
+                feature_objects[feature.name] = feature
+                items.append(feature)
+        return items, feature_objects
+
+    @typechecked
+    def __init__(self, items: Sequence[Item], **kwargs: Any):
+        if items:
+            # handle the case where the object is created from a list of
+            # Feature / FeatureGroup / FeatureList objects
+            kwargs["feature_objects"] = self._flatten_items(items)
+
+        super().__init__(items=items, **kwargs)
+        # sanity check: make sure we don't make a copy on global query graph
+        for item_origin, item in zip(items, self.items):
+            if isinstance(item_origin, Feature) and isinstance(item, Feature):
+                assert id(item_origin.graph.nodes) == id(item.graph.nodes)
 
     @property
     def _features(self) -> list[Feature]:
         """
         Retrieve list of features in the FeatureGroup object
 
         Returns
@@ -122,50 +216,14 @@
         for feature in self._features:
             entity_ids.update(feature.entity_ids)
         primary_entity = derive_primary_entity(  # type: ignore
             [Entity.get_by_id(entity_id) for entity_id in entity_ids]
         )
         return primary_entity
 
-    @root_validator
-    @classmethod
-    def _set_feature_objects(cls, values: dict[str, Any]) -> dict[str, Any]:
-        feature_objects = collections.OrderedDict()
-        feature_ids = set()
-        items = values.get("items", [])
-        for item in items:
-            if isinstance(item, Feature):
-                if item.name is None:
-                    raise ValueError(f'Feature (feature.id: "{item.id}") name must not be None!')
-                if item.name in feature_objects:
-                    raise ValueError(f'Duplicated feature name (feature.name: "{item.name}")!')
-                if item.id in feature_ids:
-                    raise ValueError(f'Duplicated feature id (feature.id: "{item.id}")!')
-                feature_objects[item.name] = item
-                feature_ids.add(item.id)
-            else:
-                for name, feature in item.feature_objects.items():
-                    if feature.name in feature_objects:
-                        raise ValueError(
-                            f'Duplicated feature name (feature.name: "{feature.name}")!'
-                        )
-                    if feature.id in feature_ids:
-                        raise ValueError(f'Duplicated feature id (feature.id: "{feature.id}")!')
-                    feature_objects[name] = feature
-        values["feature_objects"] = feature_objects
-        return values
-
-    @typechecked
-    def __init__(self, items: Sequence[Union[Feature, BaseFeatureGroup]], **kwargs: Any):
-        super().__init__(items=items, **kwargs)
-        # sanity check: make sure we don't make a copy on global query graph
-        for item_origin, item in zip(items, self.items):
-            if isinstance(item_origin, Feature) and isinstance(item, Feature):
-                assert id(item_origin.graph.nodes) == id(item.graph.nodes)
-
     def _subset_single_column(self, column: str) -> Feature:
         return self.feature_objects[column]
 
     def _subset_list_of_columns(self, columns: list[str]) -> FeatureGroup:
         return FeatureGroup([self.feature_objects[elem] for elem in columns])
 
     @typechecked
@@ -320,14 +378,42 @@
             raise RecordRetrievalException(response)
 
         return cast(
             str,
             response.json(),
         )
 
+    def _get_feature_list_batch_feature_create_payload(
+        self,
+        feature_list_id: ObjectId,
+        feature_list_name: str,
+        conflict_resolution: ConflictResolution,
+    ) -> FeatureListCreateWithBatchFeatureCreationPayload:
+        pruned_graph, node_name_map = GlobalQueryGraph().quick_prune(
+            target_node_names=[feat.node_name for feat in self.feature_objects.values()]
+        )
+        batch_feature_items = []
+        for feat in self.feature_objects.values():
+            batch_feature_items.append(
+                BatchFeatureItem(
+                    id=feat.id,
+                    name=feat.name,
+                    node_name=node_name_map[feat.node_name],
+                    tabular_source=feat.tabular_source,
+                )
+            )
+        feature_list_create = FeatureListCreateWithBatchFeatureCreationPayload(
+            name=feature_list_name,
+            conflict_resolution=conflict_resolution,
+            graph=pruned_graph,
+            features=batch_feature_items,
+            _id=feature_list_id,
+        )
+        return feature_list_create
+
 
 class FeatureGroup(BaseFeatureGroup, ParentMixin):
     """
     FeatureGroup class is the constructor class to create a FeatureGroup object. A FeatureGroup is a temporary
     collection of Feature objects that enables the easy management of features and creation of a feature list.
 
     Note that while FeatureGroup has a `save` function, it is actually the individual Feature objects within this
@@ -393,9 +479,38 @@
         --------
         >>> features = fb.FeatureGroup([
         ...     catalog.get_feature("InvoiceCount_60days"),
         ...     catalog.get_feature("InvoiceAmountAvg_60days"),
         ... ])
         >>> features.save()  # doctest: +SKIP
         """
-        for feature_name in self.feature_names:
-            self[feature_name].save(conflict_resolution=conflict_resolution)
+        # prepare batch feature create payload
+        feature_list_batch_feature_create = self._get_feature_list_batch_feature_create_payload(
+            feature_list_id=ObjectId(),
+            feature_list_name=f"_temporary_feature_list_{datetime.now()}",
+            conflict_resolution=conflict_resolution,
+        )
+
+        # save temporary feature list with list of features
+        self.post_async_task(
+            route="/feature_list/batch",
+            payload=feature_list_batch_feature_create.json_dict(),
+            retrieve_result=False,
+            has_output_url=False,
+        )
+
+        try:
+            feature_list_dict = get_api_object_by_id(
+                route="/feature_list", id_value=feature_list_batch_feature_create.id
+            )
+            items, feature_objects = self._initialize_items_and_feature_objects_from_persistent(
+                feature_list_id=feature_list_batch_feature_create.id,
+                feature_ids=feature_list_dict["feature_ids"],
+            )
+            self.items = items
+            self.feature_objects = feature_objects
+
+        finally:
+            # delete temporary feature list
+            delete_api_object_by_id(
+                route="/feature_list", id_value=feature_list_batch_feature_create.id
+            )
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_job.py` & `featurebyte-0.4.0/featurebyte/api/feature_job.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/api/feature_job_setting_analysis.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,15 +8,20 @@
 from io import BytesIO
 from pathlib import Path
 
 import pandas as pd
 from bson import ObjectId
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_handler.base import ListHandler
+from featurebyte.api.api_handler.feature_job_setting_analysis import (
+    FeatureJobSettingAnalysisListHandler,
+)
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.env_util import display_html_in_notebook
 from featurebyte.config import Configurations
 from featurebyte.models.feature_job_setting_analysis import FeatureJobSettingAnalysisModel
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.schema.feature_job_setting_analysis import (
@@ -56,28 +61,20 @@
         "blind_spot",
     ]
     _list_foreign_keys = [
         ForeignKeyMapping("event_table_id", TableApiObject, "event_table"),
     ]
 
     @classmethod
-    @typechecked
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        records = super()._post_process_list(item_list)
-        # format results into dataframe
-        analysis_options = pd.json_normalize(records.analysis_options)
-        recommendation = pd.json_normalize(records.recommended_feature_job_setting)
-
-        return pd.concat(
-            [
-                records[["id", "created_at", "event_table"]],
-                analysis_options,
-                recommendation,
-            ],
-            axis=1,
+    def _list_handler(cls) -> ListHandler:
+        return FeatureJobSettingAnalysisListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
         )
 
     @classmethod
     def list(
         cls,
         include_id: Optional[bool] = True,
         event_table_id: Optional[ObjectId] = None,
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_list.py` & `featurebyte-0.4.0/featurebyte/api/feature_list.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,67 +15,58 @@
     Sequence,
     Tuple,
     TypeVar,
     Union,
     cast,
 )
 
-import collections
 from http import HTTPStatus
 
 import pandas as pd
-from alive_progress import alive_bar
 from bson.objectid import ObjectId
 from pydantic import Field, root_validator
 from typeguard import typechecked
 
-from featurebyte.api.api_object import (
-    PAGINATED_CALL_PAGE_SIZE,
-    ApiObject,
-    ConflictResolution,
-    ForeignKeyMapping,
-)
+from featurebyte.api.api_handler.base import ListHandler
+from featurebyte.api.api_handler.feature_list import FeatureListListHandler
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature import Feature
-from featurebyte.api.feature_group import BaseFeatureGroup, FeatureGroup
+from featurebyte.api.feature_group import BaseFeatureGroup, FeatureGroup, Item
 from featurebyte.api.feature_job import FeatureJobMixin, FeatureJobStatusResult
-from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.historical_feature_table import HistoricalFeatureTable
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.api.savable_api_object import DeletableApiObject, SavableApiObject
 from featurebyte.common.descriptor import ClassInstanceMethodDescriptor
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.env_util import get_alive_bar_additional_params
 from featurebyte.common.utils import (
     convert_to_list_of_strings,
     dataframe_to_arrow_bytes,
     enforce_observation_set_row_order,
 )
 from featurebyte.config import Configurations
-from featurebyte.exception import (
-    DuplicatedRecordException,
-    RecordCreationException,
-    RecordRetrievalException,
-)
+from featurebyte.enum import ConflictResolution
+from featurebyte.exception import RecordCreationException, RecordRetrievalException
 from featurebyte.feature_manager.model import ExtendedFeatureModel
-from featurebyte.models.base import PydanticObjectId, VersionIdentifier, get_active_catalog_id
-from featurebyte.models.feature import DefaultVersionMode
+from featurebyte.models.base import PydanticObjectId, get_active_catalog_id
 from featurebyte.models.feature_list import (
     FeatureListModel,
     FeatureListStatus,
     FeatureReadinessDistribution,
     FrozenFeatureListNamespaceModel,
 )
+from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.models.tile import TileSpec
-from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.schema.deployment import DeploymentCreate
 from featurebyte.schema.feature_list import (
     FeatureListCreate,
     FeatureListGetHistoricalFeatures,
+    FeatureListModelResponse,
     FeatureListUpdate,
     FeatureVersionInfo,
 )
 from featurebyte.schema.feature_list_namespace import (
     FeatureListNamespaceModelResponse,
     FeatureListNamespaceUpdate,
 )
@@ -83,14 +74,41 @@
 
 if TYPE_CHECKING:
     from featurebyte.api.deployment import Deployment
 else:
     Deployment = TypeVar("Deployment")
 
 
+class FeatureListNamespaceListHandler(ListHandler):
+    """
+    Additional handling for feature list namespace.
+    """
+
+    def additional_post_processing(self, item_list: pd.DataFrame) -> pd.DataFrame:
+        # add information about default feature list version
+        feature_list_versions = FeatureList.list_versions(include_id=True)
+        feature_lists = item_list.merge(
+            feature_list_versions[["id", "online_frac", "deployed"]].rename(
+                columns={"id": "default_feature_list_id"}
+            ),
+            on="default_feature_list_id",
+        )
+
+        # replace id with default_feature_list_id
+        feature_lists["id"] = feature_lists["default_feature_list_id"]
+
+        feature_lists["num_feature"] = feature_lists.feature_namespace_ids.apply(len)
+        feature_lists["readiness_frac"] = feature_lists.readiness_distribution.apply(
+            lambda readiness_distribution: FeatureReadinessDistribution(
+                __root__=readiness_distribution
+            ).derive_production_ready_fraction()
+        )
+        return feature_lists
+
+
 class FeatureListNamespace(FrozenFeatureListNamespaceModel, ApiObject):
     """
     FeatureListNamespace represents all the versions of the FeatureList that have the same FeatureList name.
 
     For example, a user might have created a FeatureList called "my feature list". That feature list might in turn
     contain 2 features:
     - feature_1,
@@ -199,36 +217,21 @@
         >>> feature_list = catalog.get_feature_list("invoice_feature_list")
         >>> feature_list.status
         'TEMPLATE'
         """
         return self.cached_model.status
 
     @classmethod
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        feature_lists = super()._post_process_list(item_list)
-
-        # add information about default feature list version
-        feature_list_versions = FeatureList.list_versions(include_id=True)
-        feature_lists = feature_lists.merge(
-            feature_list_versions[["id", "online_frac", "deployed"]].rename(
-                columns={"id": "default_feature_list_id"}
-            ),
-            on="default_feature_list_id",
-        )
-
-        # replace id with default_feature_list_id
-        feature_lists["id"] = feature_lists["default_feature_list_id"]
-
-        feature_lists["num_feature"] = feature_lists.feature_namespace_ids.apply(len)
-        feature_lists["readiness_frac"] = feature_lists.readiness_distribution.apply(
-            lambda readiness_distribution: FeatureReadinessDistribution(
-                __root__=readiness_distribution
-            ).derive_production_ready_fraction()
+    def _list_handler(cls) -> ListHandler:
+        return FeatureListNamespaceListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
         )
-        return feature_lists
 
     @classmethod
     def list(
         cls,
         include_id: Optional[bool] = False,
         primary_entity: Optional[Union[str, List[str]]] = None,
         entity: Optional[str] = None,
@@ -311,83 +314,52 @@
         proxy_class="featurebyte.FeatureList",
         hide_keyword_only_params_in_class_docs=True,
     )
 
     # class variables
     _route = "/feature_list"
     _update_schema_class = FeatureListUpdate
-    _list_schema = FeatureListModel
-    _get_schema = FeatureListModel
+    _list_schema = FeatureListModelResponse
+    _get_schema = FeatureListModelResponse
     _list_fields = [
         "name",
         "version",
         "num_feature",
         "online_frac",
         "deployed",
         "created_at",
+        "is_default",
     ]
 
     # pydantic instance variable (internal use)
     internal_catalog_id: PydanticObjectId = Field(
         default_factory=get_active_catalog_id, alias="catalog_id"
     )
     internal_feature_ids: List[PydanticObjectId] = Field(alias="feature_ids", default_factory=list)
 
-    @root_validator(pre=True)
-    @classmethod
-    def _initialize_feature_objects_and_items(cls, values: dict[str, Any]) -> dict[str, Any]:
-        if "feature_ids" in values:
-            # FeatureList object constructed in SDK will not have feature_ids attribute,
-            # only the record retrieved from the persistent contains this attribute.
-            # Use this check to decide whether to make API call to retrieve features.
-            items = []
-            feature_objects = collections.OrderedDict()
-            id_value = values["_id"]
-            feature_store_map: Dict[ObjectId, FeatureStore] = {}
-            with alive_bar(
-                total=len(values["feature_ids"]),
-                title="Loading Feature(s)",
-                **get_alive_bar_additional_params(),
-            ) as progress_bar:
-                for feature_dict in cls.iterate_api_object_using_paginated_routes(
-                    route="/feature",
-                    params={"feature_list_id": id_value, "page_size": PAGINATED_CALL_PAGE_SIZE},
-                ):
-                    # store the feature store retrieve result to reuse it if same feature store are called again
-                    feature_store_id = TabularSource(
-                        **feature_dict["tabular_source"]
-                    ).feature_store_id
-                    if feature_store_id not in feature_store_map:
-                        feature_store_map[feature_store_id] = FeatureStore.get_by_id(
-                            feature_store_id
-                        )
-                    feature_dict["feature_store"] = feature_store_map[feature_store_id]
-
-                    # deserialize feature record into feature object
-                    feature = Feature.from_persistent_object_dict(object_dict=feature_dict)
-                    items.append(feature)
-                    feature_objects[feature.name] = feature
-                    progress_bar.text = feature.name
-                    progress_bar()  # pylint: disable=not-callable
-
-            values["items"] = items
-            values["feature_objects"] = feature_objects
-        return values
-
     @root_validator
     @classmethod
     def _initialize_feature_list_parameters(cls, values: dict[str, Any]) -> dict[str, Any]:
         # set the following values if it is empty (used mainly by the SDK constructed feature list)
         # for the feature list constructed during serialization, following codes should be skipped
         features = list(values["feature_objects"].values())
         values["internal_feature_ids"] = [feature.id for feature in features]
         return values
 
     @typechecked
-    def __init__(self, items: Sequence[Union[Feature, BaseFeatureGroup]], name: str, **kwargs: Any):
+    def __init__(self, items: Sequence[Item], name: str, **kwargs: Any):
+        if "_id" in kwargs and "feature_ids" in kwargs:
+            # FeatureList object constructed in SDK will not have _id & feature_ids attribute,
+            # only the record retrieved from the persistent if kwargs contain these attributes.
+            # Use this check to decide whether to make API call to retrieve features.
+            items, feature_objects = self._initialize_items_and_feature_objects_from_persistent(
+                feature_list_id=kwargs["_id"], feature_ids=kwargs["feature_ids"]
+            )
+            kwargs["feature_objects"] = feature_objects
+
         super().__init__(items=items, name=name, **kwargs)
 
     @property
     def version(self) -> str:
         """
         Returns the version identifier of a FeatureList object.
 
@@ -423,15 +395,15 @@
 
     def _get_init_params_from_object(self) -> dict[str, Any]:
         return {"items": self.items}
 
     def _get_feature_tiles_specs(self) -> List[Tuple[str, List[TileSpec]]]:
         feature_tile_specs = []
         for feature in self.feature_objects.values():
-            tile_specs = ExtendedFeatureModel(**feature.dict()).tile_specs
+            tile_specs = ExtendedFeatureModel(**feature.dict(by_alias=True)).tile_specs
             if tile_specs:
                 feature_tile_specs.append((str(feature.name), tile_specs))
         return feature_tile_specs
 
     @typechecked
     def get_feature_jobs_status(
         self,
@@ -471,14 +443,16 @@
 
     def info(self, verbose: bool = False) -> Dict[str, Any]:
         """
         Returns a dictionary that summarizes the essential information of an FeatureList object. The dictionary
         contains the following keys:
 
         - `name`: The name of the FeatureList object.
+        - `description`: The description of the FeatureList object.
+        - `namespace_description`: The namespace description of the FeatureList object.
         - `created_at`: The timestamp indicating when the FeatureList object was created.
         - `updated_at`: The timestamp indicating when the FeatureList object was last updated.
         - `primary_entity`: Details about the primary entity of the FeatureList object.
         - `entities`: List of entities involved in the computation of the features contained in the FeatureList object.
         - `tables`: List of tables involved in the computation of the features contained in the FeatureList object.
         - `default_version_mode`: Indicates whether the default version mode is 'auto' or 'manual'.
         - `version_count`: The number of versions with the same feature list namespace.
@@ -513,14 +487,15 @@
         >>> feature_list = catalog.get_feature_list("invoice_feature_list")
         >>> info = feature_list.info()
         >>> del info["created_at"]
         >>> del info["updated_at"]
         >>> info  # doctest: +ELLIPSIS
         {
           'name': 'invoice_feature_list',
+          'description': None,
           'entities': [
             {
               'name': 'grocerycustomer',
               'serving_names': [
                 'GROCERYCUSTOMERGUID'
               ],
               'catalog_name': 'grocery'
@@ -563,15 +538,16 @@
             'default': 1.0
           },
           'default_feature_fraction': {
             'this': 1.0,
             'default': 1.0
           },
           'versions_info': None,
-          'deployed': False
+          'deployed': False,
+          'namespace_description': None
         }
         """
         return super().info(verbose)
 
     @property
     def feature_names(self) -> list[str]:
         """
@@ -650,34 +626,18 @@
             feature_list_namespace = FeatureListNamespace.get(name=name)
             return cls.get_by_id(id=feature_list_namespace.default_feature_list_id)
         return cls._get(name=name, other_params={"version": version})
 
     def _get_create_payload(self) -> dict[str, Any]:
         feature_ids = [feature.id for feature in self.feature_objects.values()]
         data = FeatureListCreate(
-            **{**self.json_dict(exclude_none=True), "feature_ids": feature_ids}
+            **{**self.dict(by_alias=True, exclude_none=True), "feature_ids": feature_ids}
         )
         return data.json_dict()
 
-    def _pre_save_operations(self, conflict_resolution: ConflictResolution = "raise") -> None:
-        with alive_bar(
-            total=len(self.feature_objects),
-            title="Saving Feature(s)",
-            **get_alive_bar_additional_params(),
-        ) as progress_bar:
-            for feat_name in self.feature_objects:
-                text = f'Feature "{feat_name}" has been saved before.'
-                if not self.feature_objects[feat_name].saved:
-                    self.feature_objects[feat_name].save(conflict_resolution=conflict_resolution)
-                    text = f'Feature "{feat_name}" is saved.'
-
-                # update progress bar
-                progress_bar.text = text
-                progress_bar()  # pylint: disable=not-callable
-
     def save(
         self, conflict_resolution: ConflictResolution = "raise", _id: Optional[ObjectId] = None
     ) -> None:
         """
         Adds a FeatureList object to the catalog.
 
         A conflict could be triggered when the object being saved has violated a uniqueness check at the catalog.
@@ -688,36 +648,39 @@
         ----------
         conflict_resolution: ConflictResolution
             "raise" raises error when we encounter a conflict error (default).
             "retrieve" handle conflict error by retrieving the object with the same name.
         _id: Optional[ObjectId]
             The object ID to be used when saving the object. If not provided, a new object ID will be generated.
 
-        Raises
-        ------
-        DuplicatedRecordException
-            When a record with the same key exists at the persistent data store.
-
         Examples
         --------
         >>> feature_list = fb.FeatureList([
         ...     catalog.get_feature("InvoiceCount_60days"),
         ...     catalog.get_feature("InvoiceAmountAvg_60days"),
         ... ], name="feature_lists_invoice_features")
         >>> feature_list.save()  # doctest: +SKIP
         """
-        try:
-            super().save(conflict_resolution=conflict_resolution)
-        except DuplicatedRecordException as exc:
-            if conflict_resolution == "raise":
-                raise DuplicatedRecordException(
-                    exc.response,
-                    resolution=' Or try `feature_list.save(conflict_resolution = "retrieve")` to resolve conflict.',
-                ) from exc
-            raise exc
+        assert self.name is not None, "FeatureList name cannot be None"
+        self._check_object_not_been_saved(conflict_resolution=conflict_resolution)
+
+        # prepare feature list batch feature create payload
+        feature_list_batch_feature_create = self._get_feature_list_batch_feature_create_payload(
+            feature_list_id=self.id,
+            feature_list_name=self.name,
+            conflict_resolution=conflict_resolution,
+        )
+        self.post_async_task(
+            route="/feature_list/batch",
+            payload=feature_list_batch_feature_create.json_dict(),
+            retrieve_result=False,
+            has_output_url=False,
+        )
+        object_dict = self._get_object_dict_by_id(id_value=feature_list_batch_feature_create.id)
+        type(self).__init__(self, **object_dict, **self._get_init_params_from_object())
 
     def delete(self) -> None:
         """
         Deletes a FeatureList object from the persistent data store. A feature list can only be deleted from the
         persistent data store if:
 
         - the feature list status is DRAFT
@@ -954,24 +917,21 @@
         Returns
         -------
         Feature list status
         """
         return self.feature_list_namespace.status
 
     @classmethod
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        feature_lists = super()._post_process_list(item_list)
-        feature_lists["version"] = feature_lists["version"].apply(
-            lambda version_dict: VersionIdentifier(**version_dict).to_str()
-        )
-        feature_lists["num_feature"] = feature_lists.feature_ids.apply(len)
-        feature_lists["online_frac"] = (
-            feature_lists.online_enabled_feature_ids.apply(len) / feature_lists["num_feature"]
+    def _list_handler(cls) -> ListHandler:
+        return FeatureListListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
         )
-        return feature_lists
 
     @classmethod
     def _list_versions(cls, include_id: Optional[bool] = True) -> pd.DataFrame:
         """
         Returns a DataFrame that presents a summary of the feature list versions belonging to the namespace of the
         FeatureList object. The DataFrame contains multiple attributes of the feature list versions, such as their
         versions names, deployment states, creation dates and the percentage of their features that are
@@ -987,24 +947,24 @@
         pd.DataFrame
             Table of feature lists
 
         Examples
         --------
         List saved FeatureList versions (calling from FeatureList class):
 
-        >>> FeatureList.list_versions()  # doctest: +SKIP
-                           name  num_feature  online_frac  deployed              created_at
-        0  invoice_feature_list            1          0.0     False 2023-03-24 05:05:24.875
+        >>> FeatureList.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                           name  version  num_feature  online_frac  deployed  created_at  is_default
+        0  invoice_feature_list      ...            1          0.0     False         ...  True
 
         List FeatureList versions with the same name (calling from FeatureList object):
 
-        >>> feature_list = catalog.get_feature_list("invoice_feature_list") # doctest: +SKIP
-        >>> feature_list.list_versions()  # doctest: +SKIP
-                           name  num_feature  online_frac  deployed              created_at
-        0  invoice_feature_list            1          0.0     False 2023-03-24 01:53:51.515
+        >>> feature_list = catalog.get_feature_list("invoice_feature_list")  # doctest: +ELLIPSIS
+        >>> feature_list.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                           name  version  online_frac  deployed created_at  is_default
+        0  invoice_feature_list      ...          0.0     False        ...        True
 
         See Also
         --------
         - [FeatureList.list_features](/reference/featurebyte.api.feature_list.FeatureList.list_features/)
         """
         return super().list(include_id=include_id)
 
@@ -1021,17 +981,17 @@
         -------
         pd.DataFrame
             Table of features with the same name
 
         Examples
         --------
         >>> feature_list = catalog.get_feature_list("invoice_feature_list")
-        >>> feature_list.list_versions()  # doctest: +SKIP
-                           name  online_frac  deployed              created_at  is_default
-        0  invoice_feature_list          0.0     False 2023-03-24 01:53:51.515        True
+        >>> feature_list.list_versions(include_id=False)  # doctest: +ELLIPSIS
+                           name  version  online_frac  deployed  created_at  is_default
+        0  invoice_feature_list      ...          0.0     False         ...        True
         """
         output = self._list(include_id=True, params={"name": self.name})
         default_feature_list_id = self.feature_list_namespace.default_feature_list_id
         output["is_default"] = output["id"] == default_feature_list_id
         exclude_cols = {"num_feature"}
         if not include_id:
             exclude_cols.add("id")
@@ -1359,14 +1319,15 @@
         ...         blind_spot="60s",
         ...         frequency="3600s",
         ...         time_modulo_frequency="90s",
         ...       )
         ...     )
         ...   ]
         ... )
+        >>> current_feature.update_readiness("DEPRECATED")
         >>> new_feature.update_default_version_mode("MANUAL")
         >>> new_feature.as_default_version()
         >>> new_feature.is_default is True and current_feature.is_default is False
         True
 
         Create new version of feature list without specifying feature (uses the current default versions of feature).
 
@@ -1380,14 +1341,15 @@
         >>> new_feature_list = feature_list.create_new_version(
         ...   features=[fb.FeatureVersionInfo(name="InvoiceCount_60days", version=new_feature.version)]
         ... )
 
         Reset the default version mode of the feature to make original feature as default. Create a new version
         of feature list using original feature list should throw an error due to no change in feature list is detected.
 
+        >>> current_feature.update_readiness("PRODUCTION_READY")
         >>> current_feature.update_default_version_mode("AUTO")
         >>> current_feature.is_default
         True
         >>> feature_list.create_new_version()  # doctest: +ELLIPSIS
         Traceback (most recent call last):
         ...
         RecordCreationException: No change detected on the new feature list version.
@@ -1574,7 +1536,19 @@
         return Deployment.get_by_id(ObjectId(output["_id"]))
 
     # descriptors
     list_versions: ClassVar[ClassInstanceMethodDescriptor] = ClassInstanceMethodDescriptor(
         class_method=_list_versions,
         instance_method=_list_versions_with_same_name,
     )
+
+    @typechecked
+    def update_namespace_description(self, description: Optional[str]) -> None:
+        """
+        Update description of object namespace
+
+        Parameters
+        ----------
+        description: Optional[str]
+            Description of the object namespace
+        """
+        self.feature_list_namespace.update_description(description=description)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_namespace.py` & `featurebyte-0.4.0/featurebyte/api/feature_namespace.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,33 +1,31 @@
 """
 Feature Namespace module.
 """
 from typing import List, Optional, Union
 
 import pandas as pd
 
-from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_handler.base import ListHandler
+from featurebyte.api.api_handler.feature_namespace import FeatureNamespaceListHandler
+from featurebyte.api.feature_or_target_namespace_mixin import FeatureOrTargetNamespaceMixin
 from featurebyte.api.feature_util import (
     FEATURE_COMMON_LIST_FIELDS,
     FEATURE_LIST_FOREIGN_KEYS,
     filter_feature_list,
 )
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.feature import (
-    DefaultVersionMode,
-    FeatureReadiness,
-    FrozenFeatureNamespaceModel,
-)
+from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.schema.feature_namespace import (
     FeatureNamespaceModelResponse,
     FeatureNamespaceUpdate,
 )
 
 
-class FeatureNamespace(FrozenFeatureNamespaceModel, ApiObject):
+class FeatureNamespace(FeatureOrTargetNamespaceMixin):
     """
     FeatureNamespace represents a Feature set, in which all the features in the set have the same name. The different
     elements typically refer to different versions of a Feature.
     """
 
     # class variables
     _route = "/feature_namespace"
@@ -80,37 +78,22 @@
 
         Returns
         -------
         PydanticObjectId
         """
         return self.cached_model.default_feature_id
 
-    @property
-    def default_version_mode(self) -> DefaultVersionMode:
-        """
-        Default feature namespace version mode of this feature namespace
-
-        Returns
-        -------
-        DefaultVersionMode
-        """
-        return self.cached_model.default_version_mode
-
     @classmethod
-    def _post_process_list(cls, item_list: pd.DataFrame) -> pd.DataFrame:
-        features = super()._post_process_list(item_list)
-
-        # replace id with default_feature_id
-        features["id"] = features["default_feature_id"]
-
-        # add online_enabled
-        features["online_enabled"] = features[
-            ["default_feature_id", "online_enabled_feature_ids"]
-        ].apply(lambda row: row[0] in row[1], axis=1)
-        return features
+    def _list_handler(cls) -> ListHandler:
+        return FeatureNamespaceListHandler(
+            route=cls._route,
+            list_schema=cls._list_schema,
+            list_fields=cls._list_fields,
+            list_foreign_keys=cls._list_foreign_keys,
+        )
 
     @classmethod
     def list(
         cls,
         include_id: Optional[bool] = False,
         primary_entity: Optional[Union[str, List[str]]] = None,
         primary_table: Optional[Union[str, List[str]]] = None,
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_store.py` & `featurebyte-0.4.0/featurebyte/api/feature_store.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,15 +38,15 @@
     _list_fields = ["name", "type", "created_at"]
 
     # optional credential parameters
     database_credential: Optional[DatabaseCredential] = None
     storage_credential: Optional[StorageCredential] = None
 
     def _get_create_payload(self) -> dict[str, Any]:
-        data = FeatureStoreCreate(**self.json_dict())
+        data = FeatureStoreCreate(**self.dict(by_alias=True))
         return data.json_dict()
 
     def info(self, verbose: bool = False) -> Dict[str, Any]:
         """
         Returns a dictionary that summarizes the essential information of the feature store represented by the
         FeatureStore object. The dictionary contains the following keys:
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_util.py` & `featurebyte-0.4.0/featurebyte/api/feature_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """
 Feature util module.
 """
 from typing import List, Optional, Union
 
 import pandas as pd
 
-from featurebyte.api.api_object import ForeignKeyMapping
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.api.entity import Entity
 from featurebyte.common.utils import convert_to_list_of_strings
 
 FEATURE_COMMON_LIST_FIELDS = [
     "dtype",
     "readiness",
```

### Comparing `featurebyte-0.3.1/featurebyte/api/feature_validation_util.py` & `featurebyte-0.4.0/featurebyte/api/feature_validation_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/groupby.py` & `featurebyte-0.4.0/featurebyte/api/groupby.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,23 +4,25 @@
 from __future__ import annotations
 
 from typing import List, Literal, Optional, Union
 
 from typeguard import typechecked
 
 from featurebyte import FeatureJobSetting
+from featurebyte.api.aggregator.forward_aggregator import ForwardAggregator
 from featurebyte.api.asat_aggregator import AsAtAggregator
 from featurebyte.api.change_view import ChangeView
 from featurebyte.api.entity import Entity
 from featurebyte.api.event_view import EventView
 from featurebyte.api.feature import Feature
 from featurebyte.api.feature_group import FeatureGroup
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.scd_view import SCDView
 from featurebyte.api.simple_aggregator import SimpleAggregator
+from featurebyte.api.target import Target
 from featurebyte.api.window_aggregator import WindowAggregator
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 
 
 class GroupBy:
@@ -414,7 +416,57 @@
         ).aggregate(
             value_column=value_column,
             method=method,
             feature_name=feature_name,
             fill_value=fill_value,
             skip_fill_na=skip_fill_na,
         )
+
+    def forward_aggregate(
+        self,
+        value_column: str,
+        method: str,
+        window: Optional[str] = None,
+        target_name: Optional[str] = None,
+    ) -> Target:
+        """
+        The forward_aggregate method of a GroupBy class instance returns a Forward Aggregated Target object. This object
+        aggregates data from the column specified by the value_column parameter using the aggregation method
+        provided by the method parameter, without taking into account the order or sequence of the data. The primary
+        entity of the Target is determined by the grouping key of the GroupBy instance.
+
+        Parameters
+        ----------
+        value_column: str
+            Column to be aggregated
+        method: str
+            Aggregation method.
+        window: Optional[str]
+            Optional window to apply to the point in time column in the target request.
+        target_name: Optional[str]
+            Output target name
+
+        Returns
+        -------
+        Target
+
+        Examples
+        --------
+        >>> items_view = catalog.get_view("INVOICEITEMS")
+        >>> # Group items by the column GroceryInvoiceGuid that references the customer entity
+        >>> items_by_invoice = items_view.groupby("GroceryInvoiceGuid")
+        >>> # Get the number of items in each invoice
+        >>> invoice_item_count = items_by_invoice.forward_aggregate(  # doctest: +SKIP
+        ...   "TotalCost",
+        ...   method=fb.AggFunc.SUM,
+        ...   target_name="TargetCustomerInventory_28d",
+        ...   window='28d'
+        ... )
+        """
+        return ForwardAggregator(
+            self.view_obj, self.category, self.entity_ids, self.keys, self.serving_names
+        ).forward_aggregate(
+            value_column=value_column,
+            method=method,
+            window=window,
+            target_name=target_name,
+        )
```

### Comparing `featurebyte-0.3.1/featurebyte/api/historical_feature_table.py` & `featurebyte-0.4.0/featurebyte/api/historical_feature_table.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.schema.historical_feature_table import HistoricalFeatureTableListRecord
```

### Comparing `featurebyte-0.3.1/featurebyte/api/item_table.py` & `featurebyte-0.4.0/featurebyte/api/item_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/item_view.py` & `featurebyte-0.4.0/featurebyte/api/item_view.py`

 * *Files 1% similar despite different names*

```diff
@@ -299,15 +299,18 @@
         column_names : List[str]
             Column names in ItemView to check
 
         Returns
         -------
         bool
         """
-        operation_structure = self.graph.extract_operation_structure(self.node)
+        # FIXME: This method is may not work correctly when the column is from SCD table joined with EventTable
+        operation_structure = self.graph.extract_operation_structure(
+            self.node, keep_all_source_columns=False
+        )
         for column_name in column_names:
             column_structure = next(
                 column for column in operation_structure.columns if column.name == column_name
             )
             if isinstance(column_structure, DerivedDataColumn):
                 if not all(
                     input_column.table_type == TableDataType.EVENT_TABLE
```

### Comparing `featurebyte-0.3.1/featurebyte/api/lag.py` & `featurebyte-0.4.0/featurebyte/api/lag.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/materialized_table.py` & `featurebyte-0.4.0/featurebyte/api/materialized_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/observation_table.py` & `featurebyte-0.4.0/featurebyte/api/observation_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,16 @@
 
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.schema.observation_table import ObservationTableListRecord
```

### Comparing `featurebyte-0.3.1/featurebyte/api/periodic_task.py` & `featurebyte-0.4.0/featurebyte/api/periodic_task.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/relationship.py` & `featurebyte-0.4.0/featurebyte/api/relationship.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,16 @@
 """
 from typing import Any, Dict, Literal, Optional
 
 import pandas as pd
 from pydantic import Field
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ApiObject, ForeignKeyMapping
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.api.entity import Entity
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.relationship import RelationshipInfoModel, RelationshipType
 from featurebyte.schema.relationship_info import RelationshipInfoUpdate
```

### Comparing `featurebyte-0.3.1/featurebyte/api/request_column.py` & `featurebyte-0.4.0/featurebyte/api/request_column.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/savable_api_object.py` & `featurebyte-0.4.0/featurebyte/api/savable_api_object.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,21 +6,22 @@
 from typing import Any, Optional
 
 from http import HTTPStatus
 
 from bson import ObjectId
 from typeguard import typechecked
 
-from featurebyte.api.api_object import ApiObject, ConflictResolution
+from featurebyte.api.api_object import ApiObject
+from featurebyte.api.api_object_util import delete_api_object_by_id
 from featurebyte.config import Configurations
+from featurebyte.enum import ConflictResolution
 from featurebyte.exception import (
     DuplicatedRecordException,
     ObjectHasBeenSavedError,
     RecordCreationException,
-    RecordDeletionException,
 )
 
 
 class SavableApiObject(ApiObject):
     """
     ApiObject contains common methods used to interact with API routes
     """
@@ -31,25 +32,19 @@
 
         Returns
         -------
         dict[str, Any]
         """
         return self.json_dict(exclude_none=True)
 
-    def _pre_save_operations(self, conflict_resolution: ConflictResolution) -> None:
-        """
-        Operations to be executed before saving the api object
-
-        Parameters
-        ----------
-        conflict_resolution: ConflictResolution
-            "raise" raises error when then counters conflict error (default)
-            "retrieve" handle conflict error by retrieving object with the same name
-        """
-        _ = conflict_resolution
+    def _check_object_not_been_saved(self, conflict_resolution: ConflictResolution) -> None:
+        if self.saved and conflict_resolution == "raise":
+            raise ObjectHasBeenSavedError(
+                f'{type(self).__name__} (id: "{self.id}") has been saved before.'
+            )
 
     @typechecked
     def save(
         self, conflict_resolution: ConflictResolution = "raise", _id: Optional[ObjectId] = None
     ) -> None:
         """
         Save an object to the persistent data store.
@@ -66,16 +61,14 @@
             "raise" will raise an error when we encounter a conflict error.
             "retrieve" will handle the conflict error by retrieving the object with the same name.
         _id: Optional[ObjectId]
             The object ID to be used when saving the object. If not provided, a new object ID will be generated.
 
         Raises
         ------
-        ObjectHasBeenSavedError
-            If the object has been saved before.
         DuplicatedRecordException
             When a record with the same key exists at the persistent data store.
         RecordCreationException
             When we fail to save the new object (general failure).
 
         Examples
         --------
@@ -90,20 +83,15 @@
         Calling save again returns an error.
 
         >>> entity = fb.Entity(name="grocerycustomer", serving_names=["GROCERYCUSTOMERGUID"])  # doctest: +SKIP
         >>> entity.save()  # doctest: +SKIP
         >>> entity.save()  # doctest: +SKIP
         Entity (id: <entity.id>) has been saved before.
         """
-        if self.saved and conflict_resolution == "raise":
-            raise ObjectHasBeenSavedError(
-                f'{type(self).__name__} (id: "{self.id}") has been saved before.'
-            )
-
-        self._pre_save_operations(conflict_resolution=conflict_resolution)
+        self._check_object_not_been_saved(conflict_resolution=conflict_resolution)
         client = Configurations().get_client()
         payload = self._get_create_payload()
         if _id is not None:
             payload["_id"] = str(_id)
         response = client.post(url=self._route, json=payload)
         retrieve_object = False
         if response.status_code != HTTPStatus.CREATED:
@@ -131,11 +119,8 @@
 
 class DeletableApiObject(ApiObject):
     """
     DeleteMixin contains common methods used to delete an object
     """
 
     def _delete(self) -> None:
-        client = Configurations().get_client()
-        response = client.delete(url=f"{self._route}/{self.id}")
-        if response.status_code != HTTPStatus.OK:
-            raise RecordDeletionException(response, "Failed to delete the specified object.")
+        delete_api_object_by_id(route=self._route, id_value=self.id)
```

### Comparing `featurebyte-0.3.1/featurebyte/api/scd_table.py` & `featurebyte-0.4.0/featurebyte/api/scd_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/scd_view.py` & `featurebyte-0.4.0/featurebyte/api/scd_view.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/simple_aggregator.py` & `featurebyte-0.4.0/featurebyte/api/simple_aggregator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """
 This module contains simple aggregator related class
 """
 from __future__ import annotations
 
 from typing import List, Literal, Optional, Type
 
-from featurebyte.api.base_aggregator import BaseAggregator
+from featurebyte.api.aggregator.base_aggregator import BaseAggregator
 from featurebyte.api.feature import Feature
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.view import View
 from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.node.agg_func import construct_agg_func
 from featurebyte.query_graph.transform.reconstruction import (
```

### Comparing `featurebyte-0.3.1/featurebyte/api/source_table.py` & `featurebyte-0.4.0/featurebyte/api/source_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,47 +4,47 @@
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, ClassVar, List, Optional, Tuple, Type, TypeVar, Union, cast
 
 from abc import ABC, abstractmethod
 from datetime import datetime
-from http import HTTPStatus
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
 
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.config import Configurations
 from featurebyte.core.frame import BaseFrame
 from featurebyte.enum import DBVarType
-from featurebyte.exception import RecordRetrievalException
 from featurebyte.logging import get_logger
 from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.batch_request_table import SourceTableBatchRequestInput
 from featurebyte.models.feature_store import ConstructGraphMixin, FeatureStoreModel
-from featurebyte.models.request_input import SourceTableRequestInput
+from featurebyte.models.observation_table import SourceTableObservationInput
+from featurebyte.models.static_source_table import SourceTableStaticSourceInput
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.common_table import BaseTableData, TabularSource
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.model.table import AllTableDataT, SourceTableData
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.input import InputNode
-from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate
 from featurebyte.schema.observation_table import ObservationTableCreate
+from featurebyte.schema.static_source_table import StaticSourceTableCreate
 
 if TYPE_CHECKING:
     from featurebyte.api.batch_request_table import BatchRequestTable
     from featurebyte.api.dimension_table import DimensionTable
     from featurebyte.api.event_table import EventTable
     from featurebyte.api.item_table import ItemTable
     from featurebyte.api.observation_table import ObservationTable
     from featurebyte.api.scd_table import SCDTable
+    from featurebyte.api.static_source_table import StaticSourceTable
 else:
     DimensionTable = TypeVar("DimensionTable")
     EventTable = TypeVar("EventTable")
     ItemTable = TypeVar("ItemTable")
     SCDTable = TypeVar("SCDTable")
 
 
@@ -71,15 +71,15 @@
             assert isinstance(node, InputNode)
             graph_node = self.table_data.construct_cleaning_recipe_node(
                 input_node=node, skip_column_names=[]
             )
             if graph_node:
                 node = self.graph.add_node(node=graph_node, input_nodes=[self.node])
 
-        pruned_graph, node_name_map = self.graph.prune(target_node=node, aggressive=True)
+        pruned_graph, node_name_map = self.graph.prune(target_node=node)
         mapped_node = pruned_graph.get_node_by_name(node_name_map[node.name])
         return pruned_graph, mapped_node
 
 
 class AbstractTableData(ConstructGraphMixin, FeatureByteBaseModel, ABC):
     """
     AbstractTableDataFrame class represents the table.
@@ -92,77 +92,39 @@
     tabular_source: TabularSource = Field(allow_mutation=False)
     feature_store: FeatureStoreModel = Field(allow_mutation=False, exclude=True)
 
     # pydantic instance variable (internal use)
     internal_columns_info: List[ColumnInfo] = Field(alias="columns_info")
 
     def __init__(self, **kwargs: Any):
-        # Construct feature_store, input node & set the graph related parameters based on the given input dictionary
+        # construct feature_store based on the given input dictionary
         values = kwargs
         tabular_source = dict(values["tabular_source"])
-        table_details = tabular_source["table_details"]
         if "feature_store" not in values:
             # attempt to set feature_store object if it does not exist in the input
             from featurebyte.api.feature_store import (  # pylint: disable=import-outside-toplevel,cyclic-import
                 FeatureStore,
             )
 
             values["feature_store"] = FeatureStore.get_by_id(id=tabular_source["feature_store_id"])
 
-        feature_store = values["feature_store"]
-        if isinstance(table_details, dict):
-            table_details = TableDetails(**table_details)
-
-        to_validate_schema = values.get("_validate_schema") or "columns_info" not in values
-        if to_validate_schema:
-            client = Configurations().get_client()
-            response = client.post(
-                url=(
-                    f"/feature_store/column?"
-                    f"database_name={table_details.database_name}&"
-                    f"schema_name={table_details.schema_name}&"
-                    f"table_name={table_details.table_name}"
-                ),
-                json=feature_store.json_dict(),
-            )
-            if response.status_code == HTTPStatus.OK:
-                column_specs = response.json()
-                recent_schema = {
-                    column_spec["name"]: DBVarType(column_spec["dtype"])
-                    for column_spec in column_specs
-                }
-            else:
-                raise RecordRetrievalException(response)
-
-            if "columns_info" in values:
-                columns_info = [ColumnInfo(**dict(col)) for col in values["columns_info"]]
-                schema = {col.name: col.dtype for col in columns_info}
-                if not recent_schema.items() >= schema.items():
-                    logger.warning("Table schema has been changed.")
-            else:
-                columns_info = [
-                    ColumnInfo(name=name, dtype=var_type)
-                    for name, var_type in recent_schema.items()
-                ]
-                values["columns_info"] = columns_info
-
         # call pydantic constructor to validate input parameters
         super().__init__(**values)
 
     @property
     def table_data(self) -> BaseTableData:
         """
         Table data object of this table. This object contains information used to construct the SQL query.
 
         Returns
         -------
         BaseTableData
             Table data object used for SQL query construction.
         """
-        return self._table_data_class(**self.json_dict())
+        return self._table_data_class(**self.dict(by_alias=True))
 
     @property
     def frame(self) -> TableDataFrame:
         """
         Frame object of this table object. The frame is constructed from a local query graph.
 
         Returns
@@ -532,15 +494,15 @@
             EventTable created from the source table.
 
         Examples
         --------
         Create an event table from a source table.
 
         >>> # Register GroceryInvoice as an event data
-        >>> source_table = ds.get_table(  # doctest: +SKIP
+        >>> source_table = ds.get_source_table(  # doctest: +SKIP
         ...   database_name="spark_catalog",
         ...   schema_name="GROCERY",
         ...   table_name="GROCERYINVOICE"
         ... )
         >>> invoice_table = source_table.create_event_table(  # doctest: +SKIP
         ...   name="GROCERYINVOICE",
         ...   event_id_column="GroceryInvoiceGuid",
@@ -614,15 +576,15 @@
 
         >>> # Register invoice items as an item table
         >>> source_table = ds.get_table(  # doctest: +SKIP
         ...   database_name="spark_catalog",
         ...   schema_name="GROCERY",
         ...   table_name="INVOICEITEMS"
         ... )
-        >>> items_table.create_item_table(  # doctest: +SKIP
+        >>> source_table.create_item_table(  # doctest: +SKIP
         ...   name="INVOICEITEMS",
         ...   event_id_column="GroceryInvoiceGuid",
         ...   item_id_column="GroceryInvoiceItemGuid",
         ...   event_table_name="GROCERYINVOICE"
         ... )
         """
         # pylint: disable=import-outside-toplevel
@@ -1050,15 +1012,15 @@
         """
         # pylint: disable=import-outside-toplevel
         from featurebyte.api.observation_table import ObservationTable
 
         payload = ObservationTableCreate(
             name=name,
             feature_store_id=self.feature_store.id,
-            request_input=SourceTableRequestInput(
+            request_input=SourceTableObservationInput(
                 source=self.tabular_source,
                 columns=columns,
                 columns_rename_mapping=columns_rename_mapping,
             ),
             sample_rows=sample_rows,
         )
         observation_table_doc = ObservationTable.post_async_task(
@@ -1109,17 +1071,76 @@
         """
         # pylint: disable=import-outside-toplevel
         from featurebyte.api.batch_request_table import BatchRequestTable
 
         payload = BatchRequestTableCreate(
             name=name,
             feature_store_id=self.feature_store.id,
-            request_input=SourceTableRequestInput(
+            request_input=SourceTableBatchRequestInput(
                 source=self.tabular_source,
                 columns=columns,
                 columns_rename_mapping=columns_rename_mapping,
             ),
         )
         batch_request_table_doc = BatchRequestTable.post_async_task(
             route="/batch_request_table", payload=payload.json_dict()
         )
         return BatchRequestTable.get_by_id(batch_request_table_doc["_id"])
+
+    def create_static_source_table(
+        self,
+        name: str,
+        sample_rows: Optional[int] = None,
+        columns: Optional[list[str]] = None,
+        columns_rename_mapping: Optional[dict[str, str]] = None,
+    ) -> StaticSourceTable:
+        """
+        Creates a StaticSourceTable from the SourceTable.
+
+        Parameters
+        ----------
+        name: str
+            Static source table name.
+        sample_rows: Optional[int]
+            Optionally sample the source table to this number of rows before creating the
+            static source table.
+        columns: Optional[list[str]]
+            Include only these columns when creating the static source table. If None, all columns are
+            included.
+        columns_rename_mapping: Optional[dict[str, str]]
+            Rename columns in the source table using this mapping from old column names to new
+            column names when creating the static source table. If None, no columns are renamed.
+
+        Returns
+        -------
+        StaticSourceTable
+
+        Examples
+        --------
+        >>> ds = fb.FeatureStore.get(<feature_store_name>).get_data_source()  # doctest: +SKIP
+        >>> source_table = ds.get_source_table(  # doctest: +SKIP
+        ...   database_name="<data_base_name>",
+        ...   schema_name="<schema_name>",
+        ...   table_name=<table_name>
+        ... )
+        >>> static_source_table = source_table.create_static_source_table(  # doctest: +SKIP
+        ...   name="<static_source_table_name>",
+        ...   sample_rows=desired_sample_size,
+        ... )
+        """
+        # pylint: disable=import-outside-toplevel
+        from featurebyte.api.static_source_table import StaticSourceTable
+
+        payload = StaticSourceTableCreate(
+            name=name,
+            feature_store_id=self.feature_store.id,
+            request_input=SourceTableStaticSourceInput(
+                source=self.tabular_source,
+                columns=columns,
+                columns_rename_mapping=columns_rename_mapping,
+            ),
+            sample_rows=sample_rows,
+        )
+        static_source_table_doc = StaticSourceTable.post_async_task(
+            route="/static_source_table", payload=payload.json_dict()
+        )
+        return StaticSourceTable.get_by_id(static_source_table_doc["_id"])
```

### Comparing `featurebyte-0.3.1/featurebyte/api/table.py` & `featurebyte-0.4.0/featurebyte/api/table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/templates/online_serving/python.tpl` & `featurebyte-0.4.0/featurebyte/api/templates/online_serving/python.tpl`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/api/view.py` & `featurebyte-0.4.0/featurebyte/api/view.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,14 +26,15 @@
 from typeguard import typechecked
 
 from featurebyte.api.batch_request_table import BatchRequestTable
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature import Feature
 from featurebyte.api.feature_group import FeatureGroup
 from featurebyte.api.observation_table import ObservationTable
+from featurebyte.api.static_source_table import StaticSourceTable
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.join_utils import (
     append_rsuffix_to_column_info,
     append_rsuffix_to_columns,
     combine_column_info_of_views,
     filter_columns,
     filter_columns_info,
@@ -48,29 +49,30 @@
 from featurebyte.enum import DBVarType
 from featurebyte.exception import (
     ChangeViewNoJoinColumnError,
     NoJoinKeyFoundError,
     RepeatedColumnNamesError,
 )
 from featurebyte.logging import get_logger
-from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.batch_request_table import ViewBatchRequestInput
 from featurebyte.models.observation_table import ViewObservationInput
+from featurebyte.models.static_source_table import ViewStaticSourceInput
 from featurebyte.query_graph.enum import GraphNodeType, NodeOutputType, NodeType
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.cleaning_operation import (
     CleaningOperation,
     ColumnCleaningOperation,
 )
 from featurebyte.query_graph.node.generic import JoinMetadata, ProjectNode
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.node.nested import BaseGraphNode
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate
 from featurebyte.schema.observation_table import ObservationTableCreate
+from featurebyte.schema.static_source_table import StaticSourceTableCreate
 
 if TYPE_CHECKING:
     from featurebyte.api.groupby import GroupBy
 else:
     GroupBy = TypeVar("GroupBy")
 
 ViewT = TypeVar("ViewT", bound="View")
@@ -1001,15 +1003,15 @@
         Returns
         -------
         ViewT
         """
         return type(self)(
             feature_store=self.feature_store,
             **{
-                **self.json_dict(exclude={"feature_store": True}),
+                **self.dict(by_alias=True, exclude={"feature_store": True}),
                 "graph": self.graph,
                 "node_name": new_node_name,
                 "columns_info": joined_columns_info,
                 **self._get_create_joined_view_parameters(),
                 **kwargs,
             },
         )
@@ -1319,29 +1321,26 @@
             validate_offset_string(offset)
 
     def _project_feature_from_node(
         self,
         node: Node,
         feature_name: str,
         feature_dtype: DBVarType,
-        entity_ids: List[PydanticObjectId],
     ) -> Feature:
         """
         Create a Feature object from a node that produces features, such as groupby, lookup, etc.
 
         Parameters
         ----------
         node: Node
             Query graph node
         feature_name: str
             Feature name
         feature_dtype: DBVarType
             Variable type of the Feature
-        entity_ids: List[PydanticObjectId]
-            Entity ids associated with the Feature
 
         Returns
         -------
         Feature
         """
         feature_node = self.graph.add_operation(
             node_type=NodeType.PROJECT,
@@ -1352,15 +1351,14 @@
 
         feature = Feature(
             name=feature_name,
             feature_store=self.feature_store,
             tabular_source=self.tabular_source,
             node_name=feature_node.name,
             dtype=feature_dtype,
-            entity_ids=entity_ids,
         )
         return feature
 
     def _validate_as_features_input_columns(
         self,
         column_names: list[str],
         feature_names: list[str],
@@ -1498,15 +1496,14 @@
         )
         features = []
         for input_column_name, feature_name in zip(column_names, feature_names):
             feature = self._project_feature_from_node(
                 node=lookup_node,
                 feature_name=feature_name,
                 feature_dtype=self.column_var_type_map[input_column_name],
-                entity_ids=[entity_id],
             )
             features.append(feature)
 
         return FeatureGroup(features)
 
     def create_observation_table(
         self,
@@ -1617,7 +1614,60 @@
                 columns_rename_mapping=columns_rename_mapping,
             ),
         )
         batch_request_table_doc = BatchRequestTable.post_async_task(
             route="/batch_request_table", payload=payload.json_dict()
         )
         return BatchRequestTable.get_by_id(batch_request_table_doc["_id"])
+
+    def create_static_source_table(
+        self,
+        name: str,
+        sample_rows: Optional[int] = None,
+        columns: Optional[list[str]] = None,
+        columns_rename_mapping: Optional[dict[str, str]] = None,
+    ) -> StaticSourceTable:
+        """
+        Creates an StaticSourceTable from the View.
+
+        Parameters
+        ----------
+        name: str
+            Name of the StaticSourceTable.
+        sample_rows: Optional[int]
+            Optionally sample the source table to this number of rows before creating the
+            static source table.
+        columns: Optional[list[str]]
+            Include only these columns in the view when creating the static source table. If None, all
+            columns are included.
+        columns_rename_mapping: Optional[dict[str, str]]
+            Rename columns in the view using this mapping from old column names to new column names
+            when creating the static source table. If None, no columns are renamed.
+
+        Returns
+        -------
+        StaticSourceTable
+            StaticSourceTable object.
+
+        Examples
+        --------
+        >>> static_source_table = view.create_static_source_table(  # doctest: +SKIP
+        ...   name="<static_source_table_name>",
+        ...   sample_rows=10000,
+        ... )
+        """
+        pruned_graph, mapped_node = self.extract_pruned_graph_and_node()
+        payload = StaticSourceTableCreate(
+            name=name,
+            feature_store_id=self.feature_store.id,
+            request_input=ViewStaticSourceInput(
+                graph=pruned_graph,
+                node_name=mapped_node.name,
+                columns=columns,
+                columns_rename_mapping=columns_rename_mapping,
+            ),
+            sample_rows=sample_rows,
+        )
+        static_source_table_doc = StaticSourceTable.post_async_task(
+            route="/static_source_table", payload=payload.json_dict()
+        )
+        return StaticSourceTable.get_by_id(static_source_table_doc["_id"])
```

### Comparing `featurebyte-0.3.1/featurebyte/api/window_aggregator.py` & `featurebyte-0.4.0/featurebyte/api/window_aggregator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """
 This module contains window aggregator related class
 """
 from __future__ import annotations
 
 from typing import Any, List, Optional, Type, cast
 
-from featurebyte.api.base_aggregator import BaseAggregator
+from featurebyte.api.aggregator.base_aggregator import BaseAggregator
 from featurebyte.api.change_view import ChangeView
 from featurebyte.api.event_view import EventView
 from featurebyte.api.feature_group import FeatureGroup
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.view import View
 from featurebyte.api.window_validator import validate_window
 from featurebyte.common.typing import OptionalScalar
```

### Comparing `featurebyte-0.3.1/featurebyte/api/window_validator.py` & `featurebyte-0.4.0/featurebyte/api/window_validator.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/app.py` & `featurebyte-0.4.0/featurebyte/app.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """
 FastAPI Application
 """
-from typing import Callable
+from typing import Any, Callable, Coroutine, Optional
 
 import aioredis
 import uvicorn
 from fastapi import Depends, FastAPI, Header, Request
 from starlette.websockets import WebSocket
 
 import featurebyte.routes.batch_feature_table.api as batch_feature_table_api
@@ -26,120 +26,161 @@
 import featurebyte.routes.historical_feature_table.api as historical_feature_table_api
 import featurebyte.routes.item_table.api as item_table_api
 import featurebyte.routes.observation_table.api as observation_table_api
 import featurebyte.routes.periodic_tasks.api as periodic_tasks_api
 import featurebyte.routes.relationship_info.api as relationship_info_api
 import featurebyte.routes.scd_table.api as scd_table_api
 import featurebyte.routes.semantic.api as semantic_api
+import featurebyte.routes.static_source_table.api as static_source_table_api
 import featurebyte.routes.table.api as table_api
+import featurebyte.routes.target.api as target_api
+import featurebyte.routes.target_namespace.api as target_namespace_api
+import featurebyte.routes.target_table.api as target_table_api
 import featurebyte.routes.task.api as task_api
 import featurebyte.routes.temp_data.api as temp_data_api
+import featurebyte.routes.user_defined_function.api as user_defined_function_api
 from featurebyte.common.utils import get_version
 from featurebyte.logging import get_logger
 from featurebyte.middleware import ExceptionMiddleware
-from featurebyte.models.base import DEFAULT_CATALOG_ID, PydanticObjectId, User
-from featurebyte.routes.app_container import AppContainer
+from featurebyte.models.base import PydanticObjectId, User
+from featurebyte.routes.lazy_app_container import LazyAppContainer
+from featurebyte.routes.registry import app_container_config
 from featurebyte.schema import APIServiceStatus
 from featurebyte.schema.task import TaskId
-from featurebyte.service.task_manager import TaskManager
 from featurebyte.utils.credential import MongoBackedCredentialProvider
 from featurebyte.utils.messaging import REDIS_URI
 from featurebyte.utils.persistent import get_persistent
 from featurebyte.utils.storage import get_storage, get_temp_storage
+from featurebyte.worker import get_celery
 
 logger = get_logger(__name__)
 
 
-def _get_api_deps() -> Callable[[Request, PydanticObjectId], None]:
+def _dep_injection_func(
+    request: Request, active_catalog_id: Optional[PydanticObjectId] = None
+) -> None:
+    """
+    Inject dependencies into the requests
+
+    Parameters
+    ----------
+    request: Request
+        Request object to be updated
+    active_catalog_id: Optional[PydanticObjectId]
+        Catalog ID to be used for the request
+    """
+    request.state.persistent = get_persistent()
+    request.state.user = User()
+    request.state.get_credential = MongoBackedCredentialProvider(
+        persistent=request.state.persistent
+    ).get_credential
+    request.state.app_container = LazyAppContainer(
+        user=request.state.user,
+        persistent=request.state.persistent,
+        temp_storage=get_temp_storage(),
+        celery=get_celery(),
+        storage=get_storage(),
+        catalog_id=active_catalog_id,
+        app_container_config=app_container_config,
+    )
+
+
+def _get_api_deps() -> Callable[[Request], Coroutine[Any, Any, None]]:
     """
     Get API dependency injection function
 
     Returns
     -------
-    Callable[Request, PydanticObjectId]
+    Callable[Request]
         Dependency injection function
     """
 
-    def _dep_injection_func(
-        request: Request, active_catalog_id: PydanticObjectId = Header(None)
+    async def _wrapper(
+        request: Request,
     ) -> None:
-        """
-        Inject dependencies into the requests
+        _dep_injection_func(request)
 
-        Parameters
-        ----------
-        request: Request
-            Request object to be updated
-        active_catalog_id: PydanticObjectId
-            Catalog ID from header
-        """
-        active_catalog_id = active_catalog_id or PydanticObjectId(DEFAULT_CATALOG_ID)
-        request.state.persistent = get_persistent()
-        request.state.user = User()
-        request.state.get_credential = MongoBackedCredentialProvider(
-            persistent=request.state.persistent
-        ).get_credential
-        request.state.get_storage = get_storage
-        request.state.get_temp_storage = get_temp_storage
-        request.state.app_container = AppContainer.get_instance(
-            user=request.state.user,
-            persistent=request.state.persistent,
-            temp_storage=get_temp_storage(),
-            task_manager=TaskManager(
-                user=request.state.user,
-                persistent=request.state.persistent,
-                catalog_id=active_catalog_id,
-            ),
-            storage=get_storage(),
-            container_id=active_catalog_id,
-        )
+    return _wrapper
+
+
+def _get_api_deps_with_catalog() -> Callable[[Request], Coroutine[Any, Any, None]]:
+    """
+    Get API dependency injection function with catalog
+
+    Returns
+    -------
+    Callable[Request]
+        Dependency injection function
+    """
+
+    async def _wrapper(
+        request: Request,
+        active_catalog_id: Optional[PydanticObjectId] = Header(None),
+    ) -> None:
+        _dep_injection_func(request, active_catalog_id)
 
-    return _dep_injection_func
+    return _wrapper
 
 
 def get_app() -> FastAPI:
     """
     Get FastAPI object
 
     Returns
     -------
     FastAPI
         FastAPI object
     """
     _app = FastAPI()
 
-    # add routers into the app
+    # register routes that are not catalog-specific
     resource_apis = [
         credential_api,
+        feature_store_api,
+        semantic_api,
+        task_api,
+        temp_data_api,
+        catalog_api,
+    ]
+    dependencies = _get_api_deps()
+    for resource_api in resource_apis:
+        _app.include_router(
+            resource_api.router,
+            dependencies=[Depends(dependencies)],
+            tags=[resource_api.router.prefix[1:]],
+        )
+
+    # register routes that are catalog-specific
+    resource_apis = [
         context_api,
         deployment_api,
         dimension_table_api,
         event_table_api,
         item_table_api,
         entity_api,
         feature_api,
         feature_job_setting_analysis_api,
         feature_list_api,
         feature_list_namespace_api,
         feature_namespace_api,
-        feature_store_api,
         relationship_info_api,
         scd_table_api,
-        semantic_api,
+        static_source_table_api,
         table_api,
-        task_api,
-        temp_data_api,
-        catalog_api,
-        periodic_tasks_api,
         observation_table_api,
         historical_feature_table_api,
         batch_request_table_api,
         batch_feature_table_api,
+        target_api,
+        target_namespace_api,
+        periodic_tasks_api,
+        user_defined_function_api,
+        target_table_api,
     ]
-    dependencies = _get_api_deps()
+    dependencies = _get_api_deps_with_catalog()
     for resource_api in resource_apis:
         _app.include_router(
             resource_api.router,
             dependencies=[Depends(dependencies)],
             tags=[resource_api.router.prefix[1:]],
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/common/date_util.py` & `featurebyte-0.4.0/featurebyte/common/date_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/descriptor.py` & `featurebyte-0.4.0/featurebyte/common/descriptor.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/dict_util.py` & `featurebyte-0.4.0/featurebyte/common/dict_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/doc_util.py` & `featurebyte-0.4.0/featurebyte/common/doc_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/allowed_classes.py` & `featurebyte-0.4.0/featurebyte/common/documentation/allowed_classes.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/autodoc_processor.py` & `featurebyte-0.4.0/featurebyte/common/documentation/autodoc_processor.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/constants.py` & `featurebyte-0.4.0/featurebyte/common/documentation/constants.py`

 * *Files 10% similar despite different names*

```diff
@@ -45,15 +45,18 @@
 REQUEST_COLUMN = "RequestColumn"
 SAVE = "Save"
 SERVE = "Serve"
 SET_FEATURE_JOB = "Set Feature Job"
 SOURCE_TABLE = "SourceTable"
 TABLE = "Table"
 TABLE_COLUMN = "TableColumn"
+TARGET = "Target"
+TARGET_NAMESPACE = "TargetNamespace"
 TRANSFORM = "Transform"
 TYPE = "Type"
+USER_DEFINED_FUNCTION = "UserDefinedFunction"
 UTILITY_CLASSES = "Utility Classes"
 UTILITY_METHODS = "Utility Methods"
 VERSION = "Version"
 VIEW = "View"
 VIEW_COLUMN = "ViewColumn"
 WAREHOUSE = "Warehouse"
```

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/custom_nav.py` & `featurebyte-0.4.0/featurebyte/common/documentation/custom_nav.py`

 * *Files 6% similar despite different names*

```diff
@@ -44,16 +44,19 @@
     REQUEST_COLUMN,
     SAVE,
     SERVE,
     SET_FEATURE_JOB,
     SOURCE_TABLE,
     TABLE,
     TABLE_COLUMN,
+    TARGET,
+    TARGET_NAMESPACE,
     TRANSFORM,
     TYPE,
+    USER_DEFINED_FUNCTION,
     UTILITY_CLASSES,
     UTILITY_METHODS,
     VIEW,
     VIEW_COLUMN,
     WAREHOUSE,
 )
 
@@ -81,14 +84,17 @@
         FEATURE_GROUP,
         FEATURE_LIST,
         OBSERVATION_TABLE,
         HISTORICAL_FEATURE_TABLE,
         DEPLOYMENT,
         BATCH_REQUEST_TABLE,
         BATCH_FEATURE_TABLE,
+        USER_DEFINED_FUNCTION,
+        TARGET,
+        TARGET_NAMESPACE,
         UTILITY_CLASSES,
         UTILITY_METHODS,
     ]
 
     _custom_second_level_order = [
         CREDENTIAL,
         CLASS_METHODS,
@@ -116,14 +122,15 @@
         EXPLORE,
         INFO,
         LINEAGE,
         ENUMS,
         CLEANING_OPERATION,
         WAREHOUSE,
         REQUEST_COLUMN,
+        USER_DEFINED_FUNCTION,
     ]
 
     _custom_order_mapping = {
         0: _custom_root_level_order,
         1: _custom_second_level_order,
     }
```

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/doc_types.py` & `featurebyte-0.4.0/featurebyte/common/documentation/doc_types.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/documentation_layout.py` & `featurebyte-0.4.0/featurebyte/common/documentation/documentation_layout.py`

 * *Files 23% similar despite different names*

```diff
@@ -44,16 +44,19 @@
     REQUEST_COLUMN,
     SAVE,
     SERVE,
     SET_FEATURE_JOB,
     SOURCE_TABLE,
     TABLE,
     TABLE_COLUMN,
+    TARGET,
+    TARGET_NAMESPACE,
     TRANSFORM,
     TYPE,
+    USER_DEFINED_FUNCTION,
     UTILITY_CLASSES,
     UTILITY_METHODS,
     VIEW,
     VIEW_COLUMN,
     WAREHOUSE,
 )
 
@@ -204,30 +207,48 @@
         DocLayoutItem(
             [TABLE, ADD_METADATA, "Table.update_record_creation_timestamp_column"],
             doc_path_override="api.base_table.TableApiObject.update_record_creation_timestamp_column.md",
         ),
     ]
 
 
+def _get_sample_mixin_layout(object_type: str) -> List[DocLayoutItem]:
+    """
+    Returns the layout for the sample mixin documentation.
+
+    Parameters
+    ----------
+    object_type: str
+        The object type to use in the layout
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The layout for the sample mixin documentation
+    """
+    return [
+        DocLayoutItem([object_type, EXPLORE, f"{object_type}.{field}"])
+        for field in ["describe", "preview", "sample"]
+    ]
+
+
 def _get_table_column_layout() -> List[DocLayoutItem]:
     """
     Returns the layout for the table column documentation
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the table column documentation
     """
     return [
         DocLayoutItem([TABLE_COLUMN]),
         DocLayoutItem([TABLE_COLUMN, ADD_METADATA, "TableColumn.as_entity"]),
         DocLayoutItem([TABLE_COLUMN, ADD_METADATA, "TableColumn.update_critical_data_info"]),
-        DocLayoutItem([TABLE_COLUMN, EXPLORE, "TableColumn.describe"]),
-        DocLayoutItem([TABLE_COLUMN, EXPLORE, "TableColumn.preview"]),
-        DocLayoutItem([TABLE_COLUMN, EXPLORE, "TableColumn.sample"]),
+        *_get_sample_mixin_layout(TABLE_COLUMN),
         DocLayoutItem([TABLE_COLUMN, INFO, "TableColumn.name"]),
         DocLayoutItem([TABLE_COLUMN, INFO, "TableColumn.cleaning_operations"]),
         DocLayoutItem([TABLE_COLUMN, LINEAGE, "TableColumn.preview_sql"]),
     ]
 
 
 def _get_entity_layout() -> List[DocLayoutItem]:
@@ -250,83 +271,83 @@
         DocLayoutItem([ENTITY, INFO, "Entity.updated_at"]),
         DocLayoutItem([ENTITY, LINEAGE, "Entity.id"]),
         DocLayoutItem([ENTITY, LINEAGE, "Entity.catalog_id"]),
         DocLayoutItem([ENTITY, MANAGE, "Entity.update_name"]),
     ]
 
 
+def _get_feature_or_target_items(section: str) -> List[DocLayoutItem]:
+    """
+    Get common documentation items for Feature and Target.
+
+    Parameters
+    ----------
+    section : str
+        The section to use for the documentation items
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The common documentation items for Feature and Target
+    """
+    assert section in {FEATURE, TARGET}
+    return [
+        DocLayoutItem([section]),
+        DocLayoutItem([section, EXPLORE, f"{section}.preview"]),
+        DocLayoutItem([section, INFO, f"{section}.created_at"]),
+        DocLayoutItem([section, INFO, f"{section}.dtype"]),
+        DocLayoutItem([section, INFO, f"{section}.info"]),
+        DocLayoutItem([section, INFO, f"{section}.name"]),
+        DocLayoutItem([section, INFO, f"{section}.updated_at"]),
+        DocLayoutItem([section, INFO, f"{section}.saved"]),
+        DocLayoutItem([section, INFO, f"{section}.version"]),
+        DocLayoutItem([section, INFO, f"{section}.is_datetime"]),
+        DocLayoutItem([section, INFO, f"{section}.is_numeric"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.entity_ids"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.feature_store"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.id"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.definition"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.catalog_id"]),
+        DocLayoutItem([section, LINEAGE, f"{section}.primary_entity"]),
+        DocLayoutItem([section, SAVE, f"{section}.save"]),
+        *_get_series_properties_layout(section),
+        *_get_datetime_accessor_properties_layout(section),
+        *_get_str_accessor_properties_layout(section),
+    ]
+
+
 def _get_feature_layout() -> List[DocLayoutItem]:
     """
     Returns the layout for the feature documentation
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the feature documentation
     """
     return [
-        DocLayoutItem([FEATURE]),
-        DocLayoutItem([FEATURE, SAVE, "Feature.save"]),
-        DocLayoutItem([FEATURE, EXPLORE, "Feature.preview"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.created_at"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.dtype"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.info"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.name"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.saved"]),
+        *_get_feature_or_target_items(FEATURE),
         DocLayoutItem([FEATURE, INFO, "Feature.readiness"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.updated_at"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.is_datetime"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.is_numeric"]),
         DocLayoutItem([FEATURE, INFO, "Feature.is_default"]),
-        DocLayoutItem([FEATURE, INFO, "Feature.version"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.entity_ids"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.primary_entity"]),
         DocLayoutItem([FEATURE, LINEAGE, "Feature.feature_list_ids"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.feature_store"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.id"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.definition"]),
         DocLayoutItem([FEATURE, LINEAGE, "Feature.sql"]),
-        DocLayoutItem([FEATURE, LINEAGE, "Feature.catalog_id"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.delete"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.get_feature_jobs_status"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.as_default_version"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.create_new_version"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.list_versions"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.update_default_version_mode"]),
         DocLayoutItem([FEATURE, MANAGE, "Feature.update_readiness"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.abs"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.astype"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.cosine_similarity"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.entropy"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.get_rank"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.get_relative_frequency"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.get_value"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.most_frequent"]),
         DocLayoutItem([FEATURE, TRANSFORM, "Feature.cd.unique_count"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.ceil"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.exp"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.fillna"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.floor"]),
-        *_get_datetime_accessor_properties_layout(FEATURE),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.isin"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.isnull"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.log"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.notnull"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.pow"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.sqrt"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.contains"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.len"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.lower"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.lstrip"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.pad"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.replace"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.rstrip"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.slice"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.strip"]),
-        DocLayoutItem([FEATURE, TRANSFORM, "Feature.str.upper"]),
     ]
 
 
 def _get_feature_group_layout() -> List[DocLayoutItem]:
     """
     The layout for the FeatureGroup class.
 
@@ -479,17 +500,15 @@
             doc_path_override="api.view.GroupByMixin.groupby.md",
         ),
         DocLayoutItem([VIEW, CREATE_TABLE, "View.create_observation_table"]),
         DocLayoutItem([VIEW, CREATE_TABLE, "View.create_batch_request_table"]),
         DocLayoutItem([VIEW, JOIN, "EventView.add_feature"]),
         DocLayoutItem([VIEW, JOIN, "ItemView.join_event_table_attributes"]),
         DocLayoutItem([VIEW, JOIN, "View.join"]),
-        DocLayoutItem([VIEW, EXPLORE, "View.describe"]),
-        DocLayoutItem([VIEW, EXPLORE, "View.preview"]),
-        DocLayoutItem([VIEW, EXPLORE, "View.sample"]),
+        *_get_sample_mixin_layout(VIEW),
         DocLayoutItem([VIEW, INFO, "ChangeView.default_feature_job_setting"]),
         DocLayoutItem([VIEW, INFO, "ChangeView.get_default_feature_job_setting"]),
         DocLayoutItem([VIEW, INFO, "DimensionView.dimension_id_column"]),
         DocLayoutItem([VIEW, INFO, "EventView.default_feature_job_setting"]),
         DocLayoutItem([VIEW, INFO, "EventView.event_id_column"]),
         DocLayoutItem([VIEW, INFO, "ItemView.default_feature_job_setting"]),
         DocLayoutItem([VIEW, INFO, "ItemView.event_table_id"]),
@@ -526,15 +545,15 @@
     series_type : str
         The type of the series, either "ViewColumn" or "Feature"
 
     Returns
     -------
     List[DocLayoutItem]
     """
-    assert series_type in {VIEW_COLUMN, FEATURE}
+    assert series_type in {VIEW_COLUMN, FEATURE, TARGET}
     return [
         DocLayoutItem([series_type, TRANSFORM, f"{series_type}.dt.{field}"])
         for field in [
             "tz_offset",
             "year",
             "quarter",
             "month",
@@ -546,61 +565,103 @@
             "second",
             "millisecond",
             "microsecond",
         ]
     ]
 
 
+def _get_str_accessor_properties_layout(series_type: str) -> List[DocLayoutItem]:
+    """
+    The layout for the StringAccessor related properties
+
+    Parameters
+    ----------
+    series_type : str
+        The type of the series, either "ViewColumn" or "Feature"
+
+    Returns
+    -------
+    List[DocLayoutItem]
+    """
+    assert series_type in {VIEW_COLUMN, FEATURE, TARGET}
+    return [
+        DocLayoutItem([series_type, TRANSFORM, f"{series_type}.str.{field}"])
+        for field in [
+            "contains",
+            "len",
+            "lower",
+            "lstrip",
+            "pad",
+            "replace",
+            "rstrip",
+            "slice",
+            "strip",
+            "upper",
+        ]
+    ]
+
+
+def _get_series_properties_layout(series_type: str) -> List[DocLayoutItem]:
+    """
+    The layout for the Series related properties
+
+    Parameters
+    ----------
+    series_type : str
+        The type of the series, either "ViewColumn" or "Feature"
+
+    Returns
+    -------
+    List[DocLayoutItem]
+    """
+    assert series_type in {VIEW_COLUMN, FEATURE, TARGET}
+    return [
+        DocLayoutItem([series_type, TRANSFORM, f"{series_type}.{field}"])
+        for field in [
+            "astype",
+            "abs",
+            "ceil",
+            "exp",
+            "fillna",
+            "floor",
+            "isin",
+            "isnull",
+            "log",
+            "notnull",
+            "pow",
+            "sqrt",
+        ]
+    ]
+
+
 def _get_view_column_layout() -> List[DocLayoutItem]:
     """
     The layout for the ViewColumn class.
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the ViewColumn class.
     """
     return [
         DocLayoutItem([VIEW_COLUMN]),
         DocLayoutItem([VIEW_COLUMN, CREATE_FEATURE, "ViewColumn.as_feature"]),
-        DocLayoutItem([VIEW_COLUMN, EXPLORE, "ViewColumn.describe"]),
-        DocLayoutItem([VIEW_COLUMN, EXPLORE, "ViewColumn.preview"]),
-        DocLayoutItem([VIEW_COLUMN, EXPLORE, "ViewColumn.sample"]),
+        *_get_sample_mixin_layout(VIEW_COLUMN),
         DocLayoutItem([VIEW_COLUMN, INFO, "ViewColumn.dtype"]),
         DocLayoutItem([VIEW_COLUMN, INFO, "ViewColumn.is_datetime"]),
         DocLayoutItem([VIEW_COLUMN, INFO, "ViewColumn.is_numeric"]),
         DocLayoutItem([VIEW_COLUMN, INFO, "ViewColumn.name"]),
         DocLayoutItem([VIEW_COLUMN, INFO, "ViewColumn.cleaning_operations"]),
         DocLayoutItem([VIEW_COLUMN, LAGS, "ChangeViewColumn.lag"]),
         DocLayoutItem([VIEW_COLUMN, LAGS, "EventViewColumn.lag"]),
         DocLayoutItem([VIEW_COLUMN, LINEAGE, "ViewColumn.feature_store"]),
         DocLayoutItem([VIEW_COLUMN, LINEAGE, "ViewColumn.preview_sql"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.astype"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.abs"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.ceil"]),
         *_get_datetime_accessor_properties_layout(VIEW_COLUMN),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.exp"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.fillna"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.floor"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.isin"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.isnull"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.log"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.notnull"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.pow"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.sqrt"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.contains"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.len"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.lower"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.lstrip"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.pad"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.replace"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.rstrip"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.slice"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.strip"]),
-        DocLayoutItem([VIEW_COLUMN, TRANSFORM, "ViewColumn.str.upper"]),
+        *_get_series_properties_layout(VIEW_COLUMN),
+        *_get_str_accessor_properties_layout(VIEW_COLUMN),
     ]
 
 
 def _get_catalog_layout() -> List[DocLayoutItem]:
     """
     The layout for the Catalog module.
 
@@ -618,14 +679,16 @@
         DocLayoutItem([CATALOG, CLASS_METHODS, "Catalog.create"]),
         DocLayoutItem([CATALOG, CLASS_METHODS, "Catalog.list"]),
         DocLayoutItem([CATALOG, CREATE, "Catalog.create_entity"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_feature_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_view"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_table"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_table_by_id"]),
+        DocLayoutItem([CATALOG, GET, "Catalog.get_target"]),
+        DocLayoutItem([CATALOG, GET, "Catalog.get_target_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_feature"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_entity"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_entity_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_feature_list"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_feature_list_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_data_source"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_relationship"]),
@@ -637,24 +700,27 @@
         DocLayoutItem([CATALOG, GET, "Catalog.get_historical_feature_table_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_batch_request_table"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_batch_request_table_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_batch_feature_table"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_batch_feature_table_by_id"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_deployment"]),
         DocLayoutItem([CATALOG, GET, "Catalog.get_deployment_by_id"]),
+        DocLayoutItem([CATALOG, GET, "Catalog.get_user_defined_function_by_id"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_deployments"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_relationships"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_feature_lists"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_entities"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_features"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_tables"]),
+        DocLayoutItem([CATALOG, LIST, "Catalog.list_targets"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_observation_tables"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_historical_feature_tables"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_batch_request_tables"]),
         DocLayoutItem([CATALOG, LIST, "Catalog.list_batch_feature_tables"]),
+        DocLayoutItem([CATALOG, LIST, "Catalog.list_user_defined_functions"]),
         DocLayoutItem([CATALOG, INFO, "Catalog.created_at"]),
         DocLayoutItem([CATALOG, INFO, "Catalog.info"]),
         DocLayoutItem([CATALOG, INFO, "Catalog.name"]),
         DocLayoutItem([CATALOG, INFO, "Catalog.updated_at"]),
         DocLayoutItem([CATALOG, LINEAGE, "Catalog.id"]),
         DocLayoutItem([CATALOG, MANAGE, "Catalog.update_name"]),
     ]
@@ -717,25 +783,30 @@
             [UTILITY_CLASSES, GROUPBY, "view.GroupBy.aggregate_asat"],
             doc_path_override="api.groupby.GroupBy.aggregate_asat.md",
         ),
         DocLayoutItem(
             [UTILITY_CLASSES, GROUPBY, "view.GroupBy.aggregate_over"],
             doc_path_override="api.groupby.GroupBy.aggregate_over.md",
         ),
+        DocLayoutItem(
+            [UTILITY_CLASSES, GROUPBY, "view.GroupBy.forward_aggregate"],
+            doc_path_override="api.groupby.GroupBy.forward_aggregate.md",
+        ),
         DocLayoutItem([UTILITY_CLASSES, FEATURE, "FeatureVersionInfo"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "DatabricksDetails"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "SnowflakeDetails"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "SparkDetails"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "AccessTokenCredential"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "AzureBlobStorageCredential"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "GCSStorageCredential"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "S3StorageCredential"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "UsernamePasswordCredential"]),
         DocLayoutItem([UTILITY_CLASSES, REQUEST_COLUMN, "RequestColumn.point_in_time"]),
+        DocLayoutItem([UTILITY_CLASSES, USER_DEFINED_FUNCTION, "FunctionParameter"]),
     ]
 
 
 def _get_source_table_layout() -> List[DocLayoutItem]:
     """
     The layout for the SourceTable module.
 
@@ -748,17 +819,15 @@
         DocLayoutItem([SOURCE_TABLE]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_dimension_table"]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_event_table"]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_item_table"]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_scd_table"]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_observation_table"]),
         DocLayoutItem([SOURCE_TABLE, CREATE_TABLE, "SourceTable.create_batch_request_table"]),
-        DocLayoutItem([SOURCE_TABLE, EXPLORE, "SourceTable.describe"]),
-        DocLayoutItem([SOURCE_TABLE, EXPLORE, "SourceTable.preview"]),
-        DocLayoutItem([SOURCE_TABLE, EXPLORE, "SourceTable.sample"]),
+        *_get_sample_mixin_layout(SOURCE_TABLE),
     ]
 
 
 def _get_feature_job_layout() -> List[DocLayoutItem]:
     """
     The layout for the FeatureJob module.
 
@@ -811,126 +880,168 @@
         DocLayoutItem([DEPLOYMENT, MANAGE, "Deployment.enable"]),
         DocLayoutItem([DEPLOYMENT, MANAGE, "Deployment.disable"]),
         DocLayoutItem([DEPLOYMENT, SERVE, "Deployment.compute_batch_feature_table"]),
         DocLayoutItem([DEPLOYMENT, SERVE, "Deployment.get_online_serving_code"]),
     ]
 
 
+def _get_materialized_table_layout(table_type: str) -> List[DocLayoutItem]:
+    """
+    The common layout for MaterializedTable modules.
+
+    Parameters
+    ----------
+    table_type: str
+        The type of table to generate the layout for.
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The layout for the MaterializedTable module.
+    """
+    return [
+        DocLayoutItem([table_type]),
+        *_get_sample_mixin_layout(table_type),
+        DocLayoutItem([table_type, INFO, f"{table_type}.name"]),
+        DocLayoutItem([table_type, INFO, f"{table_type}.created_at"]),
+        DocLayoutItem([table_type, INFO, f"{table_type}.updated_at"]),
+        DocLayoutItem([table_type, LINEAGE, f"{table_type}.id"]),
+        DocLayoutItem([table_type, MANAGE, f"{table_type}.download"]),
+        DocLayoutItem([table_type, MANAGE, f"{table_type}.delete"]),
+    ]
+
+
 def _get_batch_feature_table_layout() -> List[DocLayoutItem]:
     """
     The layout for the BatchFeatureTable module.
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the BatchFeatureTable module.
     """
     return [
-        DocLayoutItem([BATCH_FEATURE_TABLE]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, EXPLORE, "BatchFeatureTable.describe"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, EXPLORE, "BatchFeatureTable.preview"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, EXPLORE, "BatchFeatureTable.sample"]),
-        # DocLayoutItem([BATCH_FEATURE_TABLE, INFO, "BatchFeatureTable.feature_store_name"]),
-        # DocLayoutItem([BATCH_FEATURE_TABLE, INFO, "BatchFeatureTable.batch_request_table_name"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, INFO, "BatchFeatureTable.name"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, INFO, "BatchFeatureTable.created_at"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, INFO, "BatchFeatureTable.updated_at"]),
+        *_get_materialized_table_layout(BATCH_FEATURE_TABLE),
         DocLayoutItem([BATCH_FEATURE_TABLE, LINEAGE, "BatchFeatureTable.batch_request_table_id"]),
         DocLayoutItem([BATCH_FEATURE_TABLE, LINEAGE, "BatchFeatureTable.deployment_id"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, LINEAGE, "BatchFeatureTable.id"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, MANAGE, "BatchFeatureTable.download"]),
-        DocLayoutItem([BATCH_FEATURE_TABLE, MANAGE, "BatchFeatureTable.delete"]),
     ]
 
 
 def _get_observation_table_layout() -> List[DocLayoutItem]:
     """
     The layout for the ObservationTable module.
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the ObservationTable module.
     """
     return [
-        DocLayoutItem([OBSERVATION_TABLE]),
-        DocLayoutItem([OBSERVATION_TABLE, EXPLORE, "ObservationTable.describe"]),
-        DocLayoutItem([OBSERVATION_TABLE, EXPLORE, "ObservationTable.preview"]),
-        DocLayoutItem([OBSERVATION_TABLE, EXPLORE, "ObservationTable.sample"]),
-        DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.name"]),
+        *_get_materialized_table_layout(OBSERVATION_TABLE),
         DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.context_id"]),
-        # DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.type"]),
-        # DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.feature_store_name"]),
-        DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.created_at"]),
-        DocLayoutItem([OBSERVATION_TABLE, INFO, "ObservationTable.updated_at"]),
-        DocLayoutItem([OBSERVATION_TABLE, LINEAGE, "ObservationTable.id"]),
-        DocLayoutItem([OBSERVATION_TABLE, MANAGE, "ObservationTable.download"]),
-        DocLayoutItem([OBSERVATION_TABLE, MANAGE, "ObservationTable.delete"]),
         DocLayoutItem([OBSERVATION_TABLE, MANAGE, "ObservationTable.to_pandas"]),
     ]
 
 
 def _get_batch_request_table_layout() -> List[DocLayoutItem]:
     """
     The layout for the BatchRequestTable module.
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the BatchRequestTable module.
     """
     return [
-        DocLayoutItem([BATCH_REQUEST_TABLE]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, EXPLORE, "BatchRequestTable.describe"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, EXPLORE, "BatchRequestTable.preview"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, EXPLORE, "BatchRequestTable.sample"]),
+        *_get_materialized_table_layout(BATCH_REQUEST_TABLE),
         DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.context_id"]),
-        # DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.type"]),
-        # DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.feature_store_name"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.name"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.created_at"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, INFO, "BatchRequestTable.updated_at"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, LINEAGE, "BatchRequestTable.id"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, MANAGE, "BatchRequestTable.download"]),
-        DocLayoutItem([BATCH_REQUEST_TABLE, MANAGE, "BatchRequestTable.delete"]),
         DocLayoutItem([BATCH_REQUEST_TABLE, MANAGE, "BatchRequestTable.to_pandas"]),
     ]
 
 
 def _get_historical_feature_table_layout() -> List[DocLayoutItem]:
     """
     The layout for the HistoricalFeatureTable module.
 
     Returns
     -------
     List[DocLayoutItem]
         The layout for the HistoricalFeatureTable module.
     """
     return [
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, EXPLORE, "HistoricalFeatureTable.describe"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, EXPLORE, "HistoricalFeatureTable.preview"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, EXPLORE, "HistoricalFeatureTable.sample"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, INFO, "HistoricalFeatureTable.name"]),
-        # DocLayoutItem([HISTORICAL_FEATURE_TABLE, INFO, "HistoricalFeatureTable.feature_store_name"]),
-        # DocLayoutItem([HISTORICAL_FEATURE_TABLE, INFO, "HistoricalFeatureTable.observation_table_name"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, INFO, "HistoricalFeatureTable.created_at"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, INFO, "HistoricalFeatureTable.updated_at"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, MANAGE, "HistoricalFeatureTable.download"]),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, MANAGE, "HistoricalFeatureTable.delete"]),
+        *_get_materialized_table_layout(HISTORICAL_FEATURE_TABLE),
         DocLayoutItem(
             [HISTORICAL_FEATURE_TABLE, LINEAGE, "HistoricalFeatureTable.feature_list_id"]
         ),
-        DocLayoutItem([HISTORICAL_FEATURE_TABLE, LINEAGE, "HistoricalFeatureTable.id"]),
         DocLayoutItem(
             [HISTORICAL_FEATURE_TABLE, LINEAGE, "HistoricalFeatureTable.observation_table_id"]
         ),
     ]
 
 
+def _get_user_defined_function_layout() -> List[DocLayoutItem]:
+    """
+    The layout for the UserDefinedFunction module.
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The layout for the UserDefinedFunction module.
+    """
+    return [
+        DocLayoutItem([USER_DEFINED_FUNCTION]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, CLASS_METHODS, "UserDefinedFunction.create"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.created_at"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.info"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.name"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.sql_function_name"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.function_parameters"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.output_dtype"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.signature"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, INFO, "UserDefinedFunction.is_global"]),
+        DocLayoutItem(
+            [USER_DEFINED_FUNCTION, MANAGE, "UserDefinedFunction.update_sql_function_name"]
+        ),
+        DocLayoutItem(
+            [USER_DEFINED_FUNCTION, MANAGE, "UserDefinedFunction.update_function_parameters"]
+        ),
+        DocLayoutItem([USER_DEFINED_FUNCTION, MANAGE, "UserDefinedFunction.update_output_dtype"]),
+        DocLayoutItem([USER_DEFINED_FUNCTION, MANAGE, "UserDefinedFunction.delete"]),
+    ]
+
+
+def _get_target_layout() -> List[DocLayoutItem]:
+    """
+    The layout for the Target module.
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The layout for the Target module.
+    """
+    return [
+        *_get_feature_or_target_items(TARGET),
+    ]
+
+
+def _get_target_namespace_layout() -> List[DocLayoutItem]:
+    """
+    The layout for the TargetNamespace module.
+
+    Returns
+    -------
+    List[DocLayoutItem]
+        The layout for the TargetNamespace module.
+    """
+    return [
+        DocLayoutItem([TARGET_NAMESPACE]),
+        DocLayoutItem([TARGET_NAMESPACE, CLASS_METHODS, "TargetNamespace.create"]),
+    ]
+
+
 def get_overall_layout() -> List[DocLayoutItem]:
     """
     The overall layout for the documentation.
 
     Returns
     -------
     List[DocLayoutItem]
@@ -954,8 +1065,11 @@
         *_get_source_table_layout(),
         *_get_feature_job_layout(),
         *_get_deployment_layout(),
         *_get_batch_feature_table_layout(),
         *_get_batch_request_table_layout(),
         *_get_observation_table_layout(),
         *_get_historical_feature_table_layout(),
+        *_get_user_defined_function_layout(),
+        *_get_target_layout(),
+        *_get_target_namespace_layout(),
     ]
```

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/extract_csv.py` & `featurebyte-0.4.0/featurebyte/common/documentation/extract_csv.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/formatters.py` & `featurebyte-0.4.0/featurebyte/common/documentation/formatters.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/gen_ref_pages_docs_builder.py` & `featurebyte-0.4.0/featurebyte/common/documentation/gen_ref_pages_docs_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -359,14 +359,20 @@
         ),
         "FeatureStringAccessor": AccessorMetadata(
             classes_using_accessor=[
                 "featurebyte.Feature",
             ],
             property_name="str",
         ),
+        "TargetStringAccessor": AccessorMetadata(
+            classes_using_accessor=[
+                "featurebyte.Target",
+            ],
+            property_name="str",
+        ),
         "CountDictAccessor": AccessorMetadata(
             classes_using_accessor=[
                 "featurebyte.Feature",
             ],
             property_name="cd",
         ),
         # Must include `datetime` prefix to avoid conflict with the `FeatureDatetimeAccessor`.
@@ -378,14 +384,20 @@
         ),
         "FeatureDatetimeAccessor": AccessorMetadata(
             classes_using_accessor=[
                 "featurebyte.Feature",
             ],
             property_name="dt",
         ),
+        "TargetDatetimeAccessor": AccessorMetadata(
+            classes_using_accessor=[
+                "featurebyte.Target",
+            ],
+            property_name="dt",
+        ),
     }
 
 
 def build_markdown_format_str(obj_path: str, obj_type: str, api_to_use: str) -> str:
     """
     Build the markdown format string for the given object path.
```

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/markdown_extension/extension.py` & `featurebyte-0.4.0/featurebyte/common/documentation/markdown_extension/extension.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/pydantic_field_docs.py` & `featurebyte-0.4.0/featurebyte/common/documentation/pydantic_field_docs.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 Docstrings for pydantic field overrides for a class.
 """
 from typing import Dict
 
 CATALOG_ID = "catalog_id"
 CREATED_AT = "created_at"
 DTYPE = "dtype"
+ENTITY_IDS = "entity_ids"
 FEATURE_STORE = "feature_store"
 ID = "id"
 NAME = "name"
 UPDATED_AT = "updated_at"
 
 
 def _get_created_at_docstring_override(class_name: str) -> str:
@@ -28,14 +29,18 @@
     return f"Returns the timestamp indicating when the {class_name} object was last updated."
 
 
 def _get_catalog_id_docstring_override(class_name: str) -> str:
     return f"Returns the unique identifier (ID) of the Catalog that is associated with the {class_name} object."
 
 
+def _get_dtype_docstring_override(class_name: str) -> str:
+    return f"Returns the data type of the {class_name}."
+
+
 def _get_doc_overrides(class_name: str) -> Dict[str, str]:
     """
     Get the docstring overrides for a class.
 
     Parameters
     ----------
     class_name : str
@@ -50,39 +55,57 @@
         CREATED_AT: _get_created_at_docstring_override(class_name),
         ID: _get_id_docstring_override(class_name),
         NAME: _get_name_docstring_override(class_name),
         UPDATED_AT: _get_updated_at_docstring_override(class_name),
     }
 
 
+def _get_target_or_feature_overrides(class_name: str) -> Dict[str, str]:
+    """
+    Get the docstring overrides for a Target or Feature class.
+
+    Parameters
+    ----------
+    class_name : str
+        The name of the class.
+
+    Returns
+    -------
+    Dict[str, str]
+        The docstring overrides for the class.
+    """
+    return {
+        **_get_doc_overrides(class_name),
+        CATALOG_ID: _get_catalog_id_docstring_override(class_name),
+        DTYPE: _get_dtype_docstring_override(class_name),
+        ENTITY_IDS: f"Returns a list of entity IDs that are linked to the {class_name}.",
+    }
+
+
 pydantic_field_doc_overrides = {
     "BatchFeatureTable": _get_doc_overrides("BatchFeatureTable"),
     "BatchRequestTable": _get_doc_overrides("BatchRequestTable"),
     "Catalog": _get_doc_overrides("Catalog"),
     "DisguisedValueImputation": {
         "imputed_value": "The value that will be used to replace any element that matches one of the values in the "
         "disguised_values list.",
     },
     "Entity": _get_doc_overrides("Entity"),
+    "Target": _get_target_or_feature_overrides("Target"),
     "Feature": {
-        CATALOG_ID: _get_catalog_id_docstring_override("Feature"),
-        CREATED_AT: _get_created_at_docstring_override("Feature"),
-        DTYPE: "Returns the data type of the Feature.",
-        "entity_ids": "Returns a list of entity IDs that are linked to the feature.",
+        **_get_target_or_feature_overrides("Feature"),
         "feature_list_ids": "Returns a list of IDs of feature lists that include the specified feature version.",
         "feature_namespace_id": "Returns the unique identifier (ID) of the feature namespace of the specified feature. "
         "All feature versions of a feature have the same feature namespace ID.",
-        ID: _get_id_docstring_override("Feature"),
         NAME: "The 'name' property of a Feature object serves to identify the feature namespace and can be used to "
         "retrieve or set its name. To save a Feature object derived from other Feature objects to a catalog, a "
         "unique name must be assigned to it using the property before saving it. The namespace of Lookup "
         "Features or Aggregate Features are defined during their declaration. The property can be used to "
         "modify it before saving it. Once the feature is saved, its name cannot be changed. Therefore, it is "
         "essential to give the Feature object a descriptive and meaningful name before saving it.",
-        UPDATED_AT: _get_updated_at_docstring_override("Feature"),
     },
     "FeatureList": {
         **_get_doc_overrides("FeatureList"),
         CATALOG_ID: _get_catalog_id_docstring_override("FeatureList"),
     },
     "FeatureStore": _get_doc_overrides("FeatureStore"),
     "HistoricalFeatureTable": _get_doc_overrides("HistoricalFeatureTable"),
@@ -116,12 +139,13 @@
         "imputed_value": "The value that will be used to replace any value that does not match the expected values "
         "in the expected_values list.",
     },
     "ValueBeyondEndpointImputation": {
         "imputed_value": "The value that will be used to replace any value that falls outside a specified range.",
     },
     "ViewColumn": {
-        DTYPE: "Returns the data type of the view column.",
+        DTYPE: _get_dtype_docstring_override("ViewColumn"),
         FEATURE_STORE: "Provides information about the feature store that the view column is connected to.",
         NAME: _get_name_docstring_override("ViewColumn"),
     },
+    "UserDefinedFunction": _get_doc_overrides("UserDefinedFunction"),
 }
```

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/resource_extractor.py` & `featurebyte-0.4.0/featurebyte/common/documentation/resource_extractor.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/documentation/resource_util.py` & `featurebyte-0.4.0/featurebyte/common/documentation/resource_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/env_util.py` & `featurebyte-0.4.0/featurebyte/common/env_util.py`

 * *Files 6% similar despite different names*

```diff
@@ -44,10 +44,10 @@
     ----------
     html_content: str
         HTML content to display
     """
 
     if is_notebook():
         # pylint: disable=import-outside-toplevel
-        from IPython.display import HTML, display  # pylint: disable=import-error
+        from IPython.display import HTML, display  # type: ignore
 
         display(HTML(html_content), metadata={"isolated": True})
```

### Comparing `featurebyte-0.3.1/featurebyte/common/formatting_util.py` & `featurebyte-0.4.0/featurebyte/common/formatting_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/join_utils.py` & `featurebyte-0.4.0/featurebyte/common/join_utils.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/model_util.py` & `featurebyte-0.4.0/featurebyte/common/model_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/path_util.py` & `featurebyte-0.4.0/featurebyte/common/path_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/progress.py` & `featurebyte-0.4.0/featurebyte/common/progress.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/typing.py` & `featurebyte-0.4.0/featurebyte/common/typing.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 Common utilities related to typing
 """
 from __future__ import annotations
 
-from typing import Any, Literal, Optional, Sequence, Type, Union, cast
+from typing import Any, Callable, Literal, Optional, Sequence, Type, Union, cast
 
 import pandas as pd
 from pandas.api.types import is_scalar
 from pydantic import StrictFloat, StrictInt, StrictStr
 
 DatetimeSupportedPropertyType = Literal[
     "year",
@@ -33,14 +33,16 @@
 OptionalScalar = Optional[Scalar]
 ScalarSequence = Sequence[Scalar]
 Numeric = Union[StrictInt, StrictFloat]
 Timestamp = Union[pd.Timestamp]
 
 AllSupportedValueTypes = Union[Scalar, ScalarSequence, Timestamp]
 
+Func = Callable[..., Any]
+
 
 def is_scalar_nan(value: Any) -> bool:
     """
     Returns whether the provided value is a scalar nan value (float('nan'), np.nan, None)
 
     Parameters
     ----------
```

### Comparing `featurebyte-0.3.1/featurebyte/common/utils.py` & `featurebyte-0.4.0/featurebyte/common/utils.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/common/validator.py` & `featurebyte-0.4.0/featurebyte/common/validator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """This module contains validators used for model input validation"""
 from __future__ import annotations
 
 from typing import Any, List, Optional, Set, Tuple
 
-from featurebyte.common.model_util import convert_version_string_to_dict
+from featurebyte.common.model_util import convert_version_string_to_dict, parse_duration_string
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.model.column_info import ColumnInfo
 
 
 def construct_data_model_root_validator(
     columns_info_key: str,
     expected_column_field_name_type_pairs: List[Tuple[str, Optional[Set[DBVarType]]]],
@@ -141,7 +141,29 @@
     """
 
     _ = cls
     # DEV-556: converted older record string value to dictionary format
     if isinstance(value, str):
         return convert_version_string_to_dict(value)
     return value
+
+
+def duration_string_validator(cls: Any, value: Any) -> Any:
+    """
+    Test whether a duration string is valid.
+
+    Parameters
+    ----------
+    cls: Any
+        Class handle
+    value: Any
+        Input duration string value
+
+    Returns
+    -------
+    Any
+    """
+    _ = cls
+    if isinstance(value, str):
+        # Try to parse using pandas#Timedelta. If it fails, a ValueError will be thrown.
+        parse_duration_string(value)
+    return value
```

### Comparing `featurebyte-0.3.1/featurebyte/config.py` & `featurebyte-0.4.0/featurebyte/config.py`

 * *Files 3% similar despite different names*

```diff
@@ -166,17 +166,17 @@
         Raises
         ------
         InvalidSettingsError
             Invalid service endpoint
         """
         try:
             headers = kwargs.get("headers", {})
-            headers["active-catalog-id"] = headers.get(
-                "active-catalog-id", str(get_active_catalog_id())
-            )
+            active_catalog_id = headers.get("active-catalog-id", get_active_catalog_id())
+            if active_catalog_id:
+                headers["active-catalog-id"] = str(active_catalog_id)
             kwargs["headers"] = headers
             kwargs["allow_redirects"] = False
             return super().request(
                 method,
                 self.base_url + str(url),
                 *args,
                 **kwargs,
@@ -289,24 +289,46 @@
         if not self._config_file_path.exists():
             self._config_file_path.parent.mkdir(parents=True, exist_ok=True)
             self._config_file_path.write_text(
                 "# featurebyte configuration\n\n"
                 "profile:\n"
                 "  - name: local\n"
                 "    api_url: http://127.0.0.1:8088\n\n"
+                "default_profile: local\n\n"
             )
 
         self.storage: LocalStorageSettings = LocalStorageSettings()
-        self.profile: Optional[Profile] = None
+        self._profile: Optional[Profile] = None
         self.profiles: Optional[List[Profile]] = None
         self.settings: Dict[str, Any] = {}
         self.logging: LoggingSettings = LoggingSettings()
         self._parse_config(self._config_file_path)
 
     @property
+    def profile(self) -> Profile:
+        """
+        Get active profile
+
+        Returns
+        -------
+        Profile
+            Active profile
+
+        Raises
+        ------
+        InvalidSettingsError
+            No valid profile specified
+        """
+        if not self._profile:
+            raise InvalidSettingsError(
+                'No valid profile specified. Update config file or specify valid profile name with "use_profile".'
+            )
+        return self._profile
+
+    @property
     def config_file_path(self) -> Path:
         """
         Config file path
 
         Returns
         -------
         Path
@@ -343,22 +365,21 @@
 
         profile_settings = self.settings.pop("profile", None)
         if profile_settings:
             # parse profile settings
             self.profiles = ProfileList(profiles=profile_settings).profiles
 
         if self.profiles:
-            # use first profile as fallback
-            self.profile = self.profiles[0]
+            profile_map = {profile.name: profile for profile in self.profiles}
+            default_profile = self.settings.pop("default_profile", None)
             selected_profile_name = os.environ.get("FEATUREBYTE_PROFILE")
             if selected_profile_name:
-                for profile in self.profiles:
-                    if profile.name == selected_profile_name:
-                        self.profile = profile
-                        break
+                self._profile = profile_map.get(selected_profile_name)
+            else:
+                self._profile = profile_map.get(default_profile)
 
     @classmethod
     def check_sdk_versions(cls) -> Dict[str, str]:
         """
         Check SDK versions of client and server for the active profile
 
         Returns
@@ -399,19 +420,21 @@
             Invalid service endpoint
 
         Examples
         --------
         Content of configuration file at `~/.featurebyte/config.yaml`
         ```
         profile:
-          - name: featurebyte
+          - name: local
             api_url: https://app.featurebyte.com/api/v1
             api_token: API_TOKEN_VALUE
+
+        default_profile: local
         ```
-        Use service profile `featurebyte`
+        Use service profile `local`
 
         >>> fb.Configurations().use_profile("local")
         """
         profile_names = [profile.name for profile in Configurations().profiles or []]
         if profile_name not in profile_names:
             raise InvalidSettingsError(f"Profile not found: {profile_name}")
 
@@ -432,32 +455,21 @@
         """
         Get a client for the configured profile.
 
         Returns
         -------
         APIClient
             API client
-
-        Raises
-        ------
-        InvalidSettingsError
-            Invalid settings
         """
         # pylint: disable=import-outside-toplevel,cyclic-import
         from featurebyte.logging import reconfigure_loggers
 
         # configure logger
         reconfigure_loggers(self)
-
-        if self.profile:
-            client = APIClient(api_url=self.profile.api_url, api_token=self.profile.api_token)
-        else:
-            raise InvalidSettingsError("No profile setting specified")
-
-        return client
+        return APIClient(api_url=self.profile.api_url, api_token=self.profile.api_token)
 
     @contextmanager
     def get_websocket_client(self, task_id: str) -> Iterator[WebsocketClient]:
         """
         Get websocket client for the configured profile.
 
         Parameters
@@ -465,23 +477,15 @@
         task_id: str
             Task ID
 
         Yields
         -------
         WebsocketClient
             Websocket client
-
-        Raises
-        ------
-        InvalidSettingsError
-            Invalid settings
         """
-        if self.profile:
-            url = self.profile.api_url.replace("http://", "ws://").replace("https://", "wss://")
-            url = f"{url}/ws/{task_id}"
-            websocket_client = WebsocketClient(url=url, access_token=self.profile.api_token)
-            try:
-                yield websocket_client
-            finally:
-                websocket_client.close()
-        else:
-            raise InvalidSettingsError("No profile setting specified")
+        url = self.profile.api_url.replace("http://", "ws://").replace("https://", "wss://")
+        url = f"{url}/ws/{task_id}"
+        websocket_client = WebsocketClient(url=url, access_token=self.profile.api_token)
+        try:
+            yield websocket_client
+        finally:
+            websocket_client.close()
```

### Comparing `featurebyte-0.3.1/featurebyte/conftest.py` & `featurebyte-0.4.0/featurebyte/conftest.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/accessor/count_dict.py` & `featurebyte-0.4.0/featurebyte/core/accessor/count_dict.py`

 * *Files 1% similar despite different names*

```diff
@@ -318,15 +318,17 @@
         additional_node_params = {}
         if isinstance(key, type(self._feature_obj)):
             assert_is_lookup_feature(key.node_types_lineage)
         else:
             # We only need to assign value if we have been passed in a single scalar value.
             additional_node_params["value"] = key
         # construct operation structure of the get value node output
-        op_struct = self._feature_obj.graph.extract_operation_structure(node=self._feature_obj.node)
+        op_struct = self._feature_obj.graph.extract_operation_structure(
+            node=self._feature_obj.node, keep_all_source_columns=True
+        )
         get_value_node = GetValueFromDictionaryNode(name="temp", parameters=additional_node_params)
 
         series_operator = DefaultSeriesBinaryOperator(self._feature_obj, key)
         return series_operator.operate(
             node_type=NodeType.GET_VALUE,
             output_var_type=get_value_node.derive_var_type([op_struct]),
             additional_node_params=additional_node_params,
@@ -390,16 +392,17 @@
             node_type=NodeType.GET_RANK,
             output_var_type=DBVarType.FLOAT,
             additional_node_params=additional_node_params,
         )
 
     def get_relative_frequency(self, key: Union[Scalar, Feature]) -> Feature:
         """
-        Computes the relative frequency of a specific key in the Cross Aggregate feature. The key may either be a
-        lookup feature or a scalar value.
+        Computes the relative frequency of a specific key in the Cross Aggregate feature. The key
+        may either be a lookup feature or a scalar value. If the key does not exist, the relative
+        frequency will be 0.
 
         Parameters
         ----------
         key: Union[Scalar, Feature]
             Key to lookup the value for.
 
         Returns
```

### Comparing `featurebyte-0.3.1/featurebyte/core/accessor/datetime.py` & `featurebyte-0.4.0/featurebyte/core/accessor/datetime.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/accessor/feature_datetime.py` & `featurebyte-0.4.0/featurebyte/core/accessor/feature_datetime.py`

 * *Files 2% similar despite different names*

```diff
@@ -144,16 +144,16 @@
         return super().week  # type: ignore[return-value]
 
     @property
     def day(self) -> Feature:
         """
         Returns the day component of each element.
 
-        This is also available for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is also available for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the day of week component values
 
         Examples
@@ -187,16 +187,16 @@
         return super().day_of_week  # type: ignore[return-value]
 
     @property
     def hour(self) -> Feature:
         """
         Returns the hour component of each element.
 
-        This is also available for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is also available for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the hour component values
 
         Examples
@@ -210,16 +210,16 @@
         return super().hour  # type: ignore[return-value]
 
     @property
     def minute(self) -> Feature:
         """
         Returns the minute component of each element.
 
-        This is also available for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is also available for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the minute component values
 
         Examples
@@ -233,16 +233,16 @@
         return super().minute  # type: ignore[return-value]
 
     @property
     def second(self) -> Feature:
         """
         Returns the second component of each element.
 
-        This is also available for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is also available for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the second component values
 
         Examples
@@ -257,16 +257,16 @@
         return super().second  # type: ignore[return-value]
 
     @property
     def millisecond(self) -> Feature:
         """
         Returns the millisecond component of each element.
 
-        This is available only for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is available only for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the millisecond component values
 
         Examples
@@ -281,16 +281,16 @@
         return super().millisecond  # type: ignore[return-value]
 
     @property
     def microsecond(self) -> Feature:
         """
         Returns the microsecond component of each element.
 
-        This is available only for Series containing timedelta values, which is a result of taking
-        the difference between two timestamp Series.
+        This is available only for Features containing timedelta values, which is a result of taking
+        the difference between two timestamp Features.
 
         Returns
         -------
         Feature
             Feature containing the microsecond component values
 
         Examples
```

### Comparing `featurebyte-0.3.1/featurebyte/core/accessor/feature_string.py` & `featurebyte-0.4.0/featurebyte/core/accessor/feature_string.py`

 * *Files 8% similar despite different names*

```diff
@@ -109,15 +109,15 @@
         Returns
         -------
         Feature
             A new Feature object.
 
         Examples
         --------
-        Remove leading and trailing "M" characters from the Title column:
+        Remove leading and trailing "M" characters from the ProductGroupLookup feature:
 
         >>> feature = catalog.get_feature("ProductGroupLookup")
         >>> feature_group = fb.FeatureGroup([feature])
         >>> feature_group["ProductGroupStrip"] = feature.str.strip("M")
         """
         return super().strip(to_strip=to_strip)  # type: ignore[return-value]
 
@@ -133,15 +133,15 @@
         Returns
         -------
         Feature
             A new Feature object.
 
         Examples
         --------
-        Remove leading "M" characters from the Title column:
+        Remove leading "M" characters from the ProductGroupLookup feature:
 
         >>> feature = catalog.get_feature("ProductGroupLookup")
         >>> feature_group = fb.FeatureGroup([feature])
         >>> feature_group["ProductGroupStrip"] = feature.str.lstrip("M")
         """
         return super().lstrip(to_strip=to_strip)  # type: ignore[return-value]
 
@@ -157,15 +157,15 @@
         Returns
         -------
         Feature
             A new Feature object.
 
         Examples
         --------
-        Remove trailing "M" characters from the Title column:
+        Remove trailing "M" characters from the ProductGroupLookup feature:
 
         >>> feature = catalog.get_feature("ProductGroupLookup")
         >>> feature_group = fb.FeatureGroup([feature])
         >>> feature_group["ProductGroupStrip"] = feature.str.rstrip(".")
         """
         return super().rstrip(to_strip=to_strip)  # type: ignore[return-value]
 
@@ -211,15 +211,15 @@
         Returns
         -------
         Feature
             A new Feature object.
 
         Examples
         --------
-        Pad the Title column to 10 characters:
+        Pad the ProductGroupLookup feature to 10 characters:
 
         >>> feature = catalog.get_feature("ProductGroupLookup")
         >>> feature_group = fb.FeatureGroup([feature])
         >>> feature_group["ProductGroupLookupPadded"] = feature.str.pad(10, fillchar="0")
         """
         return super().pad(width=width, side=side, fillchar=fillchar)  # type: ignore[return-value]
```

### Comparing `featurebyte-0.3.1/featurebyte/core/accessor/string.py` & `featurebyte-0.4.0/featurebyte/core/accessor/string.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/frame.py` & `featurebyte-0.4.0/featurebyte/core/frame.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/generic.py` & `featurebyte-0.4.0/featurebyte/core/generic.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,15 +90,15 @@
         """
         Returns the operation structure of the current node
 
         Returns
         -------
         OperationStructure
         """
-        return self.graph.extract_operation_structure(self.node)
+        return self.graph.extract_operation_structure(self.node, keep_all_source_columns=True)
 
     @property
     def row_index_lineage(self) -> Tuple[str, ...]:
         """
         A list of node names that changes number of rows leading to the current node
 
         Returns
@@ -119,15 +119,15 @@
         out = []
         pruned_graph, pruned_node = self.extract_pruned_graph_and_node()
         flattened_graph, node_name_map = GraphFlatteningTransformer(graph=pruned_graph).transform()
         flattened_node = flattened_graph.get_node_by_name(node_name_map[pruned_node.name])
 
         # prune the flattened graph as view graph node is not pruned before flattened
         graph = QueryGraph(**flattened_graph.dict())
-        pruned_flattened_graph, pruned_node_name_map = graph.prune(flattened_node, aggressive=True)
+        pruned_flattened_graph, pruned_node_name_map = graph.prune(flattened_node)
         pruned_flattened_node = pruned_flattened_graph.get_node_by_name(
             pruned_node_name_map[flattened_node.name]
         )
 
         # construct the node type lineage
         for node in dfs_traversal(pruned_flattened_graph, pruned_flattened_node):
             out.append(node.type)
@@ -164,17 +164,15 @@
 
         Returns
         -------
         tuple[QueryGraphModel, Node]
             QueryGraph & mapped Node object (within the pruned graph)
         """
         _ = kwargs
-        pruned_graph, node_name_map = GlobalQueryGraph().prune(
-            target_node=self.node, aggressive=True
-        )
+        pruned_graph, node_name_map = GlobalQueryGraph().prune(target_node=self.node)
         mapped_node = pruned_graph.get_node_by_name(node_name_map[self.node.name])
         return pruned_graph, mapped_node
 
     def _generate_code(
         self,
         to_format: bool = False,
         to_use_saved_data: bool = False,
@@ -221,15 +219,15 @@
             exclude=exclude,
             update=update_dict,
             deep=deep,
         )
 
     def dict(self, *args: Any, **kwargs: Any) -> dict[str, Any]:
         if isinstance(self.graph, GlobalQueryGraph):
-            pruned_graph, node_name_map = self.graph.prune(target_node=self.node, aggressive=False)
+            pruned_graph, node_name_map = self.graph.quick_prune(target_node_names=[self.node_name])
             mapped_node = pruned_graph.get_node_by_name(node_name_map[self.node.name])
             new_object = self.copy()
             new_object.node_name = mapped_node.name
 
             # Use the __dict__ assignment method to skip pydantic validation check. Otherwise, it will trigger
             # `_convert_query_graph_to_global_query_graph` validation check and convert the pruned graph into
             # global one.
```

### Comparing `featurebyte-0.3.1/featurebyte/core/mixin.py` & `featurebyte-0.4.0/featurebyte/core/mixin.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/series.py` & `featurebyte-0.4.0/featurebyte/core/series.py`

 * *Files 2% similar despite different names*

```diff
@@ -1059,18 +1059,44 @@
 
 class Series(FrozenSeries):
     """
     Series is a mutable version of FrozenSeries. It is used to represent a single column of a Frame.
     This class supports in-place column modification, and is the primary interface for column.
     """
 
+    def validate_series_operation(self, other_series: Series) -> bool:
+        """
+        Validate the other series for series operation. This method is when performing the __setitem__ operation
+        between two series.
+
+        Parameters
+        ----------
+        other_series: Series
+            The other series to validate
+
+        Returns
+        -------
+        bool
+        """
+        _ = other_series
+        return True
+
     @typechecked
     def __setitem__(
         self, key: FrozenSeries, value: Union[int, float, str, bool, None, FrozenSeries]
     ) -> None:
+        if isinstance(value, Series):
+            if not self.validate_series_operation(value) or not value.validate_series_operation(
+                self
+            ):
+                raise TypeError(
+                    f"Operation between {self.__class__.__name__} and {value.__class__.__name__} "
+                    f"is not supported"
+                )
+
         if self.row_index_lineage != key.row_index_lineage:
             raise ValueError(f"Row indices between '{self}' and '{key}' are not aligned!")
         if key.dtype != DBVarType.BOOL:
             raise TypeError("Only boolean Series filtering is supported!")
 
         self._assert_assignment_valid(value)
         node_params = {}
```

### Comparing `featurebyte-0.3.1/featurebyte/core/timedelta.py` & `featurebyte-0.4.0/featurebyte/core/timedelta.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/core/util.py` & `featurebyte-0.4.0/featurebyte/core/util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/__main__.py` & `featurebyte-0.4.0/featurebyte/datasets/__main__.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/app.py` & `featurebyte-0.4.0/featurebyte/datasets/app.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/creditcard.sql` & `featurebyte-0.4.0/featurebyte/datasets/creditcard.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/doctest_grocery.sql` & `featurebyte-0.4.0/featurebyte/datasets/doctest_grocery.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/grocery.sql` & `featurebyte-0.4.0/featurebyte/datasets/grocery.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/datasets/healthcare.sql` & `featurebyte-0.4.0/featurebyte/datasets/healthcare.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/docker/featurebyte.yml` & `featurebyte-0.4.0/featurebyte/docker/featurebyte.yml`

 * *Files 6% similar despite different names*

```diff
@@ -32,15 +32,15 @@
       driver: local
   featurebyte-server:
     networks:
       - featurebyte
     hostname: featurebyte-server
     restart: unless-stopped
     container_name: featurebyte-server
-    image: featurebyte/featurebyte-server:0.3.1
+    image: featurebyte/featurebyte-server:0.4.0
     depends_on:
       mongo-rs:
         condition: service_healthy
     ports:
       - "0.0.0.0:8088:8088"
     command: ["bash", "/docker-entrypoint.sh", "server"]
     environment:
@@ -49,14 +49,16 @@
       - "REDIS_URI=redis://redis:6379"
       - "MONGODB_URI=mongodb://mongo-rs:27021,mongo-rs:27022/?replicaSet=rs0"
       - "API_HOST=0.0.0.0"
       - "API_PORT=8088"
       - "LOG_LEVEL=${LOG_LEVEL}"
       - "LOCAL_UID=${LOCAL_UID}"
       - "LOCAL_GID=${LOCAL_GID}"
+      - "KRB5_REALM=${KRB5_REALM}"
+      - "KRB5_KDC=${KRB5_KDC}"
     healthcheck:
       test: ["CMD", "curl", "-f", "http://127.0.0.1:8088/status"]
       interval: 5s
       start_period: 30s
     volumes:
       - files-data:/app/.featurebyte/data/files
       - staging-data:/data/staging
@@ -65,28 +67,30 @@
       driver: local
   featurebyte-worker:
     networks:
       - featurebyte
     hostname: featurebyte-worker
     restart: unless-stopped
     container_name: featurebyte-worker
-    image: featurebyte/featurebyte-server:0.3.1
+    image: featurebyte/featurebyte-server:0.4.0
     depends_on:
       mongo-rs:
         condition: service_healthy
       redis:
         condition: service_healthy
     environment:
       - "FEATUREBYTE_HOME=/app/.featurebyte"
       - "MPLCONFIGDIR=/app/matplotlib"
       - "REDIS_URI=redis://redis:6379"
       - "MONGODB_URI=mongodb://mongo-rs:27021,mongo-rs:27022/?replicaSet=rs0"
       - "LOG_LEVEL=${LOG_LEVEL}"
       - "LOCAL_UID=${LOCAL_UID}"
       - "LOCAL_GID=${LOCAL_GID}"
+      - "KRB5_REALM=${KRB5_REALM}"
+      - "KRB5_KDC=${KRB5_KDC}"
     command: ["bash", "/docker-entrypoint.sh", "worker"]
     volumes:
       - files-data:/app/.featurebyte/data/files
       - staging-data:/data/staging
       - file-store:/tmp
     logging:
       driver: local
```

### Comparing `featurebyte-0.3.1/featurebyte/docker/manager.py` & `featurebyte-0.4.0/featurebyte/docker/manager.py`

 * *Files 14% similar despite different names*

```diff
@@ -37,26 +37,41 @@
     FEATUREBYTE = "featurebyte"
     SPARK = "spark"
 
 console = Console()
 
 
 @contextmanager
-def get_docker_client() -> Generator[DockerClient, None, None]:
+def get_docker_client(krb5_realm: Optional[str] = None,
+                      krb5_kdc: Optional[str] = None) -> Generator[DockerClient, None, None]:
     """
     Get docker client
 
+    Parameters
+    ----------
+    krb5_realm : Optional[str]
+        Kerberos realm, by default None
+    krb5_kdc : Optional[str]
+        Kerberos kdc of the krb5_realm, by default None
+
     Yields
     -------
     Generator[DockerClient, None, None]
         Docker client
     """
+    # Set as empty strings if None
+    krb5_realm = krb5_realm if krb5_realm is not None else ""
+    krb5_kdc = krb5_kdc if krb5_kdc else ""
+
     with tempfile.TemporaryDirectory() as temp_dir:
         compose_env_file = os.path.join(temp_dir, ".env")
-        env_file_lines = []
+        env_file_lines = [
+            f"KRB5_REALM={krb5_realm}\n",
+            f"KRB5_KDC={krb5_kdc}\n",
+        ]
         if sys.platform != "win32":
             import pwd  # pylint: disable=import-outside-toplevel
             uid = os.getuid()
             user = pwd.getpwuid(uid)
             env_file_lines.extend(
                 [
                     f'LOCAL_UID="{uid}"\n',
@@ -132,24 +147,30 @@
                 continue
             style = "green" if source == "stdout" else "red"
             console.print(Text(line, style=style))
 
 
 def start_app(
     app_name: ApplicationName,
+    krb5_realm: Optional[str] = None,
+    krb5_kdc: Optional[str] = None,
     services: List[str] = None,
     verbose: bool = True,
 ) -> None:
     """
     Start application
 
     Parameters
     ----------
     app_name : ApplicationName
         Application name
+    krb5_realm : Optional[str]
+        Kerberos realm, by default None
+    krb5_kdc : Optional[str]
+        Kerberos kdc of the krb5_realm, by default None
     services : List[str]
         List of services to start
     verbose : bool
         Print verbose output
 
     Raises
     ------
@@ -159,15 +180,15 @@
     try:
         services = services or get_service_names(app_name)
 
         # Load config to ensure it exists before starting containers
         Configurations()
         __setup_network()
 
-        with get_docker_client() as docker:
+        with get_docker_client(krb5_realm=krb5_realm, krb5_kdc=krb5_kdc) as docker:
             docker.compose.up(services=services, detach=True)
 
             # Wait for all services to be healthy
             def _wait_for_healthy(spinner_status: Optional[Status] = None) -> None:
                 while True:
                     unhealthy_containers = []
                     for container in docker.compose.ps():
@@ -201,35 +222,42 @@
                 _wait_for_healthy()
     except DockerException as exc:
         if "Is the docker daemon running" in str(exc):
             raise DockerError("Docker is not running, please start docker and try again.") from exc
         raise DockerError(f"Failed to start application: {exc.stderr}") from exc
 
 def start_playground(datasets: Optional[List[str]] = None,
-                     force_import: bool = False, verbose: bool = True) -> None:
+                     krb5_realm: Optional[str] = None,
+                     krb5_kdc: Optional[str] = None,
+                     force_import: bool = False,
+                     verbose: bool = True) -> None:
     """
     Start featurebyte playground environment
 
     Parameters
     ----------
     datasets : Optional[List[str]]
         List of datasets to import, by default None (import all datasets)
+    krb5_realm : Optional[str]
+        Kerberos realm, by default None
+    krb5_kdc : Optional[str]
+        Kerberos kdc of the krb5_realm, by default None
     force_import : bool
         Import datasets even if they are already imported, by default False
     verbose : bool
         Print verbose output, by default True
     """
     num_steps = 4
 
     # determine services to start
     services = get_service_names(ApplicationName.FEATUREBYTE) + get_service_names(ApplicationName.SPARK)
 
     step = 1
     logger.info(f"({step}/{num_steps}) Starting featurebyte services")
-    start_app(ApplicationName.FEATUREBYTE, services=services, verbose=verbose)
+    start_app(ApplicationName.FEATUREBYTE, krb5_realm=krb5_realm, krb5_kdc=krb5_kdc, services=services, verbose=verbose)
     step += 1
 
     # create local spark feature store
     logger.info(f"({step}/{num_steps}) Creating local spark feature store")
     Configurations().use_profile("local")
     feature_store = FeatureStore.get_or_create(
         name="playground",
```

### Comparing `featurebyte-0.3.1/featurebyte/enum.py` & `featurebyte-0.4.0/featurebyte/enum.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """
 This module contains all the enums used across different modules
 """
 from __future__ import annotations
 
+from typing import Literal
+
 import functools
 from enum import Enum
 
 from featurebyte.common.doc_util import FBAutoDoc
 
 
 @functools.total_ordering
@@ -161,15 +163,14 @@
 
         Returns
         -------
         str | None
         """
         mapping = {
             self.BOOL: "bool",
-            self.CHAR: "str",
             self.VARCHAR: "str",
             self.FLOAT: "float",
             self.INT: "int",
         }
         return mapping.get(self)  # type: ignore
 
 
@@ -247,14 +248,16 @@
     """
 
     __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.StorageType")
 
     FILE = "file", "Local file storage."
     S3 = "s3", "S3 Storage."
     GCS = "gcs", "Google Cloud Storage."
+    AZURE = "azure", "Azure Blob Storage."
+    WEBHDFS = "webhdfs", "WebHDFS."
 
 
 class SpecialColumnName(StrEnum):
     """
     Special column names such as POINT_IN_TIME
     """
 
@@ -283,14 +286,19 @@
     FIRST_TILE_INDEX = "__FB_FIRST_TILE_INDEX"
 
     POINT_IN_TIME_SQL_PLACEHOLDER = "__FB_POINT_IN_TIME_SQL_PLACEHOLDER"
 
     MIGRATION_VERSION = "MIGRATION_VERSION"
     ROW_INDEX = "__FB_ROW_INDEX"
 
+    ONLINE_STORE_RESULT_NAME_COLUMN = "AGGREGATION_RESULT_NAME"
+    ONLINE_STORE_VALUE_COLUMN = "VALUE"
+    ONLINE_STORE_VERSION_COLUMN = "VERSION"
+    ONLINE_STORE_VERSION_PLACEHOLDER_SUFFIX = "_VERSION_PLACEHOLDER"
+
 
 class WorkerCommand(StrEnum):
     """
     Command names for worker tasks
     """
 
     FEATURE_JOB_SETTING_ANALYSIS_CREATE = "FEATURE_JOB_SETTING_ANALYSIS_CREATE"
@@ -298,14 +306,17 @@
     HISTORICAL_FEATURE_TABLE_CREATE = "HISTORICAL_TABLE_CREATE"
     OBSERVATION_TABLE_CREATE = "OBSERVATION_TABLE_CREATE"
     DEPLOYMENT_CREATE_UPDATE = "DEPLOYMENT_CREATE_UPDATE"
     BATCH_REQUEST_TABLE_CREATE = "BATCH_REQUEST_TABLE_CREATE"
     BATCH_FEATURE_TABLE_CREATE = "BATCH_FEATURE_TABLE_CREATE"
     MATERIALIZED_TABLE_DELETE = "MATERIALIZED_TABLE_DELETE"
     BATCH_FEATURE_CREATE = "BATCH_FEATURE_CREATE"
+    FEATURE_LIST_CREATE_WITH_BATCH_FEATURE_CREATE = "FEATURE_LIST_CREATE_WITH_BATCH_FEATURE_CREATE"
+    STATIC_SOURCE_TABLE_CREATE = "STATIC_SOURCE_TABLE_CREATE"
+    TARGET_TABLE_CREATE = "TARGET_TABLE_CREATE"
     TEST = "TEST"
     TILE_COMPUTE = "TILE_COMPUTE"
 
 
 class TableDataType(StrEnum):
     """
     TableDataType enum
@@ -334,7 +345,41 @@
 
     EVENT_TIMESTAMP = "event_timestamp"
     EVENT_ID = "event_id"
     ITEM_ID = "item_id"
     DIMENSION_ID = "dimension_id"
     SCD_NATURAL_KEY_ID = "scd_natural_key_id"
     SCD_SURROGATE_KEY_ID = "scd_surrogate_key_id"
+
+
+class MaterializedTableNamePrefix(StrEnum):
+    """
+    Prefixes for the physical table names of materialized tables
+    """
+
+    OBSERVATION_TABLE = "OBSERVATION_TABLE"
+    HISTORICAL_FEATURE_TABLE = "HISTORICAL_FEATURE_TABLE"
+    BATCH_REQUEST_TABLE = "BATCH_REQUEST_TABLE"
+    BATCH_FEATURE_TABLE = "BATCH_FEATURE_TABLE"
+    TARGET_TABLE = "TARGET_TABLE"
+
+    @classmethod
+    def all(cls) -> list[str]:
+        """
+        List all prefixes
+
+        Returns
+        -------
+        list[str]
+        """
+        return [c.value for c in cls]
+
+
+class FunctionParameterInputForm(StrEnum):
+    """Generic function's parameter input form type"""
+
+    VALUE = "value"  # value is used as function argument
+    COLUMN = "column"  # column is used as function argument
+
+
+# enum used for handle conflict when saving object to persistent storage
+ConflictResolution = Literal["raise", "retrieve"]
```

### Comparing `featurebyte-0.3.1/featurebyte/exception.py` & `featurebyte-0.4.0/featurebyte/exception.py`

 * *Files 2% similar despite different names*

```diff
@@ -236,14 +236,20 @@
 
 class DocumentInconsistencyError(DocumentError):
     """
     Raise when the document consistency issue is detected
     """
 
 
+class DocumentModificationBlockedError(DocumentError):
+    """
+    Raise when the document modification is blocked
+    """
+
+
 class GraphInconsistencyError(DocumentError):
     """
     Raise when the graph consistency issue is detected
     """
 
 
 class QueryExecutionTimeOut(DocumentError):
@@ -318,20 +324,14 @@
     """
     Raise when get_join_column is called in ChangeView.
 
     ChangeView's don't have a primary key, and as such we don't expect there to be a join column.
     """
 
 
-class TileScheduleNotSupportedError(NotImplementedError):
-    """
-    Raise when the Tile Scheduling is not supported
-    """
-
-
 class NoFeatureJobSettingInSourceError(FeatureByteException):
     """
     Raise when the input table does not have any feature job setting.
     """
 
 
 class NoChangesInFeatureVersionError(DocumentError):
@@ -370,7 +370,13 @@
     """
 
 
 class TableNotFoundError(FeatureByteException):
     """
     Raise when the requested table does not exist in the data warehouse
     """
+
+
+class CatalogNotSpecifiedError(FeatureByteException):
+    """
+    Raise when the catalog is not specified in a catalog-specific request
+    """
```

### Comparing `featurebyte-0.3.1/featurebyte/feature_manager/manager.py` & `featurebyte-0.4.0/featurebyte/service/tile/tile_task_executor.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,262 +1,276 @@
 """
-Feature Manager class
+Tile Generate Schedule script
 """
-from __future__ import annotations
+from typing import Any, Dict, List, Optional
 
-from typing import Any, Optional
+import traceback
+from datetime import datetime, timedelta
 
-from datetime import datetime, timedelta, timezone
-
-import pandas as pd
-from pydantic import BaseModel, PrivateAttr
+import dateutil.parser
 
 from featurebyte.common import date_util
-from featurebyte.common.date_util import get_next_job_datetime
-from featurebyte.common.tile_util import tile_manager_from_session
-from featurebyte.feature_manager.sql_template import (
-    tm_delete_online_store_mapping,
-    tm_delete_tile_feature_mapping,
-    tm_feature_tile_monitor,
-    tm_upsert_online_store_mapping,
-    tm_upsert_tile_feature_mapping,
-)
+from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
-from featurebyte.models.online_store import OnlineFeatureSpec
-from featurebyte.models.tile import TileSpec, TileType
-from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
-from featurebyte.service.task_manager import TaskManager
+from featurebyte.models.tile import TileScheduledJobParameters, TileType
+from featurebyte.models.tile_job_log import TileJobLogModel
+from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
+from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
+from featurebyte.service.tile_job_log import TileJobLogService
+from featurebyte.service.tile_registry_service import TileRegistryService
 from featurebyte.session.base import BaseSession
-from featurebyte.tile.manager import TileManager
-from featurebyte.utils.snowflake.sql import escape_column_names
+from featurebyte.sql.tile_common import TileCommon
+from featurebyte.sql.tile_generate import TileGenerate
+from featurebyte.sql.tile_monitor import TileMonitor
+from featurebyte.sql.tile_schedule_online_store import TileScheduleOnlineStore
 
 logger = get_logger(__name__)
 
 
-class FeatureManager(BaseModel):
+class TileTaskExecutor:
     """
-    Snowflake Feature Manager class
+    This implements the steps that are run in the scheduled task. This includes tiles calculation,
+    tiles consistency monitoring and online store table updates.
     """
 
-    _session: BaseSession = PrivateAttr()
-    _tile_manager: TileManager = PrivateAttr()
-    _adapter: BaseAdapter = PrivateAttr()
-
     def __init__(
         self,
-        session: BaseSession,
-        task_manager: Optional[TaskManager] = None,
-        **kw: Any,
-    ) -> None:
+        online_store_table_version_service: OnlineStoreTableVersionService,
+        online_store_compute_query_service: OnlineStoreComputeQueryService,
+        tile_registry_service: TileRegistryService,
+        tile_job_log_service: TileJobLogService,
+    ):
+        self.online_store_table_version_service = online_store_table_version_service
+        self.online_store_compute_query_service = online_store_compute_query_service
+        self.tile_registry_service = tile_registry_service
+        self.tile_job_log_service = tile_job_log_service
+
+    # pylint: disable=too-many-locals,too-many-statements
+    async def execute(self, session: BaseSession, params: TileScheduledJobParameters) -> None:
         """
-        Custom constructor for TileSnowflake to instantiate a datasource session
+        Execute steps in the scheduled task
 
         Parameters
         ----------
         session: BaseSession
-            input session for datasource
-        task_manager: Optional[TaskManager]
-            input task manager
-        kw: Any
-            constructor arguments
+            Session object to be used for executing queries in data warehouse
+        params: TileScheduledJobParameters
+            Parameters for the scheduled task
+
+        Raises
+        ------
+        Exception
+            Related exception from the triggered stored procedures if it fails
         """
-        super().__init__(**kw)
-        self._session = session
-        self._adapter = get_sql_adapter(session.source_type)
-        self._tile_manager = tile_manager_from_session(
-            session=session,
-            task_manager=task_manager,
+        date_format = "%Y-%m-%d %H:%M:%S"
+        used_job_schedule_ts = params.job_schedule_ts or datetime.now().strftime(date_format)
+        candidate_last_tile_end_ts = dateutil.parser.isoparse(used_job_schedule_ts)
+
+        # derive the correct job schedule ts based on input job schedule ts
+        # the input job schedule ts might be between 2 intervals
+        corrected_job_ts = self._derive_correct_job_ts(
+            candidate_last_tile_end_ts, params.frequency_minute, params.time_modulo_frequency_second
+        )
+        logger.debug(
+            "Tile end ts details",
+            extra={
+                "corrected_job_ts": corrected_job_ts,
+                "candidate_last_tile_end_ts": candidate_last_tile_end_ts,
+            },
         )
 
-    async def online_enable(
-        self, feature_spec: OnlineFeatureSpec, schedule_time: datetime = datetime.utcnow()
-    ) -> None:
-        """
-        Schedule both online and offline tile jobs
-
-        Parameters
-        ----------
-        feature_spec: OnlineFeatureSpec
-            Instance of OnlineFeatureSpec
-        schedule_time: datetime
-            the moment of scheduling the job
-        """
-        logger.info(
-            "online_enable",
-            extra={"feature_name": feature_spec.feature.name, "schedule_time": schedule_time},
+        tile_end_ts = corrected_job_ts - timedelta(seconds=params.blind_spot_second)
+        tile_type = params.tile_type.upper()
+        lookback_period = params.frequency_minute * (params.monitor_periods + 1)
+        tile_id = params.tile_id.upper()
+
+        if tile_type == "OFFLINE":
+            lookback_period = params.offline_period_minute
+            tile_end_ts = tile_end_ts - timedelta(minutes=lookback_period)
+
+        tile_start_ts = tile_end_ts - timedelta(minutes=lookback_period)
+        tile_start_ts_str = tile_start_ts.strftime(date_format)
+        monitor_tile_start_ts_str = tile_start_ts_str
+
+        # use the last_tile_start_date from tile registry as tile_start_ts_str if it is earlier than tile_start_ts_str
+        tile_model = await self.tile_registry_service.get_tile_model(
+            params.tile_id, params.aggregation_id
         )
+        if tile_model is not None and tile_model.last_run_metadata_online is not None:
+            registry_last_tile_start_ts = tile_model.last_run_metadata_online.tile_end_date
+            logger.info(f"Last tile start date from registry - {registry_last_tile_start_ts}")
+
+            if registry_last_tile_start_ts.strftime(date_format) < tile_start_ts.strftime(
+                date_format
+            ):
+                logger.info(
+                    f"Use last tile start date from registry - {registry_last_tile_start_ts} instead of {tile_start_ts_str}"
+                )
+                tile_start_ts_str = registry_last_tile_start_ts.strftime(date_format)
+
+        session_id = f"{tile_id}|{datetime.now()}"
+
+        async def _add_log_entry(
+            log_status: str, log_message: str, formatted_traceback: Optional[str] = None
+        ) -> None:
+            document = TileJobLogModel(
+                tile_id=tile_id,
+                aggregation_id=params.aggregation_id,
+                tile_type=TileType[tile_type],  # TODO: tile_type to be passed as enum
+                session_id=session_id,
+                status=log_status,
+                message=log_message,
+                traceback=formatted_traceback,
+            )
+            await self.tile_job_log_service.create_document(document)
+
+        await _add_log_entry("STARTED", "")
 
-        # insert records into tile-feature mapping table
-        await self._update_tile_feature_mapping_table(feature_spec)
+        monitor_end_ts = tile_end_ts - timedelta(minutes=params.frequency_minute)
+        monitor_tile_end_ts_str = monitor_end_ts.strftime(date_format)
 
-        # enable tile generation with scheduled jobs
-        for tile_spec in feature_spec.feature.tile_specs:
-            tile_job_exists = await self._tile_manager.tile_job_exists(tile_spec=tile_spec)
-            if not tile_job_exists:
-                # enable online tiles scheduled job
-                tile_spec.user_id = feature_spec.feature.user_id
-                tile_spec.feature_store_id = feature_spec.feature.tabular_source.feature_store_id
-                tile_spec.catalog_id = feature_spec.feature.catalog_id
-
-                await self._tile_manager.schedule_online_tiles(tile_spec=tile_spec)
-                logger.debug(f"Done schedule_online_tiles for {tile_spec.aggregation_id}")
-
-                # enable offline tiles scheduled job
-                await self._tile_manager.schedule_offline_tiles(tile_spec=tile_spec)
-                logger.debug(f"Done schedule_offline_tiles for {tile_spec.aggregation_id}")
-
-            # generate historical tiles
-            await self._generate_historical_tiles(tile_spec=tile_spec)
-
-            # populate feature store
-            await self._populate_feature_store(tile_spec=tile_spec, schedule_time=schedule_time)
-
-    async def _populate_feature_store(self, tile_spec: TileSpec, schedule_time: datetime) -> None:
-        next_job_time = get_next_job_datetime(
-            input_dt=schedule_time,
-            frequency_minutes=tile_spec.frequency_minute,
-            time_modulo_frequency_seconds=tile_spec.time_modulo_frequency_second,
+        monitor_input_sql = params.sql.replace(
+            f"{InternalName.TILE_START_DATE_SQL_PLACEHOLDER}", "'" + monitor_tile_start_ts_str + "'"
+        ).replace(
+            f"{InternalName.TILE_END_DATE_SQL_PLACEHOLDER}", "'" + monitor_tile_end_ts_str + "'"
         )
-        job_schedule_ts = next_job_time - timedelta(minutes=tile_spec.frequency_minute)
-        job_schedule_ts_str = job_schedule_ts.strftime("%Y-%m-%d %H:%M:%S")
 
-        await self._tile_manager.populate_feature_store(tile_spec, job_schedule_ts_str)
+        tile_end_ts_str = tile_end_ts.strftime(date_format)
+        generate_input_sql = params.sql.replace(
+            f"{InternalName.TILE_START_DATE_SQL_PLACEHOLDER}", "'" + tile_start_ts_str + "'"
+        ).replace(f"{InternalName.TILE_END_DATE_SQL_PLACEHOLDER}", "'" + tile_end_ts_str + "'")
 
-    async def _generate_historical_tiles(self, tile_spec: TileSpec) -> None:
-        # generate historical tile_values
-        date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
-
-        # derive the latest tile_start_date
-        end_ind = date_util.timestamp_utc_to_tile_index(
-            datetime.utcnow(),
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
-        )
-        end_ts = date_util.tile_index_to_timestamp_utc(
-            end_ind,
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
+        logger.info(
+            "Tile Schedule information",
+            extra={
+                "tile_id": tile_id,
+                "tile_start_ts_str": tile_start_ts_str,
+                "tile_end_ts_str": tile_end_ts_str,
+                "tile_type": tile_type,
+            },
         )
-        end_ts_str = end_ts.strftime(date_format)
 
-        start_ts = datetime(1970, 1, 1, tzinfo=timezone.utc)
-        last_tile_start_ts_df = await self._session.execute_query(
-            f"SELECT LAST_TILE_START_DATE_OFFLINE FROM TILE_REGISTRY "
-            f"WHERE TILE_ID = '{tile_spec.tile_id}' "
-            f"AND AGGREGATION_ID = '{tile_spec.aggregation_id}' "
-            f"AND LAST_TILE_START_DATE_OFFLINE IS NOT NULL "
-        )
-        if last_tile_start_ts_df is not None and len(last_tile_start_ts_df) > 0:
-            # generate tiles from last_tile_start_date to now
-            logger.debug(f"last_tile_start_ts_df: {last_tile_start_ts_df}")
-            start_ts = last_tile_start_ts_df.iloc[0]["LAST_TILE_START_DATE_OFFLINE"]
-        logger.info(f"start_ts: {start_ts}")
-
-        start_ind = date_util.timestamp_utc_to_tile_index(
-            start_ts,
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
-        )
-        start_ts = date_util.tile_index_to_timestamp_utc(
-            start_ind,
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
+        tile_monitor_ins = TileMonitor(
+            session=session,
+            feature_store_id=params.feature_store_id,
+            tile_id=tile_id,
+            time_modulo_frequency_second=params.time_modulo_frequency_second,
+            blind_spot_second=params.blind_spot_second,
+            frequency_minute=params.frequency_minute,
+            sql=generate_input_sql,
+            monitor_sql=monitor_input_sql,
+            entity_column_names=params.entity_column_names,
+            value_column_names=params.value_column_names,
+            value_column_types=params.value_column_types,
+            tile_type=params.tile_type,
+            aggregation_id=params.aggregation_id,
+            tile_registry_service=self.tile_registry_service,
         )
-        start_ts_str = start_ts.strftime(date_format)
 
-        await self._tile_manager.generate_tiles(
-            tile_spec=tile_spec,
-            tile_type=TileType.OFFLINE,
-            end_ts_str=end_ts_str,
-            start_ts_str=start_ts_str,
-            last_tile_start_ts_str=end_ts_str,
+        tile_generate_ins = TileGenerate(
+            session=session,
+            feature_store_id=params.feature_store_id,
+            tile_id=tile_id,
+            time_modulo_frequency_second=params.time_modulo_frequency_second,
+            blind_spot_second=params.blind_spot_second,
+            frequency_minute=params.frequency_minute,
+            sql=generate_input_sql,
+            entity_column_names=params.entity_column_names,
+            value_column_names=params.value_column_names,
+            value_column_types=params.value_column_types,
+            tile_type=params.tile_type,
+            last_tile_start_str=tile_end_ts_str,
+            aggregation_id=params.aggregation_id,
+            tile_registry_service=self.tile_registry_service,
         )
 
-    async def _update_tile_feature_mapping_table(self, feature_spec: OnlineFeatureSpec) -> None:
-        """
-        Insert records into tile-feature mapping table
-
-        Parameters
-        ----------
-        feature_spec: OnlineFeatureSpec
-            Instance of OnlineFeatureSpec
-        """
-        for tile_spec in feature_spec.feature.tile_specs:
-            upsert_sql = tm_upsert_tile_feature_mapping.render(
-                tile_id=tile_spec.tile_id,
-                aggregation_id=tile_spec.aggregation_id,
-                feature_name=feature_spec.feature.name,
-                feature_type=feature_spec.value_type,
-                feature_version=feature_spec.feature.version.to_str(),
-                feature_readiness=str(feature_spec.feature.readiness),
-                feature_event_table_ids=",".join([str(i) for i in feature_spec.event_table_ids]),
-                is_deleted=False,
-            )
-            await self._session.execute_query(upsert_sql)
-            logger.debug(f"Done insert tile_feature_mapping for {tile_spec.aggregation_id}")
-
-        for query in feature_spec.precompute_queries:
-            upsert_sql = tm_upsert_online_store_mapping.render(
-                tile_id=query.tile_id,
-                aggregation_id=query.aggregation_id,
-                result_id=query.result_name,
-                result_type=query.result_type,
-                sql_query=self._adapter.escape_quote_char(query.sql),
-                online_store_table_name=query.table_name,
-                entity_column_names=",".join(escape_column_names(query.serving_names)),
-                is_deleted=False,
-            )
-            await self._session.execute_query(upsert_sql)
-            logger.debug(f"Done insert tile_feature_mapping for {query.result_name}")
-
-    async def online_disable(self, feature_spec: OnlineFeatureSpec) -> None:
-        """
-        Schedule both online and offline tile jobs
+        tile_online_store_ins = TileScheduleOnlineStore(
+            session=session,
+            aggregation_id=params.aggregation_id,
+            job_schedule_ts_str=corrected_job_ts.strftime(date_format),
+            online_store_table_version_service=self.online_store_table_version_service,
+            online_store_compute_query_service=self.online_store_compute_query_service,
+        )
 
-        Parameters
-        ----------
-        feature_spec: OnlineFeatureSpec
-            input feature instance
-        """
-        # delete records from tile-feature mapping table
-        for agg_id in feature_spec.aggregation_ids:
-            delete_sql = tm_delete_tile_feature_mapping.render(
-                aggregation_id=agg_id,
-                feature_name=feature_spec.feature.name,
-                feature_version=feature_spec.feature.version.to_str(),
-            )
-            await self._session.execute_query(delete_sql)
-            logger.debug(f"Done delete tile_feature_mapping for {agg_id}")
-            delete_sql = tm_delete_online_store_mapping.render(aggregation_id=agg_id)
-            await self._session.execute_query(delete_sql)
-            logger.debug(f"Done delete online_store_mapping for {agg_id}")
-
-        # disable tile scheduled jobs
-        for tile_spec in feature_spec.feature.tile_specs:
-            await self._tile_manager.remove_tile_jobs(tile_spec)
-
-    async def retrieve_feature_tile_inconsistency_data(
-        self, query_start_ts: str, query_end_ts: str
-    ) -> pd.DataFrame:
+        step_specs: List[Dict[str, Any]] = [
+            {
+                "name": "tile_monitor",
+                "trigger": tile_monitor_ins,
+                "status": {
+                    "fail": "MONITORED_FAILED",
+                    "success": "MONITORED",
+                },
+            },
+            {
+                "name": "tile_generate",
+                "trigger": tile_generate_ins,
+                "status": {
+                    "fail": "GENERATED_FAILED",
+                    "success": "GENERATED",
+                },
+            },
+            {
+                "name": "tile_online_store",
+                "trigger": tile_online_store_ins,
+                "status": {
+                    "fail": "ONLINE_STORE_FAILED",
+                    "success": "COMPLETED",
+                },
+            },
+        ]
+
+        for spec in step_specs:
+            try:
+                logger.info(f"Calling {spec['name']}")
+                tile_ins: TileCommon = spec["trigger"]
+                await tile_ins.execute()
+                logger.info(f"End of calling {spec['name']}")
+            except Exception as exception:
+                message = str(exception).replace("'", "")
+                fail_code = spec["status"]["fail"]
+                formatted_traceback = traceback.format_exc()
+
+                logger.error(f"fail_insert_sql exception: {exception}")
+                await _add_log_entry(fail_code, message, formatted_traceback)
+                raise exception
+
+            success_code = spec["status"]["success"]
+            await _add_log_entry(success_code, "")
+
+    def _derive_correct_job_ts(
+        self, input_dt: datetime, frequency_minutes: int, time_modulo_frequency_seconds: int
+    ) -> datetime:
         """
-        Retrieve the raw table of feature tile inconsistency monitoring
+        Derive correct job schedule datetime
 
         Parameters
         ----------
-        query_start_ts: str
-            start monitoring timestamp of tile inconsistency
-        query_end_ts: str
-            end monitoring timestamp of tile inconsistency
+        input_dt: datetime
+            input job schedule datetime
+        frequency_minutes: int
+            frequency in minutes
+        time_modulo_frequency_seconds: int
+            time modulo frequency in seconds
 
         Returns
         -------
-            raw table of feature-tile inconsistency as dataframe
+        datetime
         """
-        sql = tm_feature_tile_monitor.render(
-            query_start_ts=query_start_ts, query_end_ts=query_end_ts
+        input_dt = input_dt.replace(tzinfo=None)
+
+        next_job_time = date_util.get_next_job_datetime(
+            input_dt=input_dt,
+            frequency_minutes=frequency_minutes,
+            time_modulo_frequency_seconds=time_modulo_frequency_seconds,
+        )
+
+        logger.debug(
+            "Inside derive_correct_job_ts",
+            extra={"next_job_time": next_job_time, "input_dt": input_dt},
         )
-        result = await self._session.execute_query(sql)
-        return result
+
+        if next_job_time == input_dt:
+            # if next_job_time is same as input_dt, then return next_job_time
+            return next_job_time
+
+        # if next_job_time is not same as input_dt, then return (next_job_time - frequency_minutes)
+        return next_job_time - timedelta(minutes=frequency_minutes)
```

### Comparing `featurebyte-0.3.1/featurebyte/feature_manager/model.py` & `featurebyte-0.4.0/featurebyte/feature_manager/model.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,20 +1,14 @@
 """
 This modules contains feature manager specific models
 """
 from __future__ import annotations
 
-from typing import List, Optional
-
-from pydantic import Field, StrictStr
-
 from featurebyte.enum import SourceType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
 from featurebyte.models.feature import FeatureModel
-from featurebyte.models.feature_list import FeatureListModel, FeatureListStatus
 from featurebyte.models.tile import TileSpec
 from featurebyte.query_graph.sql.interpreter import GraphInterpreter
 
 
 class ExtendedFeatureModel(FeatureModel):
     """
     ExtendedFeatureModel contains tile manager specific methods or properties
@@ -55,36 +49,11 @@
                 tile_sql=info.sql,
                 entity_column_names=entity_column_names,
                 value_column_names=info.tile_value_columns,
                 value_column_types=info.tile_value_types,
                 tile_id=info.tile_table_id,
                 aggregation_id=info.aggregation_id,
                 category_column_name=info.value_by_column,
+                feature_store_id=self.tabular_source.feature_store_id,
             )
             out.append(tile_spec)
         return out
-
-
-class FeatureSignature(FeatureByteBaseModel):
-    """
-    FeatureSignature class used in FeatureList object
-
-    id: PydanticObjectId
-        Feature id of the object
-    name: str
-        Name of the feature
-    version: VersionIdentifier
-        Feature version
-    """
-
-    id: PydanticObjectId
-    name: Optional[StrictStr]
-    version: VersionIdentifier
-
-
-class ExtendedFeatureListModel(FeatureListModel):
-    """
-    ExtendedFeatureListModel class has additional features attribute
-    """
-
-    feature_signatures: List[FeatureSignature] = Field(default_factory=list)
-    status: FeatureListStatus = Field(allow_mutation=False, default=FeatureListStatus.DRAFT)
```

### Comparing `featurebyte-0.3.1/featurebyte/feature_utility.py` & `featurebyte-0.4.0/featurebyte/feature_utility.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/logging.py` & `featurebyte-0.4.0/featurebyte/logging.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/middleware.py` & `featurebyte-0.4.0/featurebyte/middleware.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 
 from fastapi import FastAPI, Request, Response
 from pydantic import ValidationError
 from starlette.middleware.base import BaseHTTPMiddleware
 from starlette.responses import JSONResponse
 
 from featurebyte.exception import (
+    CatalogNotSpecifiedError,
     CredentialsError,
     DatabaseNotFoundError,
     DocumentConflictError,
     DocumentError,
     DocumentNotFoundError,
     FeatureStoreSchemaCollisionError,
     LimitExceededError,
@@ -213,14 +214,20 @@
 
 ExecutionContext.register(
     TableNotFoundError,
     handle_status_code=HTTPStatus.FAILED_DEPENDENCY,
     handle_message="Table not found. Please specify a valid table name.",
 )
 
+ExecutionContext.register(
+    CatalogNotSpecifiedError,
+    handle_status_code=HTTPStatus.FAILED_DEPENDENCY,
+    handle_message="Catalog not specified. Please specify a catalog.",
+)
+
 
 class ExceptionMiddleware(BaseHTTPMiddleware):
     """
     Middleware used by FastAPI to process each request
     """
 
     def __init__(self, app: FastAPI):
```

### Comparing `featurebyte-0.3.1/featurebyte/migration/migration_data_service.py` & `featurebyte-0.4.0/featurebyte/migration/migration_data_service.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/migration/model.py` & `featurebyte-0.4.0/featurebyte/migration/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 
 class BaseMigrationMetadataModel(FeatureByteBaseDocumentModel):
     """
     BaseMigrationMetadata model
     """
 
-    class Settings:
+    class Settings(FeatureByteBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "__migration_metadata"
         unique_constraints: List[UniqueValuesConstraint] = [
             UniqueValuesConstraint(
```

### Comparing `featurebyte-0.3.1/featurebyte/migration/run.py` & `featurebyte-0.4.0/featurebyte/migration/run.py`

 * *Files 11% similar despite different names*

```diff
@@ -4,36 +4,34 @@
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator, Callable, cast
 
 import importlib
 import inspect
 
+from celery import Celery
+
 from featurebyte.common.path_util import import_submodules
 from featurebyte.logging import get_logger
 from featurebyte.migration.migration_data_service import SchemaMetadataService
 from featurebyte.migration.model import MigrationMetadata, SchemaMetadataModel, SchemaMetadataUpdate
 from featurebyte.migration.service import MigrationInfo
-from featurebyte.migration.service.mixin import DataWarehouseMigrationMixin
-from featurebyte.models.base import (
-    DEFAULT_CATALOG_ID,
-    FeatureByteBaseDocumentModel,
-    FeatureByteBaseModel,
-    User,
+from featurebyte.migration.service.mixin import (
+    BaseMigrationServiceMixin,
+    DataWarehouseMigrationMixin,
 )
+from featurebyte.models.base import DEFAULT_CATALOG_ID, User
 from featurebyte.persistent.base import Persistent
 from featurebyte.persistent.mongo import MongoDB
-from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
-from featurebyte.service.base_document import BaseDocumentService
+from featurebyte.routes.app_container_config import _get_class_name
+from featurebyte.routes.lazy_app_container import LazyAppContainer
+from featurebyte.routes.registry import app_container_config
 from featurebyte.utils.credential import MongoBackedCredentialProvider
-
-BaseDocumentServiceT = BaseDocumentService[
-    FeatureByteBaseDocumentModel, FeatureByteBaseModel, BaseDocumentServiceUpdateSchema
-]
-
+from featurebyte.utils.storage import get_storage, get_temp_storage
+from featurebyte.worker import get_celery
 
 logger = get_logger(__name__)
 
 
 def _extract_migrate_method_marker(migrate_method: Any) -> MigrationInfo:
     # extract migrate_method marker from the given migrate_method
     marker = getattr(migrate_method, "_MigrationInfo__marker")
@@ -79,15 +77,15 @@
     migration_service_dir = f"{__name__.rsplit('.', 1)[0]}.service"
     importlib.import_module(migration_service_dir)
 
     migrate_methods = {}
     for mod in import_submodules(migration_service_dir).values():
         for attr_name in dir(mod):
             attr = getattr(mod, attr_name)
-            if inspect.isclass(attr) and issubclass(attr, BaseDocumentService):
+            if inspect.isclass(attr) and issubclass(attr, BaseMigrationServiceMixin):
                 if data_warehouse_migrations_only and not issubclass(
                     attr, DataWarehouseMigrationMixin
                 ):
                     continue
                 for version, migrate_method_name in _extract_migrate_methods(attr):
                     migrate_method_data = {
                         "module": attr.__module__,
@@ -104,113 +102,130 @@
     return migrate_methods
 
 
 async def migrate_method_generator(
     user: Any,
     persistent: Persistent,
     get_credential: Any,
+    celery: Celery,
     schema_metadata: SchemaMetadataModel,
     include_data_warehouse_migrations: bool,
-) -> AsyncGenerator[tuple[BaseDocumentServiceT, Callable[..., Any]], None]:
+) -> AsyncGenerator[tuple[BaseMigrationServiceMixin, Callable[..., Any]], None]:
     """
     Migrate method generator
 
     Parameters
     ----------
     user: Any
         User object contains id information
     persistent: Persistent
         Persistent storage object
     get_credential: Any
         Callback to retrieve credential
+    celery: Celery
+        Celery object
     schema_metadata: SchemaMetadataModel
         Schema metadata
     include_data_warehouse_migrations: bool
         Whether to include data warehouse migrations
 
     Yields
     ------
     migrate_service
         Service to be migrated
     migrate_method
         Migration method
     """
+    app_container = LazyAppContainer(
+        user=user,
+        persistent=persistent,
+        catalog_id=DEFAULT_CATALOG_ID,
+        temp_storage=get_temp_storage(),
+        celery=get_celery(),
+        storage=get_storage(),
+        app_container_config=app_container_config,
+    )
     migrate_methods = retrieve_all_migration_methods()
     version_start = schema_metadata.version + 1
     for version in range(version_start, len(migrate_methods) + 1):
         migrate_method_data = migrate_methods[version]
         module = importlib.import_module(migrate_method_data["module"])
         migrate_service_class = getattr(module, migrate_method_data["class"])
-        migrate_service = migrate_service_class(
-            user=user, persistent=persistent, catalog_id=DEFAULT_CATALOG_ID
-        )
+        migrate_service = app_container.get(_get_class_name(migrate_service_class.__name__))
         if isinstance(migrate_service, DataWarehouseMigrationMixin):
             if not include_data_warehouse_migrations:
                 continue
             migrate_service.set_credential_callback(get_credential)
+            migrate_service.set_celery(celery)
         migrate_method = getattr(migrate_service, migrate_method_data["method"])
         yield migrate_service, migrate_method
 
 
-async def post_migration_sanity_check(service: BaseDocumentServiceT) -> None:
+async def post_migration_sanity_check(service: BaseMigrationServiceMixin) -> None:
     """
     Post migration sanity check
 
     Parameters
     ----------
-    service: BaseDocumentServiceT
+    service: BaseMigrationServiceMixin
         Service used to perform the sanity check
     """
     # check document deserialization
-    docs = await service.list_documents(page_size=0)
+    docs = await service.delegate_service.list_documents_as_dict(page_size=0)
     step_size = max(len(docs["data"]) // 5, 1)
     audit_record_count = 0
     for i, doc_dict in enumerate(docs["data"]):
-        document = service.document_class(**doc_dict)
+        document = service.delegate_service.document_class(**doc_dict)
 
         # check audit records
         if i % step_size == 0:
-            async for _ in service.historical_document_generator(document_id=document.id):
+            async for _ in service.delegate_service.historical_document_generator(
+                document_id=document.id
+            ):
                 audit_record_count += 1
 
     logger.info(
         f"Successfully loaded {len(docs['data'])} records & {audit_record_count} audit records."
     )
 
 
 async def run_migration(
     user: Any,
     persistent: Persistent,
     get_credential: Any,
+    celery: Celery,
     include_data_warehouse_migrations: bool = True,
 ) -> None:
     """
     Run database migration
 
     Parameters
     ----------
     user: Any
         User object
     persistent: Persistent
         Persistent object
     get_credential: Any
         Callback to retrieve credential
+    celery: Celery
+        Celery object
     include_data_warehouse_migrations: bool
         Whether to include data warehouse migrations
     """
     schema_metadata_service = SchemaMetadataService(
         user=user, persistent=persistent, catalog_id=DEFAULT_CATALOG_ID
     )
     schema_metadata = await schema_metadata_service.get_or_create_document(
         name=MigrationMetadata.SCHEMA_METADATA
     )
     method_generator = migrate_method_generator(
         user=user,
         persistent=persistent,
         get_credential=get_credential,
+        celery=celery,
         schema_metadata=schema_metadata,
         include_data_warehouse_migrations=include_data_warehouse_migrations,
     )
     async for service, migrate_method in method_generator:
         marker = _extract_migrate_method_marker(migrate_method)
         logger.info(f"Run migration (version={marker.version}): {marker.description}")
         await migrate_method()
@@ -236,9 +251,10 @@
         Mongo persistent object
     """
     credential_provider = MongoBackedCredentialProvider(persistent=persistent)
     await run_migration(
         User(),
         persistent,
         credential_provider.get_credential,
+        celery=get_celery(),
         include_data_warehouse_migrations=False,
     )
```

### Comparing `featurebyte-0.3.1/featurebyte/migration/service/__init__.py` & `featurebyte-0.4.0/featurebyte/migration/service/__init__.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/migration/service/data_warehouse.py` & `featurebyte-0.4.0/featurebyte/migration/service/data_warehouse.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,58 +1,55 @@
 """
 Migration service for data warehouse working schema
 """
 from __future__ import annotations
 
-from typing import Any, Optional
+from typing import Optional
 
 import textwrap
 
 import pandas as pd
 from snowflake.connector.errors import ProgrammingError
 
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.logging import get_logger
 from featurebyte.migration.service import migrate
 from featurebyte.migration.service.mixin import DataWarehouseMigrationMixin
-from featurebyte.models.base import DEFAULT_CATALOG_ID
 from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.persistent.base import Persistent
+from featurebyte.models.persistent import QueryFilter
+from featurebyte.persistent import Persistent
 from featurebyte.service.feature import FeatureService
+from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.working_schema import WorkingSchemaService
 from featurebyte.session.base import BaseSession
 
 logger = get_logger(__name__)
 
 
 class TileColumnTypeExtractor:
     """
     Responsible for building a mapping from tile column names to tile types based on saved Features
     """
 
-    def __init__(self, user: Any, persistent: Persistent):
-        self.feature_service = FeatureService(user, persistent, catalog_id=DEFAULT_CATALOG_ID)
+    def __init__(self, feature_service: FeatureService):
+        self.feature_service = feature_service
         self.tile_column_name_to_type: Optional[dict[str, str]] = None
 
     async def setup(self) -> None:
         """
         Set up the object by loading existing Feature documents
         """
-        self.tile_column_name_to_type = await self._build_tile_column_name_to_type_mapping(
-            self.feature_service
-        )
+        self.tile_column_name_to_type = await self._build_tile_column_name_to_type_mapping()
 
-    @staticmethod
-    async def _build_tile_column_name_to_type_mapping(
-        feature_service: FeatureService,
-    ) -> dict[str, str]:
+    async def _build_tile_column_name_to_type_mapping(self) -> dict[str, str]:
         tile_column_name_to_type = {}
         # activate use of raw query filter to retrieve all documents regardless of catalog membership
-        with feature_service.allow_use_raw_query_filter():
-            feature_documents = feature_service.list_documents_iterator(
+        with self.feature_service.allow_use_raw_query_filter():
+            feature_documents = self.feature_service.list_documents_as_dict_iterator(
                 query_filter={}, use_raw_query_filter=True
             )
 
             async for doc in feature_documents:
                 feature_model = ExtendedFeatureModel(**doc)
                 try:
                     tile_specs = feature_model.tile_specs
@@ -116,30 +113,38 @@
     DataWarehouseMigrationService class
 
     Responsible for migrating the featurebyte working schema in the data warehouse
     """
 
     extractor: TileColumnTypeExtractor
 
+    def __init__(
+        self,
+        persistent: Persistent,
+        session_manager_service: SessionManagerService,
+        feature_store_service: FeatureStoreService,
+        tile_column_type_extractor: TileColumnTypeExtractor,
+    ):
+        super().__init__(persistent, session_manager_service, feature_store_service)
+        self.tile_column_type_extractor = tile_column_type_extractor
+
     async def _add_tile_value_types_column(self, migration_version: int) -> None:
         """
         Add VALUE_COLUMN_TYPES column in TILE_REGISTRY table
 
         Parameters
         ----------
         migration_version: int
             Migration version
         """
         collection_names = await self.persistent.list_collection_names()
         if "feature_store" not in collection_names:
             return
 
-        tile_column_type_extractor = TileColumnTypeExtractor(self.user, self.persistent)
-        await tile_column_type_extractor.setup()
-        self.extractor = tile_column_type_extractor
+        await self.tile_column_type_extractor.setup()
 
         # migrate all records and audit records
         await self.migrate_all_records(version=migration_version)
 
     @migrate(version=1, description="Add VALUE_COLUMN_TYPES column in TILE_REGISTRY table")
     async def add_tile_value_types_column(self) -> None:
         """
@@ -163,15 +168,15 @@
     ) -> None:
         _ = feature_store
 
         df_tile_registry = await session.execute_query("SELECT * FROM TILE_REGISTRY")
         if "VALUE_COLUMN_TYPES" in df_tile_registry:  # type: ignore[operator]
             return
 
-        df_tile_registry["VALUE_COLUMN_TYPES"] = self.extractor.get_tile_column_types_from_names(  # type: ignore[index]
+        df_tile_registry["VALUE_COLUMN_TYPES"] = self.tile_column_type_extractor.get_tile_column_types_from_names(  # type: ignore[index]
             df_tile_registry["VALUE_COLUMN_NAMES"]  # type: ignore[index]
         )
         await session.register_table(
             "UPDATED_TILE_REGISTRY",
             df_tile_registry[["TILE_ID", "VALUE_COLUMN_NAMES", "VALUE_COLUMN_TYPES"]],  # type: ignore[index]
         )
 
@@ -218,29 +223,36 @@
     """
     Reset working schema from scratch due to major changes in the warehouse schema in order to:
 
     1. Add online serving support for complex features involving multiple tile tables
     2. Fix jobs scheduling bug caused by tile_id collision
     """
 
+    def __init__(
+        self,
+        persistent: Persistent,
+        session_manager_service: SessionManagerService,
+        feature_store_service: FeatureStoreService,
+        working_schema_service: WorkingSchemaService,
+    ):
+        super().__init__(persistent, session_manager_service, feature_store_service)
+        self.working_schema_service = working_schema_service
+
     @migrate(version=3, description="Reset working schema from scratch")
-    async def reset_working_schema(self, query_filter: Optional[dict[str, Any]] = None) -> None:
+    async def reset_working_schema(self, query_filter: Optional[QueryFilter] = None) -> None:
         """
         Reset working schema from scratch
 
         Parameters
         ----------
-        query_filter: Optional[dict[str, Any]]
+        query_filter: Optional[QueryFilter]
             Query filter used to filter the documents used for migration. Used only in test when
             intending to migrate a specific document.
         """
         await self.migrate_all_records(version=8, query_filter=query_filter)
 
     async def migrate_record_with_session(
         self, feature_store: FeatureStoreModel, session: BaseSession
     ) -> None:
-        working_schema_service = WorkingSchemaService(
-            user=self.user, persistent=self.persistent, catalog_id=self.catalog_id
-        )
-        await working_schema_service.recreate_working_schema(
+        await self.working_schema_service.recreate_working_schema(
             feature_store_id=feature_store.id, session=session
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/migration/service/mixin.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/online_serving.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,266 +1,258 @@
 """
-MigrationServiceMixin class
+SQL generation for online serving
 """
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, Iterator, Optional, Protocol
+from typing import Any, Dict, List, Optional, Union, cast
 
-from abc import ABC, abstractmethod
-from contextlib import contextmanager
+import time
+from dataclasses import dataclass
 
-from bson import ObjectId
+import pandas as pd
+from sqlglot import expressions
+from sqlglot.expressions import select
 
-from featurebyte.enum import InternalName
-from featurebyte.exception import CredentialsError
+from featurebyte.common.utils import prepare_dataframe_for_json
+from featurebyte.enum import SourceType, SpecialColumnName
 from featurebyte.logging import get_logger
-from featurebyte.models.base import User
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.persistent import Document, QueryFilter
-from featurebyte.persistent.base import Persistent
-from featurebyte.service.base_document import DocumentUpdateSchema
-from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.session_manager import SessionManagerService
-from featurebyte.service.session_validator import SessionValidatorService
-from featurebyte.utils.credential import MongoBackedCredentialProvider
-
-if TYPE_CHECKING:
-    from featurebyte.session.base import BaseSession
-
+from featurebyte.models.batch_request_table import BatchRequestTableModel
+from featurebyte.models.parent_serving import ParentServingPreparation
+from featurebyte.query_graph.enum import NodeType
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.node.schema import TableDetails
+from featurebyte.query_graph.sql.adapter import get_sql_adapter
+from featurebyte.query_graph.sql.common import (
+    REQUEST_TABLE_NAME,
+    get_fully_qualified_table_name,
+    quoted_identifier,
+    sql_to_string,
+)
+from featurebyte.query_graph.sql.dataframe import construct_dataframe_sql_expr
+from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
+from featurebyte.query_graph.sql.online_serving_util import get_version_placeholder
+from featurebyte.query_graph.sql.template import SqlExpressionTemplate
+from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
+from featurebyte.session.base import BaseSession
 
 logger = get_logger(__name__)
 
 
-class BaseMigrationServiceMixin(Protocol):
-    """BaseMigrationServiceMixin class"""
-
-    persistent: Persistent
-
-    @property
-    @abstractmethod
-    def collection_name(self) -> str:
-        """
-        Collection name
-
-        Returns
-        -------
-        Collection name
-        """
-
-    @abstractmethod
-    def _construct_list_query_filter(
-        self,
-        query_filter: Optional[dict[str, Any]] = None,
-        use_raw_query_filter: bool = False,
-        **kwargs: Any,
-    ) -> QueryFilter:
-        ...
-
-    @abstractmethod
-    async def migrate_record(self, document: Document, version: Optional[int]) -> None:
-        """
-        Perform migration for a Document
-
-        Parameters
-        ----------
-        document: Document
-            Document to be migrated
-        version: Optional[int]
-            Migration number
-        """
-
-    @abstractmethod
-    @contextmanager
-    def allow_use_raw_query_filter(self) -> Iterator[None]:
-        """Activate use of raw query filter"""
-
-    async def migrate_all_records(
-        self,
-        query_filter: Optional[dict[str, Any]] = None,
-        page_size: int = 10,
-        version: Optional[int] = None,
-    ) -> None:
-        """
-        Migrate all records in this service's collection & audit collection
-
-        Parameters
-        ----------
-        query_filter: Optional[dict[str, Any]]
-            Query filter used to filter the documents used for migration
-        page_size: int
-            Page size
-        version: Optional[int]
-            Optional migration version number
-        """
-        # migrate all records and audit records
-        if query_filter is None:
-            with self.allow_use_raw_query_filter():
-                query_filter = dict(self._construct_list_query_filter(use_raw_query_filter=True))
-
-        logger.info(f'Start migrating all records (collection: "{self.collection_name}")')
-        to_iterate, page = True, 1
-        while to_iterate:
-            docs, total = await self.persistent.find(
-                collection_name=self.collection_name,
-                query_filter=query_filter,
-                page=page,
-                page_size=page_size,
-            )
-            for doc in docs:
-                await self.migrate_record(doc, version)
-
-            to_iterate = bool(total > (page * page_size))
-            page += 1
-
-        logger.info(f'Complete migration (collection: "{self.collection_name}")')
-
-
-class MigrationServiceMixin(BaseMigrationServiceMixin, ABC):
-    """MigrationServiceMixin class"""
-
-    @classmethod
-    def migrate_document_record(cls, record: dict[str, Any]) -> dict[str, Any]:
-        """
-        Migrate older document record to the current document record format
-
-        Parameters
-        ----------
-        record: dict[str, Any]
-            Older document record
-
-        Returns
-        -------
-        dict[str, Any]
-            Record in newer format
-        """
-        return cls.document_class(**record).dict(by_alias=True)  # type: ignore
-
-    async def migrate_record(self, document: Document, version: Optional[int]) -> None:
-        _ = version
-        await self.persistent.migrate_record(
-            collection_name=self.collection_name,
-            document=document,
-            migrate_func=self.migrate_document_record,
-        )
+def is_online_store_eligible(graph: QueryGraph, node: Node) -> bool:
+    """
+    Check whether the feature represented by the given node is eligible for online store lookup
 
+    Parameters
+    ----------
+    graph : QueryGraph
+        Query graph
+    node : Node
+        Query graph node
+
+    Returns
+    -------
+    bool
+    """
+    op_struct = graph.extract_operation_structure(node, keep_all_source_columns=True)
+    if not op_struct.is_time_based:
+        return False
+    has_point_in_time_groupby = False
+    for _ in graph.iterate_nodes(node, NodeType.GROUPBY):
+        has_point_in_time_groupby = True
+    return has_point_in_time_groupby
 
-class DataWarehouseMigrationMixin(FeatureStoreService, BaseMigrationServiceMixin, ABC):
-    """DataWarehouseMigrationMixin class
 
-    Provides common functionalities required for migrating data warehouse
+@dataclass
+class OnlineStoreRetrievalTemplate:
     """
+    SQL code template for retrieving data from online store
 
-    get_credential: Any
+    sql_template: SqlExpressionTemplate
+        SQL code template with online store table version placeholders
+    aggregation_result_names: list[str]
+        Aggregation result names involved in the online serving query
+    """
 
-    async def create_document(self, data: DocumentUpdateSchema) -> Document:  # type: ignore[override]
-        # Currently any implementation of DataWarehouseMigrationMixin is required to only make
-        # modification to data warehouse and not to mongo due to the way they are tested.
-        raise NotImplementedError()
-
-    async def update_document(  # type: ignore[override]
-        self,
-        document_id: ObjectId,
-        data: DocumentUpdateSchema,
-        exclude_none: bool = True,
-        document: Optional[Document] = None,
-        return_document: bool = True,
-    ) -> Optional[Document]:
-        # Currently any implementation of DataWarehouseMigrationMixin is required to only make
-        # modification to data warehouse and not to mongo due to the way they are tested.
-        raise NotImplementedError()
+    sql_template: SqlExpressionTemplate
+    aggregation_result_names: list[str]
 
-    async def get_session(self, feature_store: FeatureStoreModel) -> BaseSession:
+    def fill_version_placeholders(self, versions: Dict[str, int]) -> expressions.Select:
         """
-        Get a BaseSession object corresponding to the provided feature store model
+        Fill the version placeholders in the SQL template
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            Feature store model
+        versions : Dict[str, int]
+            Mapping from aggregation result name to version
 
         Returns
         -------
-        BaseSession
+        expressions.Select
         """
-        credential_provider = MongoBackedCredentialProvider(persistent=self.persistent)
-        user = User(id=feature_store.user_id)
-        session_validator_service = SessionValidatorService(
-            user, self.persistent, self.catalog_id, credential_provider
-        )
-        session_manager_service = SessionManagerService(
-            user=user,
-            persistent=self.persistent,
-            catalog_id=self.catalog_id,
-            credential_provider=credential_provider,
-            session_validator_service=session_validator_service,
-        )
-        session = await session_manager_service.get_feature_store_session(
-            feature_store, get_credential=self.get_credential
+        placeholders_mapping = {
+            get_version_placeholder(agg_result_name): version
+            for (agg_result_name, version) in versions.items()
+        }
+        return cast(
+            expressions.Select, self.sql_template.render(placeholders_mapping, as_str=False)
         )
-        return session
 
-    def set_credential_callback(self, get_credential: Any) -> None:
-        """
-        Set the get_credential callback
 
-        Parameters
-        ----------
-        get_credential: Any
-            Callback to retrieve credential
-        """
-        self.get_credential = get_credential
+def get_online_store_retrieval_template(
+    graph: QueryGraph,
+    nodes: list[Node],
+    source_type: SourceType,
+    request_table_columns: list[str],
+    request_table_name: Optional[str] = None,
+    request_table_expr: Optional[expressions.Select] = None,
+    parent_serving_preparation: Optional[ParentServingPreparation] = None,
+) -> OnlineStoreRetrievalTemplate:
+    """
+    Construct SQL code that can be used to lookup pre-computed features from online store
 
-    async def migrate_record(self, document: Document, version: Optional[int]) -> None:
-        # Data warehouse migration requires version to be provided when calling migrate_all_records
-        # so that the warehouse metadata can be updated accordingly
-        assert version is not None
-
-        feature_store = FeatureStoreModel(**document)
-        try:
-            session = await self.get_session(feature_store)
-            # Verify that session is fully functional by attempting to execute a query
-            _ = await session.execute_query("SELECT 1 AS A")
-        except CredentialsError:
-            logger.debug(f"Got CredentialsError, skipping migration for {feature_store.name}")
-            return
-        except Exception:  # pylint: disable=broad-except
-            logger.exception(
-                f"Got unexpected error when creating session, skipping migration for {feature_store.name}"
-            )
-            return
-        await self.migrate_record_with_session(feature_store, session)
-        await self.update_migration_version(session, version)
+    Parameters
+    ----------
+    graph: QueryGraph
+        Query graph
+    nodes: list[Node]
+        List of query graph nodes
+    source_type: SourceType
+        Source type information
+    request_table_columns: list[str]
+        Request table columns
+    request_table_name: Optional[str]
+        Name of the request table
+    request_table_expr: Optional[expressions.Select]
+        Select statement for the request table
+    parent_serving_preparation: Optional[ParentServingPreparation]
+        Preparation required for serving parent features
+
+    Returns
+    -------
+    expressions.Select
+    """
+    planner = FeatureExecutionPlanner(
+        graph,
+        source_type=source_type,
+        is_online_serving=True,
+        parent_serving_preparation=parent_serving_preparation,
+    )
+    plan = planner.generate_plan(nodes)
+
+    # Form a request table as a common table expression (CTE) and add the point in time column
+    expr = select(*[f"REQ.{quoted_identifier(col).sql()}" for col in request_table_columns])
+    adapter = get_sql_adapter(source_type)
+    expr = expr.select(
+        expressions.alias_(adapter.current_timestamp(), alias=SpecialColumnName.POINT_IN_TIME)
+    )
+    request_table_columns.append(SpecialColumnName.POINT_IN_TIME)
+
+    if request_table_name is not None:
+        # Case 1: Request table is already registered as a table with a name
+        expr = expr.from_(expressions.alias_(quoted_identifier(request_table_name), alias="REQ"))
+    else:
+        # Case 2: Request table is provided as an embedded query
+        assert request_table_expr is not None
+        expr = expr.from_(request_table_expr.subquery(alias="REQ"))
+        request_table_name = REQUEST_TABLE_NAME
+
+    request_table_name = "ONLINE_" + request_table_name
+    ctes = [(request_table_name, expr)]
+
+    output_expr = plan.construct_combined_sql(
+        request_table_name=request_table_name,
+        point_in_time_column=SpecialColumnName.POINT_IN_TIME,
+        request_table_columns=request_table_columns,
+        prior_cte_statements=ctes,
+        exclude_columns={SpecialColumnName.POINT_IN_TIME},
+    )
+
+    return OnlineStoreRetrievalTemplate(
+        sql_template=SqlExpressionTemplate(output_expr, source_type),
+        aggregation_result_names=plan.tile_based_aggregation_result_names,
+    )
+
+
+async def get_online_features(
+    session: BaseSession,
+    graph: QueryGraph,
+    nodes: list[Node],
+    request_data: Union[pd.DataFrame, BatchRequestTableModel],
+    source_type: SourceType,
+    online_store_table_version_service: OnlineStoreTableVersionService,
+    parent_serving_preparation: Optional[ParentServingPreparation] = None,
+    output_table_details: Optional[TableDetails] = None,
+) -> Optional[List[Dict[str, Any]]]:
+    """
+    Get online features
 
-    @staticmethod
-    async def update_migration_version(session: BaseSession, version: int) -> None:
-        """
-        Update MIGRATION_VERSION in warehouse metadata
+    Parameters
+    ----------
+    session: BaseSession
+        Session to use for executing the query
+    graph: QueryGraph
+        Query graph
+    nodes: list[Node]
+        List of query graph nodes
+    request_data: Union[pd.DataFrame, BatchRequestTableModel]
+        Request data as a dataframe or a BatchRequestTableModel
+    source_type: SourceType
+        Source type information
+    online_store_table_version_service: OnlineStoreTableVersionService
+        Online store table version service
+    parent_serving_preparation: Optional[ParentServingPreparation]
+        Preparation required for serving parent features
+    output_table_details: Optional[TableDetails]
+        Optional output table details to write the results to. If this parameter is provided, the
+        function will return None (intended to be used when handling asynchronous batch online feature requests).
+
+    Returns
+    -------
+    Optional[List[Dict[str, Any]]]
+    """
+    tic = time.time()
 
-        Parameters
-        ----------
-        session: BaseSession
-            BaseSession object to interact with data warehouse
-        version: int
-            Current migration version number
-        """
-        df_metadata = await session.execute_query("SELECT * FROM METADATA_SCHEMA")
-        if InternalName.MIGRATION_VERSION not in df_metadata:  # type: ignore[operator]
-            await session.execute_query(
-                f"ALTER TABLE METADATA_SCHEMA ADD COLUMN {InternalName.MIGRATION_VERSION} INT"
-            )
-        await session.execute_query(
-            f"UPDATE METADATA_SCHEMA SET {InternalName.MIGRATION_VERSION} = {version}"
+    if isinstance(request_data, pd.DataFrame):
+        request_table_expr = construct_dataframe_sql_expr(request_data, date_cols=[])
+        request_table_columns = request_data.columns.tolist()
+    else:
+        request_table_expr = expressions.select("*").from_(
+            get_fully_qualified_table_name(request_data.location.table_details.dict())
         )
+        request_table_columns = [col.name for col in request_data.columns_info]
 
-    async def migrate_record_with_session(
-        self, feature_store: FeatureStoreModel, session: BaseSession
-    ) -> None:
-        """
-        Migrate a FeatureStore document with an associated session object
-
-        Parameters
-        ----------
-        feature_store: FeatureStoreModel
-            Feature store whose data warehouse is to be migrated
-        session: BaseSession
-            Session object to interact with data warehouse
-        """
+    retrieval_template = get_online_store_retrieval_template(
+        graph,
+        nodes,
+        source_type=source_type,
+        request_table_columns=request_table_columns,
+        request_table_expr=request_table_expr,
+        parent_serving_preparation=parent_serving_preparation,
+    )
+    versions = await online_store_table_version_service.get_versions(
+        retrieval_template.aggregation_result_names
+    )
+    retrieval_expr = retrieval_template.fill_version_placeholders(versions)
+    logger.debug(f"OnlineServingService sql prep elapsed: {time.time() - tic:.6f}s")
+
+    tic = time.time()
+    if output_table_details is None:
+        retrieval_sql = sql_to_string(retrieval_expr, source_type=source_type)
+        df_features = await session.execute_query(retrieval_sql)
+        assert df_features is not None
+
+        features = []
+        prepare_dataframe_for_json(df_features)
+        for _, row in df_features.iterrows():
+            features.append(row.to_dict())
+        logger.debug(f"OnlineServingService sql execution elapsed: {time.time() - tic:.6f}s")
+        return features
+
+    # write the request to the output table
+    expression = get_sql_adapter(session.source_type).create_table_as(
+        table_details=output_table_details, select_expr=retrieval_expr
+    )
+    query = sql_to_string(expression, source_type=session.source_type)
+    await session.execute_query_long_running(query)
+    logger.debug(f"OnlineServingService sql execution elapsed: {time.time() - tic:.6f}s")
+    return None
```

### Comparing `featurebyte-0.3.1/featurebyte/models/__init__.py` & `featurebyte-0.4.0/featurebyte/models/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 """
 Document models for serialization to persistent storage
 """
 from featurebyte.models.dimension_table import DimensionTableModel
 from featurebyte.models.entity import EntityModel
 from featurebyte.models.event_table import EventTableModel
-from featurebyte.models.feature import FeatureModel, FeatureNamespaceModel
+from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureListModel
+from featurebyte.models.feature_namespace import FeatureNamespaceModel
 from featurebyte.models.feature_store import FeatureStoreModel
 
 all_models = [
     "DimensionTableModel",
     "EntityModel",
     "EventTableModel",
     "FeatureListModel",
```

### Comparing `featurebyte-0.3.1/featurebyte/models/base.py` & `featurebyte-0.4.0/featurebyte/models/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,49 +2,51 @@
 FeatureByte specific BaseModel
 """
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union
 
 import json
+import re
 from datetime import datetime
 
 from bson.errors import InvalidId
 from bson.objectid import ObjectId
 from pydantic import BaseModel, Field, StrictStr, root_validator, validator
 from pydantic.errors import DictError, PydanticTypeError
 from pymongo.operations import IndexModel
 
 from featurebyte.enum import StrEnum
 
 Model = TypeVar("Model", bound="FeatureByteBaseModel")
 
-DEFAULT_CATALOG_ID = ObjectId("63eda344d0313fb925f7883a")
-ACTIVE_CATALOG_ID: ObjectId = DEFAULT_CATALOG_ID
+DEFAULT_CATALOG_ID = ObjectId("23eda344d0313fb925f7883a")
+ACTIVE_CATALOG_ID: Optional[ObjectId] = None
+CAMEL_CASE_TO_SNAKE_CASE_PATTERN = re.compile("((?!^)(?<!_)[A-Z][a-z]+|(?<=[a-z0-9])[A-Z])")
 
 
-def get_active_catalog_id() -> ObjectId:
+def get_active_catalog_id() -> Optional[ObjectId]:
     """
     Get active catalog id
 
     Returns
     -------
-    ObjectId
+    Optional[ObjectId]
     """
     return ACTIVE_CATALOG_ID
 
 
-def activate_catalog(catalog_id: ObjectId) -> None:
+def activate_catalog(catalog_id: Optional[ObjectId]) -> None:
     """
     Set active catalog
 
     Parameters
     ----------
-    catalog_id: ObjectId
-        Catalog ID to set as active
+    catalog_id: Optional[ObjectId]
+        Catalog ID to set as active, or None to set no active catalog
     """
     global ACTIVE_CATALOG_ID  # pylint: disable=global-statement
     ACTIVE_CATALOG_ID = catalog_id
 
 
 class PydanticObjectId(ObjectId):
     """
@@ -212,28 +214,50 @@
 
     fields: List[str]
     conflict_fields_signature: Dict[str, Any]
     resolution_signature: Optional[UniqueConstraintResolutionSignature]
     extra_query_params: Optional[Dict[str, Any]] = Field(default=None)
 
 
+class ReferenceInfo(FeatureByteBaseModel):
+    """
+    Reference information for a document
+    """
+
+    asset_name: str
+    document_id: PydanticObjectId
+
+    @property
+    def collection_name(self) -> str:
+        """
+        Collection name of the reference document
+
+        Returns
+        -------
+        str
+        """
+        return CAMEL_CASE_TO_SNAKE_CASE_PATTERN.sub(r"_\1", self.asset_name).lower()
+
+
 class FeatureByteBaseDocumentModel(FeatureByteBaseModel):
     """
     FeatureByte specific BaseDocumentModel
 
     id: PydanticObjectId
         Identity value of the child class document model object
     user_id: PydanticObjectId
         Identity value of the user created this document
     name: Optional[StrictStr]
         Name of the child class document model object (value is None when name has not been set)
     created_at: Optional[datetime]
         Record creation datetime when the document get stored at the persistent
     updated_at: Optional[datetime]
         Record update datetime when the document get updated at the persistent
+    block_modifications_by: List[ReferenceInfo]
+        List of reference information that blocks modifications to the document
     """
 
     id: PydanticObjectId = Field(
         default_factory=ObjectId, alias="_id", allow_mutation=False, description="Record identifier"
     )
     user_id: Optional[PydanticObjectId] = Field(
         default=None, allow_mutation=False, description="User identifier"
@@ -241,14 +265,20 @@
     name: Optional[StrictStr] = Field(description="Record name")
     created_at: Optional[datetime] = Field(
         default=None, allow_mutation=False, description="Record creation time"
     )
     updated_at: Optional[datetime] = Field(
         default=None, allow_mutation=False, description="Record last updated time"
     )
+    block_modification_by: List[ReferenceInfo] = Field(
+        default_factory=list,
+        allow_mutation=False,
+        description="List of reference information that blocks modifications to the document",
+    )
+    description: Optional[StrictStr] = Field(default=None, description="Record description")
 
     @validator("id", pre=True)
     @classmethod
     def validate_id(cls, value: Any) -> Any:
         """
         Base document model id field validator. If the value of the ID is None, generate a valid ID.
 
@@ -300,14 +330,16 @@
         indexes: List[Union[IndexModel, List[Tuple[str, str]]]] = [
             IndexModel("user_id"),
             IndexModel("name"),
             IndexModel("created_at"),
             IndexModel("updated_at"),
         ]
 
+        auditable: bool = True
+
 
 class VersionIdentifier(BaseModel):
     """
     VersionIdentifier model
 
     name: str
         Version name like `V220917`
@@ -359,15 +391,15 @@
     catalog_id: PydanticObjectId
 
     @root_validator(pre=True)
     @classmethod
     def _validate_catalog_id(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         catalog_id = values.get("catalog_id")
         if catalog_id is None:
-            values["catalog_id"] = get_active_catalog_id()
+            values["catalog_id"] = DEFAULT_CATALOG_ID
         return values
 
     class Settings(FeatureByteBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/models/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/models/batch_feature_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/models/batch_request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/catalog.py` & `featurebyte-0.4.0/featurebyte/models/catalog.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/context.py` & `featurebyte-0.4.0/featurebyte/models/context.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 
     entity_ids: List[PydanticObjectId]
         List of entity ids associated with this context
     graph: Optional[QueryGraph]
         Graph to store the context view
     """
 
+    # TODO: make graph attribute lazy
+
     entity_ids: List[PydanticObjectId]
     graph: Optional[QueryGraph]
     node_name: Optional[str]
 
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB Settings
```

### Comparing `featurebyte-0.3.1/featurebyte/models/credential.py` & `featurebyte-0.4.0/featurebyte/models/credential.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 Document model for stored credentials
 """
 from typing import Dict, List, Literal, Optional, Union
 from typing_extensions import Annotated
 
+import base64  # pylint: disable=wrong-import-order
 import os  # pylint: disable=wrong-import-order
 
 import pymongo
 from cryptography.fernet import Fernet
 from pydantic import Field, StrictStr
 
 from featurebyte.common.doc_util import FBAutoDoc
@@ -107,14 +108,15 @@
 class DatabaseCredentialType(StrEnum):
     """
     Credential Type
     """
 
     USERNAME_PASSWORD = "USERNAME_PASSWORD"
     ACCESS_TOKEN = "ACCESS_TOKEN"
+    KERBEROS_KEYTAB = "KERBEROS_KEYTAB"
 
 
 class BaseDatabaseCredential(BaseCredential):
     """
     Storage credential only
     """
 
@@ -155,16 +157,69 @@
 
     type: Literal[DatabaseCredentialType.ACCESS_TOKEN] = Field(
         DatabaseCredentialType.ACCESS_TOKEN, const=True
     )
     access_token: StrictStr = Field(description="The access token used to connect.")
 
 
+class KerberosKeytabCredential(BaseDatabaseCredential):
+    """
+    Data class for a kerberos key tab credential.
+
+    Examples
+    --------
+    >>> kerberos_key_tab_credential = KerberosKeytabCredential.from_file(
+    ... keytab="/path/to/keytab", principal="user@FEATUREBYTE.COM"
+    ... )  # doctest: +SKIP
+    """
+
+    __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.KerberosKeytabCredential")
+
+    type: Literal[DatabaseCredentialType.KERBEROS_KEYTAB] = Field(
+        DatabaseCredentialType.KERBEROS_KEYTAB, const=True
+    )
+    principal: StrictStr = Field(description="The principal used to connect.")
+    encoded_key_tab: StrictStr = Field(description="The key tab used to connect.")
+
+    @property
+    def keytab(self) -> bytes:
+        """
+        Returns the keytab
+
+        Returns
+        -------
+        bytes
+            The key tab
+        """
+        return base64.b64decode(self.encoded_key_tab)
+
+    @classmethod
+    def from_file(cls, keytab_filepath: str, principal: str) -> "KerberosKeytabCredential":
+        """
+        Create a KerberosKeytabCredential from a keytab file.
+
+        Parameters
+        ----------
+        keytab_filepath: str
+            Path to the keytab file.
+        principal: str
+            Principal to use with the keytab.
+
+        Returns
+        -------
+        KerberosKeytabCredential
+        """
+        return KerberosKeytabCredential(
+            principal=principal,
+            encoded_key_tab=base64.b64encode(open(keytab_filepath, "rb").read()).decode("utf-8"),
+        )
+
+
 DatabaseCredential = Annotated[
-    Union[UsernamePasswordCredential, AccessTokenCredential],
+    Union[UsernamePasswordCredential, AccessTokenCredential, KerberosKeytabCredential],
     Field(discriminator="type"),
 ]
 
 
 # Storage Credentials
 class StorageCredentialType(StrEnum):
     """
```

### Comparing `featurebyte-0.3.1/featurebyte/models/deployment.py` & `featurebyte-0.4.0/featurebyte/models/deployment.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/dimension_table.py` & `featurebyte-0.4.0/featurebyte/models/dimension_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/entity.py` & `featurebyte-0.4.0/featurebyte/models/entity.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/entity_validation.py` & `featurebyte-0.4.0/featurebyte/models/entity_validation.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/event_table.py` & `featurebyte-0.4.0/featurebyte/models/event_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/feature.py` & `featurebyte-0.4.0/featurebyte/models/feature.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,238 +3,96 @@
 """
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 import pymongo
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr, root_validator, validator
+from pydantic import Field, PrivateAttr, root_validator, validator
 
-from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.validator import construct_sort_validator, version_validator
-from featurebyte.enum import DBVarType, OrderedStrEnum, StrEnum
+from featurebyte.common.validator import version_validator
+from featurebyte.enum import DBVarType
 from featurebyte.models.base import (
-    FeatureByteBaseModel,
     FeatureByteCatalogBaseDocumentModel,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
     VersionIdentifier,
 )
+from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.operation import GroupOperationStructure
+from featurebyte.query_graph.sql.interpreter import GraphInterpreter
+from featurebyte.query_graph.sql.online_store_compute_query import (
+    get_online_store_precompute_queries,
+)
 
 
-class FeatureReadiness(OrderedStrEnum):
-    """Feature readiness"""
-
-    DEPRECATED = "DEPRECATED"
-    DRAFT = "DRAFT"
-    PUBLIC_DRAFT = "PUBLIC_DRAFT"
-    PRODUCTION_READY = "PRODUCTION_READY"
-
-
-class DefaultVersionMode(StrEnum):
-    """
-    Default feature setting mode.
-    """
-
-    __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.DefaultVersionMode")
-
-    AUTO = "AUTO", "Automatically select the version to use."
-    MANUAL = "MANUAL", "Manually select the version to use."
-
-
-class FrozenFeatureNamespaceModel(FeatureByteCatalogBaseDocumentModel):
-    """
-    FrozenFeatureNamespaceModel store all the attributes that are fixed after object construction.
-    """
-
-    dtype: DBVarType = Field(
-        allow_mutation=False, description="database variable type for the feature"
-    )
-    entity_ids: List[PydanticObjectId] = Field(allow_mutation=False)
-    table_ids: List[PydanticObjectId] = Field(allow_mutation=False)
-
-    # pydantic validators
-    _sort_ids_validator = validator("entity_ids", "table_ids", allow_reuse=True)(
-        construct_sort_validator()
-    )
-
-    class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
-        """
-        MongoDB settings
-        """
-
-        collection_name: str = "feature_namespace"
-        unique_constraints: List[UniqueValuesConstraint] = [
-            UniqueValuesConstraint(
-                fields=("_id",),
-                conflict_fields_signature={"id": ["_id"]},
-                resolution_signature=None,
-            ),
-            UniqueValuesConstraint(
-                fields=("name",),
-                conflict_fields_signature={"name": ["name"]},
-                resolution_signature=UniqueConstraintResolutionSignature.RENAME,
-            ),
-        ]
-
-        indexes = FeatureByteCatalogBaseDocumentModel.Settings.indexes + [
-            pymongo.operations.IndexModel("dtype"),
-            pymongo.operations.IndexModel("entity_ids"),
-            pymongo.operations.IndexModel("table_ids"),
-        ]
-
-
-class FeatureNamespaceModel(FrozenFeatureNamespaceModel):
-    """
-    Feature set with the same feature name
-
-    id: PydanticObjectId
-        Feature namespace id
-    name: str
-        Feature name
-    dtype: DBVarType
-        Variable type of the feature
-    feature_ids: List[PydanticObjectId]
-        List of feature version id
-    online_enabled_feature_ids: List[PydanticObjectId]
-        List of online enabled feature version id
-    readiness: FeatureReadiness
-        Aggregated readiness across all feature versions of the same feature namespace
-    created_at: datetime
-        Datetime when the FeatureNamespace was first saved or published
-    default_feature_id: PydanticObjectId
-        Default feature version id
-    default_version_mode: DefaultVersionMode
-        Default feature version mode
-    entity_ids: List[PydanticObjectId]
-        Entity IDs used by the feature
-    table_ids: List[PydanticObjectId]
-        Table IDs used by the feature
-    """
-
-    feature_ids: List[PydanticObjectId] = Field(allow_mutation=False)
-    online_enabled_feature_ids: List[PydanticObjectId] = Field(
-        allow_mutation=False, default_factory=list
-    )
-    readiness: FeatureReadiness = Field(allow_mutation=False)
-    default_feature_id: PydanticObjectId = Field(allow_mutation=False)
-    default_version_mode: DefaultVersionMode = Field(
-        default=DefaultVersionMode.AUTO, allow_mutation=False
-    )
-
-    # pydantic validators
-    _sort_feature_ids_validator = validator("feature_ids", allow_reuse=True)(
-        construct_sort_validator()
-    )
-
-    class Settings(FrozenFeatureNamespaceModel.Settings):
-        """
-        MongoDB settings
-        """
-
-        indexes = FrozenFeatureNamespaceModel.Settings.indexes + [
-            pymongo.operations.IndexModel("feature_ids"),
-            pymongo.operations.IndexModel("online_enabled_feature_ids"),
-            pymongo.operations.IndexModel("readiness"),
-            pymongo.operations.IndexModel("default_feature_id"),
-            pymongo.operations.IndexModel(
-                [
-                    ("name", pymongo.TEXT),
-                ],
-            ),
-        ]
-
-
-class FeatureModel(FeatureByteCatalogBaseDocumentModel):
+class BaseFeatureModel(FeatureByteCatalogBaseDocumentModel):
     """
-    Model for Feature asset
-
-    id: PydanticObjectId
-        Feature id of the object
-    name: str
-        Feature name
-    dtype: DBVarType
-        Variable type of the feature
-    graph: QueryGraph
-        Graph contains steps of transformation to generate the feature
-    node_name: str
-        Node name of the graph which represent the feature
-    tabular_source: TabularSource
-        Tabular source used to construct this feature
-    readiness: FeatureReadiness
-        Feature readiness
-    version: VersionIdentifier
-        Feature version
-    online_enabled: bool
-        Whether to make this feature version online enabled
-    definition: str
-        Feature definition
-    entity_ids: List[PydanticObjectId]
-        Entity IDs used by the feature
-    table_ids: List[PydanticObjectId]
-        Table IDs used by the feature
-    primary_table_ids: Optional[List[PydanticObjectId]]
-        Primary table IDs of the feature (auto-derive from graph)
-    feature_namespace_id: PydanticObjectId
-        Feature namespace id of the object
-    feature_list_ids: List[PydanticObjectId]
-        FeatureList versions which use this feature version
-    deployed_feature_list_ids: List[PydanticObjectId]
-        Deployed FeatureList versions which use this feature version
-    created_at: Optional[datetime]
-        Datetime when the Feature was first saved
-    updated_at: Optional[datetime]
-        When the Feature get updated
+    BaseFeatureModel is the base class for FeatureModel & TargetModel.
+    It contains all the attributes that are shared between FeatureModel & TargetModel.
     """
 
     dtype: DBVarType = Field(allow_mutation=False, default=DBVarType.UNKNOWN)
-    graph: QueryGraph = Field(allow_mutation=False)
     node_name: str
     tabular_source: TabularSource = Field(allow_mutation=False)
-    readiness: FeatureReadiness = Field(allow_mutation=False, default=FeatureReadiness.DRAFT)
     version: VersionIdentifier = Field(allow_mutation=False, default=None)
-    online_enabled: bool = Field(allow_mutation=False, default=False)
     definition: Optional[str] = Field(allow_mutation=False, default=None)
 
-    # list of IDs attached to this feature
+    # special handling for those attributes that are expensive to deserialize
+    # internal_* is used to store the raw data from persistence, _* is used as a cache
+    internal_graph: Any = Field(allow_mutation=False, alias="graph")
+    _graph: Optional[QueryGraph] = PrivateAttr(default=None)
+
+    # list of IDs attached to this feature or target
     entity_ids: List[PydanticObjectId] = Field(allow_mutation=False, default_factory=list)
     table_ids: List[PydanticObjectId] = Field(allow_mutation=False, default_factory=list)
     primary_table_ids: List[PydanticObjectId] = Field(allow_mutation=False, default_factory=list)
-    feature_namespace_id: PydanticObjectId = Field(allow_mutation=False, default_factory=ObjectId)
-    feature_list_ids: List[PydanticObjectId] = Field(allow_mutation=False, default_factory=list)
-    deployed_feature_list_ids: List[PydanticObjectId] = Field(
+    user_defined_function_ids: List[PydanticObjectId] = Field(
         allow_mutation=False, default_factory=list
     )
 
     # pydantic validators
     _version_validator = validator("version", pre=True, allow_reuse=True)(version_validator)
 
     @root_validator
     @classmethod
     def _add_derived_attributes(cls, values: dict[str, Any]) -> dict[str, Any]:
-        # extract table ids & entity ids from the graph
-        graph = values["graph"]
-        node_name = values["node_name"]
-        values["primary_table_ids"] = graph.get_primary_table_ids(node_name=node_name)
-        values["table_ids"] = graph.get_table_ids(node_name=node_name)
-        values["entity_ids"] = graph.get_entity_ids(node_name=node_name)
-
-        # extract dtype from the graph
-        node = graph.get_node_by_name(node_name)
-        op_struct = graph.extract_operation_structure(node=node)
-        if len(op_struct.aggregations) != 1:
-            raise ValueError("Feature graph must have exactly one aggregation output")
+        # do not check entity_ids as the derived result can be an empty list
+        derived_attributes = [
+            values.get("primary_table_ids"),
+            values.get("table_ids"),
+            values.get("dtype"),
+        ]
+        if any(not x for x in derived_attributes):
+            # only derive attributes if any of them is missing
+            # extract table ids & entity ids from the graph
+            graph_dict = values["internal_graph"]
+            if isinstance(graph_dict, QueryGraphModel):
+                graph_dict = graph_dict.dict(by_alias=True)
+            graph = QueryGraph(**graph_dict)
+            node_name = values["node_name"]
+            values["primary_table_ids"] = graph.get_primary_table_ids(node_name=node_name)
+            values["table_ids"] = graph.get_table_ids(node_name=node_name)
+            values["entity_ids"] = graph.get_entity_ids(node_name=node_name)
+            values["user_defined_function_ids"] = graph.get_user_defined_function_ids(
+                node_name=node_name
+            )
+
+            # extract dtype from the graph
+            node = graph.get_node_by_name(node_name)
+            op_struct = graph.extract_operation_structure(node=node, keep_all_source_columns=True)
+            if len(op_struct.aggregations) != 1:
+                raise ValueError("Feature or target graph must have exactly one aggregation output")
 
-        values["dtype"] = op_struct.aggregations[0].dtype
+            values["dtype"] = op_struct.aggregations[0].dtype
         return values
 
     @property
     def node(self) -> Node:
         """
         Retrieve node
 
@@ -242,14 +100,37 @@
         -------
         Node
             Node object
         """
 
         return self.graph.get_node_by_name(self.node_name)
 
+    @property
+    def graph(self) -> QueryGraph:
+        """
+        Get the graph. If the graph is not loaded, load it first.
+
+        Returns
+        -------
+        QueryGraph
+            QueryGraph object
+        """
+        # TODO: make this a cached_property for pydantic v2
+        if self._graph is None:
+            if isinstance(self.internal_graph, QueryGraph):
+                self._graph = self.internal_graph
+            else:
+                if isinstance(self.internal_graph, dict):
+                    graph_dict = self.internal_graph
+                else:
+                    # for example, QueryGraphModel
+                    graph_dict = self.internal_graph.dict(by_alias=True)
+                self._graph = QueryGraph(**graph_dict)
+        return self._graph
+
     def extract_pruned_graph_and_node(self, **kwargs: Any) -> tuple[QueryGraphModel, Node]:
         """
         Extract pruned graph and node
 
         Parameters
         ----------
         **kwargs: Any
@@ -257,36 +138,38 @@
 
         Returns
         -------
         tuple[QueryGraph, Node]
             Pruned graph and node
         """
         _ = kwargs
-        pruned_graph, node_name_map = self.graph.prune(target_node=self.node, aggressive=True)
+        pruned_graph, node_name_map = self.graph.prune(target_node=self.node)
         mapped_node = pruned_graph.get_node_by_name(node_name_map[self.node.name])
         return pruned_graph, mapped_node
 
     def extract_operation_structure(self) -> GroupOperationStructure:
         """
-        Extract feature operation structure based on query graph.
+        Extract feature or target operation structure based on query graph. This method is mainly
+        used for deriving feature or target metadata used in feature/target info.
 
         Returns
         -------
         GroupOperationStructure
         """
         # group the view columns by source columns & derived columns
-        operation_structure = self.graph.extract_operation_structure(self.node)
+        operation_structure = self.graph.extract_operation_structure(
+            self.node, keep_all_source_columns=False
+        )
         return operation_structure.to_group_operation_structure()
 
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
-        collection_name: str = "feature"
         unique_constraints = [
             UniqueValuesConstraint(
                 fields=("_id",),
                 conflict_fields_signature={"id": ["_id"]},
                 resolution_signature=UniqueConstraintResolutionSignature.GET_BY_ID,
             ),
             UniqueValuesConstraint(
@@ -294,39 +177,123 @@
                 conflict_fields_signature={"name": ["name"], "version": ["version"]},
                 resolution_signature=UniqueConstraintResolutionSignature.GET_BY_ID,
             ),
         ]
         indexes = FeatureByteCatalogBaseDocumentModel.Settings.indexes + [
             pymongo.operations.IndexModel("dtype"),
             pymongo.operations.IndexModel("version"),
-            pymongo.operations.IndexModel("readiness"),
-            pymongo.operations.IndexModel("online_enabled"),
             pymongo.operations.IndexModel("entity_ids"),
             pymongo.operations.IndexModel("table_ids"),
             pymongo.operations.IndexModel("primary_table_ids"),
-            pymongo.operations.IndexModel("feature_namespace_id"),
-            pymongo.operations.IndexModel("feature_list_ids"),
-            pymongo.operations.IndexModel("deployed_feature_list_ids"),
+            pymongo.operations.IndexModel("user_defined_function_ids"),
             pymongo.operations.IndexModel(
                 [
                     ("name", pymongo.TEXT),
                     ("version", pymongo.TEXT),
                 ],
             ),
         ]
 
 
-class FeatureSignature(FeatureByteBaseModel):
+class FeatureModel(BaseFeatureModel):
     """
-    FeatureSignature class used in FeatureList object
+    Model for Feature asset
 
     id: PydanticObjectId
         Feature id of the object
     name: str
-        Name of the feature
+        Feature name
+    dtype: DBVarType
+        Variable type of the feature
+    graph: QueryGraph
+        Graph contains steps of transformation to generate the feature
+    node_name: str
+        Node name of the graph which represent the feature
+    tabular_source: TabularSource
+        Tabular source used to construct this feature
+    readiness: FeatureReadiness
+        Feature readiness
     version: VersionIdentifier
         Feature version
+    online_enabled: bool
+        Whether to make this feature version online enabled
+    definition: str
+        Feature definition
+    entity_ids: List[PydanticObjectId]
+        Entity IDs used by the feature
+    table_ids: List[PydanticObjectId]
+        Table IDs used by the feature
+    primary_table_ids: Optional[List[PydanticObjectId]]
+        Primary table IDs of the feature (auto-derive from graph)
+    feature_namespace_id: PydanticObjectId
+        Feature namespace id of the object
+    feature_list_ids: List[PydanticObjectId]
+        FeatureList versions which use this feature version
+    deployed_feature_list_ids: List[PydanticObjectId]
+        Deployed FeatureList versions which use this feature version
+    created_at: Optional[datetime]
+        Datetime when the Feature was first saved
+    updated_at: Optional[datetime]
+        When the Feature get updated
     """
 
-    id: PydanticObjectId
-    name: Optional[StrictStr]
-    version: VersionIdentifier
+    readiness: FeatureReadiness = Field(allow_mutation=False, default=FeatureReadiness.DRAFT)
+    online_enabled: bool = Field(allow_mutation=False, default=False)
+
+    # ID related fields associated with this feature
+    feature_namespace_id: PydanticObjectId = Field(allow_mutation=False, default_factory=ObjectId)
+    feature_list_ids: List[PydanticObjectId] = Field(allow_mutation=False, default_factory=list)
+    deployed_feature_list_ids: List[PydanticObjectId] = Field(
+        allow_mutation=False, default_factory=list
+    )
+    aggregation_ids: List[str] = Field(allow_mutation=False, default_factory=list)
+    aggregation_result_names: List[str] = Field(allow_mutation=False, default_factory=list)
+
+    @root_validator
+    @classmethod
+    def _add_tile_derived_attributes(cls, values: dict[str, Any]) -> dict[str, Any]:
+        # Each aggregation_id refers to a set of columns in a tile table. It is associated to a
+        # specific scheduled tile task. An aggregation_id can produce multiple aggregation results
+        # using different feature derivation windows.
+        if values.get("aggregation_ids") and values.get("aggregation_result_names"):
+            return values
+
+        graph_dict = values["internal_graph"]
+        if isinstance(graph_dict, QueryGraphModel):
+            graph_dict = graph_dict.dict(by_alias=True)
+        graph = QueryGraph(**graph_dict)
+        node_name = values["node_name"]
+        feature_store_type = graph.get_input_node(node_name).parameters.feature_store_details.type
+
+        interpreter = GraphInterpreter(graph, feature_store_type)
+        node = graph.get_node_by_name(node_name)
+        tile_infos = interpreter.construct_tile_gen_sql(node, is_on_demand=False)
+
+        aggregation_ids = []
+        for info in tile_infos:
+            aggregation_ids.append(info.aggregation_id)
+
+        values["aggregation_ids"] = aggregation_ids
+
+        values["aggregation_result_names"] = [
+            query.result_name
+            for query in get_online_store_precompute_queries(
+                graph, graph.get_node_by_name(node_name), feature_store_type
+            )
+        ]
+        return values
+
+    class Settings(BaseFeatureModel.Settings):
+        """
+        MongoDB settings
+        """
+
+        collection_name: str = "feature"
+        indexes = BaseFeatureModel.Settings.indexes + [
+            pymongo.operations.IndexModel("readiness"),
+            pymongo.operations.IndexModel("online_enabled"),
+            pymongo.operations.IndexModel("feature_namespace_id"),
+            pymongo.operations.IndexModel("feature_list_ids"),
+            pymongo.operations.IndexModel("deployed_feature_list_ids"),
+            pymongo.operations.IndexModel("aggregation_ids"),
+            pymongo.operations.IndexModel("aggregation_result_names"),
+        ]
```

### Comparing `featurebyte-0.3.1/featurebyte/models/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/models/feature_job_setting_analysis.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/feature_list.py` & `featurebyte-0.4.0/featurebyte/models/feature_list.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,29 +6,30 @@
 from typing import Any, List, Optional
 
 import functools
 from collections import defaultdict
 
 import pymongo
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr, root_validator, validator
+from pydantic import Field, PrivateAttr, StrictStr, root_validator, validator
 from typeguard import typechecked
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import construct_sort_validator, version_validator
 from featurebyte.enum import DBVarType, OrderedStrEnum
 from featurebyte.models.base import (
     FeatureByteBaseModel,
     FeatureByteCatalogBaseDocumentModel,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
     VersionIdentifier,
 )
-from featurebyte.models.feature import DefaultVersionMode, FeatureModel, FeatureReadiness
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
 from featurebyte.models.relationship import RelationshipType
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.pruning_util import get_prune_graph_and_nodes
 
 
 class FeatureListStatus(OrderedStrEnum):
@@ -460,41 +461,47 @@
         Feature list version
     deployed: bool
         Whether to deploy this feature list version
     feature_list_namespace_id: PydanticObjectId
         Feature list namespace id of the object
     created_at: Optional[datetime]
         Datetime when the FeatureList was first saved or published
-    feature_clusters: List[FeatureCluster]
+    internal_feature_clusters: Optional[List[Any]]
         List of combined graphs for features from the same feature store
     """
 
     version: VersionIdentifier = Field(allow_mutation=False, description="Feature list version")
-    feature_clusters: Optional[List[FeatureCluster]] = Field(allow_mutation=False)  # DEV-556
     relationships_info: Optional[List[EntityRelationshipInfo]] = Field(
         allow_mutation=False, default=None  # DEV-556
     )
     readiness_distribution: FeatureReadinessDistribution = Field(
         allow_mutation=False, default_factory=list
     )
     deployed: bool = Field(allow_mutation=False, default=False)
 
+    # special handling for those attributes that are expensive to deserialize
+    # internal_* is used to store the raw data from persistence, _* is used as a cache
+    internal_feature_clusters: Optional[List[Any]] = Field(
+        allow_mutation=False, alias="feature_clusters"
+    )
+    _feature_clusters: Optional[List[FeatureCluster]] = PrivateAttr(default=None)
+
     # list of IDs attached to this feature list
     feature_ids: List[PydanticObjectId]
     feature_list_namespace_id: PydanticObjectId = Field(
         allow_mutation=False, default_factory=ObjectId
     )
     online_enabled_feature_ids: List[PydanticObjectId] = Field(
         allow_mutation=False, default_factory=list
     )
 
     # pydantic validators
-    _sort_feature_ids_validator = validator(
-        "feature_ids", "online_enabled_feature_ids", allow_reuse=True
-    )(construct_sort_validator())
+    _sort_feature_ids_validator = validator("online_enabled_feature_ids", allow_reuse=True)(
+        construct_sort_validator()
+    )
     _version_validator = validator("version", pre=True, allow_reuse=True)(version_validator)
 
     @root_validator(pre=True)
     @classmethod
     def _derive_feature_related_attributes(cls, values: dict[str, Any]) -> dict[str, Any]:
         # "features" is not an attribute to the FeatureList model, when it appears in the input to
         # constructor, it is intended to be used to derive other feature-related attributes
@@ -563,14 +570,32 @@
                     feature_store_id=feature_store_id,
                     graph=pruned_graph,
                     node_names=[node.name for node in mapped_nodes],
                 )
             )
         return feature_clusters
 
+    @property
+    def feature_clusters(self) -> Optional[List[FeatureCluster]]:
+        """
+        List of combined graphs for features from the same feature store
+
+        Returns
+        -------
+        Optional[List[FeatureCluster]]
+        """
+        # TODO: make this a cached_property for pydantic v2
+        if self.internal_feature_clusters is None:
+            return None
+        if self._feature_clusters is None:
+            self._feature_clusters = [
+                FeatureCluster(**cluster) for cluster in self.internal_feature_clusters
+            ]
+        return self._feature_clusters
+
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "feature_list"
         unique_constraints: List[UniqueValuesConstraint] = [
```

### Comparing `featurebyte-0.3.1/featurebyte/models/feature_store.py` & `featurebyte-0.4.0/featurebyte/models/feature_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,15 +37,15 @@
         """
         Get feature store details
 
         Returns
         -------
         FeatureStoreDetails
         """
-        return FeatureStoreDetails(**self.json_dict())
+        return FeatureStoreDetails(**self.dict(by_alias=True))
 
     class Settings(FeatureByteBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "feature_store"
@@ -171,15 +171,15 @@
         """
         Table table
 
         Returns
         -------
         BaseTableData
         """
-        return self._table_data_class(**self.json_dict())
+        return self._table_data_class(**self.dict(by_alias=True))
 
     @property
     @abstractmethod
     def primary_key_columns(self) -> List[str]:
         """
         Primary key column names
```

### Comparing `featurebyte-0.3.1/featurebyte/models/historical_feature_table.py` & `featurebyte-0.4.0/featurebyte/models/historical_feature_table.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,22 @@
 """
 HistoricalFeatureTableModel
 """
 from __future__ import annotations
 
-from typing import Optional
-
 from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base_feature_or_target_table import BaseFeatureOrTargetTableModel
 from featurebyte.models.materialized_table import MaterializedTableModel
 
 
-class HistoricalFeatureTableModel(MaterializedTableModel):
+class HistoricalFeatureTableModel(BaseFeatureOrTargetTableModel):
     """
     HistoricalFeatureTable is the result of asynchronous historical features requests
     """
 
-    observation_table_id: Optional[PydanticObjectId]
     feature_list_id: PydanticObjectId
 
     class Settings(MaterializedTableModel.Settings):
         """
         MongoDB settings
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/models/item_table.py` & `featurebyte-0.4.0/featurebyte/models/item_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/materialized_table.py` & `featurebyte-0.4.0/featurebyte/models/materialized_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/observation_table.py` & `featurebyte-0.4.0/featurebyte/models/observation_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/online_store.py` & `featurebyte-0.4.0/featurebyte/models/online_store.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,21 +4,21 @@
 from typing import Any, Dict, List, cast
 
 from pydantic import validator
 
 from featurebyte.enum import TableDataType
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.online_store_compute_query import OnlineStoreComputeQueryModel
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
-from featurebyte.query_graph.sql.online_serving import (
-    OnlineStorePrecomputeQuery,
+from featurebyte.query_graph.sql.online_serving import is_online_store_eligible
+from featurebyte.query_graph.sql.online_store_compute_query import (
     get_online_store_precompute_queries,
-    is_online_store_eligible,
 )
 
 
 class OnlineFeatureSpec(FeatureByteBaseModel):
     """
     Model for Online Feature Store
 
@@ -31,22 +31,22 @@
     tile_ids: List[str]
         derived tile_ids from tile_specs
     entity_column_names: List[str]
         derived entity column names from tile_specs
     """
 
     feature: ExtendedFeatureModel
-    precompute_queries: List[OnlineStorePrecomputeQuery] = []
+    precompute_queries: List[OnlineStoreComputeQueryModel] = []
 
     @validator("precompute_queries", always=True)
     def _generate_precompute_queries(  # pylint: disable=no-self-argument
         cls,
-        val: List[OnlineStorePrecomputeQuery],
+        val: List[OnlineStoreComputeQueryModel],
         values: Dict[str, Any],
-    ) -> List[OnlineStorePrecomputeQuery]:
+    ) -> List[OnlineStoreComputeQueryModel]:
         if val:
             # Allow direct setting; mainly used in integration tests
             return val
         feature = values["feature"]
         precompute_queries = get_online_store_precompute_queries(
             graph=feature.graph,
             node=feature.node,
```

### Comparing `featurebyte-0.3.1/featurebyte/models/parent_serving.py` & `featurebyte-0.4.0/featurebyte/models/parent_serving.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/periodic_task.py` & `featurebyte-0.4.0/featurebyte/models/periodic_task.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/persistent.py` & `featurebyte-0.4.0/featurebyte/models/persistent.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/proxy_table.py` & `featurebyte-0.4.0/featurebyte/models/proxy_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/relationship.py` & `featurebyte-0.4.0/featurebyte/models/relationship.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/relationship_analysis.py` & `featurebyte-0.4.0/featurebyte/models/relationship_analysis.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/request_input.py` & `featurebyte-0.4.0/featurebyte/models/request_input.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """
 RequestInput is the base class for all request input types.
 """
 from __future__ import annotations
 
-from typing import Dict, List, Literal, Optional, cast
+from typing import Any, Dict, List, Literal, Optional, cast
 
 from abc import abstractmethod
 
-from pydantic import Field, StrictStr
+from pydantic import Field, PrivateAttr, StrictStr
 from sqlglot.expressions import Select
 
 from featurebyte.enum import SourceType, StrEnum
 from featurebyte.exception import ColumnNotFoundError
 from featurebyte.models.base import FeatureByteBaseModel
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.graph import QueryGraphModel
@@ -181,18 +181,39 @@
         The query graph that defines the view
     node_name: str
         The name of the node in the query graph that defines the view
     type: Literal[RequestInputType.VIEW]
         The type of the input. Must be VIEW for this class
     """
 
-    graph: QueryGraphModel
     node_name: StrictStr
     type: Literal[RequestInputType.VIEW] = Field(RequestInputType.VIEW, const=True)
 
+    # special handling for those attributes that are expensive to deserialize
+    # internal_* is used to store the raw data from persistence, _* is used as a cache
+    internal_graph: Any = Field(alias="graph")
+    _graph: Optional[QueryGraphModel] = PrivateAttr(default=None)
+
+    @property
+    def graph(self) -> QueryGraphModel:
+        """
+        Get the graph. If the graph is not loaded, load it first.
+
+        Returns
+        -------
+        QueryGraphModel
+        """
+        # TODO: make this a cached_property for pydantic v2
+        if self._graph is None:
+            if isinstance(self.internal_graph, dict):
+                self._graph = QueryGraphModel(**self.internal_graph)
+            else:
+                self._graph = self.internal_graph
+        return self._graph
+
     def get_query_expr(self, source_type: SourceType) -> Select:
         return get_view_expr(graph=self.graph, node_name=self.node_name, source_type=source_type)
 
     async def get_column_names(self, session: BaseSession) -> List[str]:
         node = self.graph.get_node_by_name(self.node_name)
         op_struct_info = OperationStructureExtractor(graph=self.graph).extract(node=node)
         op_struct = op_struct_info.operation_structure_map[node.name]
```

### Comparing `featurebyte-0.3.1/featurebyte/models/scd_table.py` & `featurebyte-0.4.0/featurebyte/models/scd_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/semantic.py` & `featurebyte-0.4.0/featurebyte/models/semantic.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/task.py` & `featurebyte-0.4.0/featurebyte/models/task.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/models/tile.py` & `featurebyte-0.4.0/featurebyte/models/tile.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 """
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
 from pydantic import Field, root_validator, validator
 
 from featurebyte.enum import StrEnum
-from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 
 
 class TileType(StrEnum):
     """Tile Type"""
 
     ONLINE = "ONLINE"
     OFFLINE = "OFFLINE"
@@ -36,27 +36,24 @@
     value_column_names: List[str]
         tile value column names for the tile table
     category_column_name: Optional[str]
         optional category column name when the groupby operation specifies a category
     """
 
     time_modulo_frequency_second: int = Field(ge=0)
-    blind_spot_second: int
+    blind_spot_second: int = Field(ge=0)
     frequency_minute: int = Field(gt=0)
     tile_sql: str
     entity_column_names: List[str]
     value_column_names: List[str]
     value_column_types: List[str]
     tile_id: str
     aggregation_id: str
     category_column_name: Optional[str]
-
     feature_store_id: Optional[ObjectId]
-    user_id: Optional[ObjectId]
-    catalog_id: Optional[ObjectId]
 
     class Config:
         """
         Config for pydantic model
         """
 
         arbitrary_types_allowed: bool = True
@@ -112,7 +109,41 @@
 
         if values["time_modulo_frequency_second"] > values["frequency_minute"] * 60:
             raise ValueError(
                 f"time_modulo_frequency_second must be less than {values['frequency_minute'] * 60}"
             )
 
         return values
+
+
+class TileCommonParameters(FeatureByteBaseModel):
+    """
+    Model for common parameters used by various steps within a tile scheduled job
+    """
+
+    feature_store_id: PydanticObjectId
+    tile_id: str
+    aggregation_id: str
+    time_modulo_frequency_second: int
+    blind_spot_second: int
+    frequency_minute: int
+
+    sql: str
+    entity_column_names: List[str]
+    value_column_names: List[str]
+    value_column_types: List[str]
+
+    class Config(FeatureByteBaseModel.Config):
+        """Model configuration"""
+
+        extra = "forbid"
+
+
+class TileScheduledJobParameters(TileCommonParameters):
+    """
+    Model for the parameters for a scheduled tile job
+    """
+
+    offline_period_minute: int
+    tile_type: str
+    monitor_periods: int
+    job_schedule_ts: Optional[str] = Field(default=None)
```

### Comparing `featurebyte-0.3.1/featurebyte/persistent/audit.py` & `featurebyte-0.4.0/featurebyte/persistent/audit.py`

 * *Files 2% similar despite different names*

```diff
@@ -275,14 +275,17 @@
                 Keyword arguments for the function
 
             Returns
             -------
             Any
                 Return value from execution of the function
             """
+            if kwargs.pop("disable_audit", False):
+                return await func(persistent, collection_name=collection_name, *args, **kwargs)
+
             async with persistent.start_transaction() as session:
                 return_value, num_updated, original_docs = await _execute_transaction(
                     persistent=session,
                     collection_name=collection_name,
                     async_execution=func(session, collection_name=collection_name, *args, **kwargs),
                     **kwargs,
                 )
```

### Comparing `featurebyte-0.3.1/featurebyte/persistent/base.py` & `featurebyte-0.4.0/featurebyte/persistent/base.py`

 * *Files 9% similar despite different names*

```diff
@@ -54,27 +54,30 @@
 
     @audit_transaction(mode=AuditTransactionMode.SINGLE, action_type=AuditActionType.INSERT)
     async def insert_one(
         self,
         collection_name: str,
         document: Document,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> ObjectId:
         """
         Insert record into collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject user_id and catalog_id into the document automatically.
 
         Parameters
         ----------
         collection_name: str
             Name of collection to use
         document: Document
             Document to insert
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         ObjectId
             Id of the inserted document
         """
         document["created_at"] = get_utc_now()
@@ -82,27 +85,30 @@
 
     @audit_transaction(mode=AuditTransactionMode.MULTI, action_type=AuditActionType.INSERT)
     async def insert_many(
         self,
         collection_name: str,
         documents: Iterable[Document],
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> List[ObjectId]:
         """
         Insert records into collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject user_id and catalog_id into the document automatically.
 
         Parameters
         ----------
         collection_name: str
             Name of collection to use
         documents: Iterable[Document]
             Documents to insert
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         List[ObjectId]
             Ids of the inserted document
         """
         utc_now = get_utc_now()
@@ -112,15 +118,15 @@
         return await self._insert_many(collection_name=collection_name, documents=documents)
 
     async def find_one(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         user_id: Optional[ObjectId] = None,  # pylint: disable=unused-argument
-    ) -> Optional[Document]:
+    ) -> Optional[Document]:  # pylint: disable=unused-argument
         """
         Find one record from collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically.
 
         Parameters
         ----------
         collection_name: str
@@ -185,14 +191,15 @@
     @audit_transaction(mode=AuditTransactionMode.SINGLE, action_type=AuditActionType.UPDATE)
     async def update_one(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         update: DocumentUpdate,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> int:
         """
         Update one record in collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically, and
         it does not inject user_id and catalog_id into the update automatically.
 
         Parameters
@@ -201,14 +208,16 @@
             ID of user who performed this operation
         collection_name: str
             Name of collection to use
         query_filter: QueryFilter
             Conditions to filter on
         update: DocumentUpdate
             Values to update
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         int
             Number of records modified
 
         Raises
@@ -231,14 +240,15 @@
     @audit_transaction(mode=AuditTransactionMode.MULTI, action_type=AuditActionType.UPDATE)
     async def update_many(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         update: DocumentUpdate,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> int:
         """
         Update many records in collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically, and
         it does not inject user_id and catalog_id into the update automatically.
 
         Parameters
@@ -247,14 +257,16 @@
             Name of collection to use
         query_filter: QueryFilter
             Conditions to filter on
         update: DocumentUpdate
             Values to update
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         int
             Number of records modified
 
         Raises
@@ -277,14 +289,15 @@
     @audit_transaction(mode=AuditTransactionMode.SINGLE, action_type=AuditActionType.REPLACE)
     async def replace_one(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         replacement: Document,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> int:
         """
         Replace one record in collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically, and
         it does not inject user_id and catalog_id into the update automatically.
 
         Parameters
@@ -293,14 +306,16 @@
             Name of collection to use
         query_filter: QueryFilter
             Conditions to filter on
         replacement: Document
             New document to replace existing one
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         int
             Number of records modified
         """
         replacement["created_at"] = replacement["updated_at"] = get_utc_now()
@@ -312,54 +327,60 @@
 
     @audit_transaction(mode=AuditTransactionMode.SINGLE, action_type=AuditActionType.DELETE)
     async def delete_one(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> int:
         """
         Delete one record from collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically.
 
         Parameters
         ----------
         collection_name: str
             Name of collection to use
         query_filter: QueryFilter
             Conditions to filter on
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         int
             Number of records deleted
         """
         return await self._delete_one(collection_name=collection_name, query_filter=query_filter)
 
     @audit_transaction(mode=AuditTransactionMode.MULTI, action_type=AuditActionType.DELETE)
     async def delete_many(
         self,
         collection_name: str,
         query_filter: QueryFilter,
         user_id: Optional[ObjectId],  # pylint: disable=unused-argument
+        disable_audit: bool = False,  # pylint: disable=unused-argument
     ) -> int:
         """
         Delete many records from collection. Note that when using this method inside a non BaseDocumentService,
         please use with caution as it does not inject catalog_id into the query filter automatically.
 
         Parameters
         ----------
         collection_name: str
             Name of collection to use
         query_filter: QueryFilter
             Conditions to filter on
         user_id: Optional[ObjectId]
             ID of user who performed this operation
+        disable_audit: bool
+            Whether to disable creating an audit record for this operation
 
         Returns
         -------
         int
             Number of records deleted
         """
         return await self._delete_many(collection_name=collection_name, query_filter=query_filter)
```

### Comparing `featurebyte-0.3.1/featurebyte/persistent/mongo.py` & `featurebyte-0.4.0/featurebyte/persistent/mongo.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/algorithm.py` & `featurebyte-0.4.0/featurebyte/query_graph/algorithm.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/enum.py` & `featurebyte-0.4.0/featurebyte/query_graph/enum.py`

 * *Files 6% similar despite different names*

```diff
@@ -46,14 +46,15 @@
     GROUPBY = "groupby"
     ITEM_GROUPBY = "item_groupby"
     AGGREGATE_AS_AT = "aggregate_as_at"
     LOOKUP = "lookup"
     JOIN = "join"
     JOIN_FEATURE = "join_feature"
     TRACK_CHANGES = "track_changes"
+    FORWARD_AGGREGATE = "forward_aggregate"
 
     # other operations
     ASSIGN = "assign"
     CONDITIONAL = "conditional"
     ALIAS = "alias"
     IS_NULL = "is_null"
     CAST = "cast"
@@ -92,14 +93,17 @@
     INPUT = "input"
     REQUEST_COLUMN = "request_column"
 
     # graph node to support nested graph
     GRAPH = "graph"
     PROXY_INPUT = "proxy_input"
 
+    # generic function node
+    GENERIC_FUNCTION = "generic_function"
+
 
 class NodeOutputType(StrEnum):
     """
     Query graph node output type
     """
 
     FRAME = "frame"
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/graph.py` & `featurebyte-0.4.0/featurebyte/query_graph/graph.py`

 * *Files 15% similar despite different names*

```diff
@@ -6,41 +6,45 @@
     Callable,
     DefaultDict,
     Dict,
     Iterator,
     List,
     Literal,
     Optional,
+    Set,
     Tuple,
     TypedDict,
     cast,
 )
 
 from collections import OrderedDict, defaultdict
 
 from bson import ObjectId
 from pydantic import Field
 
 from featurebyte.common.singleton import SingletonMeta
-from featurebyte.query_graph.enum import NodeOutputType, NodeType
+from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.graph_node.base import GraphNode
 from featurebyte.query_graph.model.graph import Edge, GraphNodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.base import NodeT
-from featurebyte.query_graph.node.generic import GroupByNode, LookupNode
+from featurebyte.query_graph.node.function import GenericFunctionNode
+from featurebyte.query_graph.node.generic import ForwardAggregateNode, GroupByNode, LookupNode
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.node.metadata.operation import (
     DerivedDataColumn,
     OperationStructure,
     SourceDataColumn,
 )
 from featurebyte.query_graph.node.mixin import BaseGroupbyParameters
+from featurebyte.query_graph.transform.entity_extractor import EntityExtractor
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
 from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
 from featurebyte.query_graph.transform.pruning import prune_query_graph
+from featurebyte.query_graph.transform.quick_pruning import QuickGraphStructurePruningTransformer
 from featurebyte.query_graph.transform.reconstruction import GraphReconstructionTransformer
 
 
 class QueryGraph(QueryGraphModel):
     """
     Graph data structure
     """
@@ -126,24 +130,64 @@
             Name of the node to get entity IDs for
 
         Returns
         -------
         List[ObjectId]
             List of entity IDs in the query graph
         """
+        entity_state = EntityExtractor(graph=self).extract(
+            node=self.get_node_by_name(node_name=node_name)
+        )
+        return sorted(entity_state.entity_ids)
+
+    def get_user_defined_function_ids(self, node_name: str) -> List[ObjectId]:
+        """
+        Get user defined function IDs of the query graph given the target node name
+
+        Parameters
+        ----------
+        node_name: str
+            Name of the node to get user defined function IDs for
+
+        Returns
+        -------
+        List[ObjectId]
+            List of user defined function IDs in the query graph
+        """
         output = []
         target_node = self.get_node_by_name(node_name)
-        for node in self.iterate_nodes(target_node=target_node, node_type=None):
-            if isinstance(node.parameters, BaseGroupbyParameters):
-                if node.parameters.entity_ids:
-                    output.extend(node.parameters.entity_ids)
-            elif isinstance(node, LookupNode):
-                output.append(node.parameters.entity_id)
+        for node in self.iterate_nodes(
+            target_node=target_node, node_type=NodeType.GENERIC_FUNCTION
+        ):
+            assert isinstance(node, GenericFunctionNode)
+            output.append(node.parameters.function_id)
         return sorted(set(output))
 
+    def get_forward_aggregate_window(self, node_name: str) -> Optional[str]:
+        """
+        Get the window of the forward aggregate node
+
+        Parameters
+        ----------
+        node_name: str
+            Name of the node to get the window for
+
+        Returns
+        -------
+        Optional[str]
+            window of the forward aggregate node
+        """
+        target_node = self.get_node_by_name(node_name)
+        for node in self.iterate_nodes(
+            target_node=target_node, node_type=NodeType.FORWARD_AGGREGATE
+        ):
+            assert isinstance(node, ForwardAggregateNode)
+            return node.parameters.window
+        return None
+
     def get_entity_columns(self, node_name: str) -> List[str]:
         """
         Get entity columns of the query graph given the target node name
 
         Parameters
         ----------
         node_name: str
@@ -202,17 +246,16 @@
             assert timestamp_col is not None, "Timestamp column not found"
             if isinstance(timestamp_col, SourceDataColumn):
                 table_id = timestamp_col.table_id
             else:
                 # for the item view, the timestamp column is from the event table, not the item table
                 # therefore the column is a DerivedDataColumn
                 assert isinstance(timestamp_col, DerivedDataColumn)
-                table_id = None
-                for column in timestamp_col.columns:
-                    table_id = column.table_id
+                # use the last column's table ID as timestamp column's table ID
+                table_id = timestamp_col.columns[-1].table_id
 
             assert table_id is not None, "Table ID not found"
             yield group_by_node, table_id
 
     def load(self, graph: QueryGraphModel) -> Tuple["QueryGraph", Dict[str, str]]:
         """
         Load the query graph into the query graph
@@ -229,71 +272,111 @@
         """
         node_name_map: Dict[str, str] = {}
         for node in graph.iterate_sorted_nodes():
             input_nodes = [
                 self.get_node_by_name(node_name_map[input_node_name])
                 for input_node_name in graph.backward_edges_map[node.name]
             ]
-            node_global = self.add_operation(
-                node_type=NodeType(node.type),
-                node_params=node.parameters.dict(),
-                node_output_type=NodeOutputType(node.output_type),
-                input_nodes=input_nodes,
-            )
+            node_global = self.add_operation_node(node=node, input_nodes=input_nodes)
             node_name_map[node.name] = node_global.name
         return self, node_name_map
 
-    def extract_operation_structure(self, node: Node, **kwargs: Any) -> OperationStructure:
+    def extract_operation_structure(
+        self,
+        node: Node,
+        keep_all_source_columns: bool = True,
+        **kwargs: Any,
+    ) -> OperationStructure:
         """
         Extract operation structure from the graph given target node
 
         Parameters
         ----------
         node: Node
             Target node used to construct the operation structure
+        keep_all_source_columns: bool
+            Whether to keep all source columns in the operation structure
         kwargs: Any
             Additional arguments to be passed to the OperationStructureExtractor.extract() method
 
         Returns
         -------
         OperationStructure
         """
-        op_struct_info = OperationStructureExtractor(graph=self).extract(node=node, **kwargs)
+        op_struct_info = OperationStructureExtractor(graph=self).extract(
+            node=node, keep_all_source_columns=keep_all_source_columns, **kwargs
+        )
         return op_struct_info.operation_structure_map[node.name]
 
-    def prune(self, target_node: Node, aggressive: bool) -> GraphNodeNameMap:
+    def extract_table_id_to_table_column_names(self, node: Node) -> Dict[ObjectId, Set[str]]:
+        """
+        Extract table ID to table column names based on the graph & given target node.
+        This mapping is used to prune the view graph node parameters or extract the table columns used by the
+        query graph object.
+
+        Parameters
+        ----------
+        node: Node
+            Target node used to the mapping
+
+        Returns
+        -------
+        dict[ObjectId, set[str]]
+            Table ID to table column names
+        """
+        operation_structure = self.extract_operation_structure(node, keep_all_source_columns=True)
+        # prepare table ID to source column names mapping, use this mapping to prune the view graph node parameters
+        table_id_to_source_column_names: Dict[ObjectId, Set[str]] = defaultdict(set)
+        for src_col in operation_structure.iterate_source_columns():
+            assert src_col.table_id is not None, "Source table ID is missing."
+            table_id_to_source_column_names[src_col.table_id].add(src_col.name)
+        return table_id_to_source_column_names
+
+    def prune(self, target_node: Node) -> GraphNodeNameMap:
         """
         Prune the query graph and return the pruned graph & mapped node.
 
         To prune the graph, this function first traverses from the target node to the input node.
         The unused branches of the graph will get pruned in this step. After that, a new graph is
         reconstructed by adding the required nodes back.
 
         Parameters
         ----------
         target_node: Node
             Target end node
-        aggressive: bool
-            Flag to enable aggressive mode. When the `aggressive` is True, those prunable nodes
-            may get removed if they do not contribute to the target node output. In addition,
-            all the node parameters will get pruned based on the output of the node. When the
-            `aggressive` is False, a graph traversal from the target node to the input node
-            is performed, all the traversed nodes will be kept without any modification on the
-            node parameters.
 
         Returns
         -------
         GraphNodeNameMap
             Tuple of pruned graph & its node name mapping
         """
-        pruned_graph, node_name_map, _ = prune_query_graph(
-            graph=self, node=target_node, aggressive=aggressive
-        )
+        pruned_graph, node_name_map, _ = prune_query_graph(graph=self, node=target_node)
         return pruned_graph, node_name_map
 
+    def quick_prune(self, target_node_names: List[str]) -> GraphNodeNameMap:
+        """
+        Quick prune the query graph and return the pruned graph & mapped node.
+
+        The main difference between `quick_prune` and `prune` is that `quick_prune` does not change existing
+        node parameters. The generated graph's node parameters are the same as the input graph. It is less
+        expensive than `prune` as it does not perform any operation structure extraction.
+
+        Parameters
+        ----------
+        target_node_names: List[str]
+            Target end node names
+
+        Returns
+        -------
+        GraphNodeNameMap
+        """
+        return QuickGraphStructurePruningTransformer(graph=self).transform(
+            target_node_names=target_node_names
+        )
+
     def reconstruct(
         self, node_name_to_replacement_node: Dict[str, Node], regenerate_groupby_hash: bool
     ) -> GraphNodeNameMap:
         """
         Reconstruct the query graph using the replacement node mapping
 
         Parameters
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/graph_node/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/graph_node/base.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/column_info.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/column_info.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/common_table.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/common_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/critical_data_info.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/critical_data_info.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/feature_job_setting.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/feature_job_setting.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/graph.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/graph.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 """
 This model contains query graph internal model structures
 """
 from typing import Any, DefaultDict, Dict, Iterator, List, Optional, Set, Tuple
 
-import json
 from collections import defaultdict
 
 from pydantic import Field, root_validator, validator
 
 from featurebyte.exception import GraphInconsistencyError
 from featurebyte.models.base import FeatureByteBaseModel
 from featurebyte.query_graph.algorithm import dfs_traversal, topological_sort
@@ -42,15 +41,15 @@
     edges_map: DefaultDict[str, List[str]] = Field(default=defaultdict(list), exclude=True)
     backward_edges_map: DefaultDict[str, List[str]] = Field(default=defaultdict(list), exclude=True)
     node_type_counter: DefaultDict[str, int] = Field(default=defaultdict(int), exclude=True)
     node_name_to_ref: Dict[str, str] = Field(default_factory=dict, exclude=True)
     ref_to_node_name: Dict[str, str] = Field(default_factory=dict, exclude=True)
 
     def __repr__(self) -> str:
-        return json.dumps(self.json_dict(), indent=4)
+        return self.json(by_alias=True, indent=4)
 
     def __str__(self) -> str:
         return repr(self)
 
     @property
     def sorted_node_names_by_ref(self) -> List[str]:
         """
@@ -445,44 +444,67 @@
         self.edges_map[parent.name].append(child.name)
         self.backward_edges_map[child.name].append(parent.name)
 
     def _generate_node_name(self, node_type: NodeType) -> str:
         self.node_type_counter[node_type] += 1
         return f"{node_type}_{self.node_type_counter[node_type]}"
 
-    def _add_node(
-        self, node_type: NodeType, node_params: Dict[str, Any], node_output_type: NodeOutputType
-    ) -> Node:
+    def _add_node(self, node: Node) -> Node:
         """
         Add node to the graph by specifying node type, parameters & output type
 
         Parameters
         ----------
-        node_type: NodeType
-            node type
-        node_params: dict[str, Any]
-            parameters in dictionary format
-        node_output_type: NodeOutputType
-            node output type
+        node: Node
+            Node to add
 
         Returns
         -------
         node: Node
         """
-        node_dict = {
-            "name": self._generate_node_name(node_type),
-            "type": node_type,
-            "parameters": node_params,
-            "output_type": node_output_type,
-        }
-        node = construct_node(**node_dict)
+        node = node.copy()
+        node.name = self._generate_node_name(node.type)
         self.nodes.append(node)
         self.nodes_map[node.name] = node
         return node
 
+    def add_operation_node(self, node: Node, input_nodes: List[Node]) -> Node:
+        """
+        Add operation node to the query graph.
+
+        Parameters
+        ----------
+        node: Node
+            operation node to add
+        input_nodes: list[Node]
+            list of input nodes
+
+        Returns
+        -------
+        Node
+            operation node of the given input
+        """
+        input_node_refs = [self.node_name_to_ref[node.name] for node in input_nodes]
+        node_ref = hash_node(
+            node.type,
+            self._get_node_parameter_for_compute_node_hash(node),
+            node.output_type,
+            input_node_refs,
+        )
+        if node_ref not in self.ref_to_node_name:
+            node = self._add_node(node)
+            for input_node in input_nodes:
+                self._add_edge(input_node, node)
+
+            self.ref_to_node_name[node_ref] = node.name
+            self.node_name_to_ref[node.name] = node_ref
+        else:
+            node = self.nodes_map[self.ref_to_node_name[node_ref]]
+        return node
+
     def add_operation(
         self,
         node_type: NodeType,
         node_params: Dict[str, Any],
         node_output_type: NodeOutputType,
         input_nodes: List[Node],
     ) -> Node:
@@ -501,35 +523,19 @@
             list of input nodes
 
         Returns
         -------
         Node
             operation node of the given input
         """
-        input_node_refs = [self.node_name_to_ref[node.name] for node in input_nodes]
         # create a temp_node to validate the node parameters & use only the required parameters to hash
         temp_node = construct_node(
             name=str(node_type),
             type=node_type,
             parameters=node_params,
             output_type=node_output_type,
         )
-        node_ref = hash_node(
-            temp_node.type,
-            self._get_node_parameter_for_compute_node_hash(temp_node),
-            temp_node.output_type,
-            input_node_refs,
-        )
-        if node_ref not in self.ref_to_node_name:
-            node = self._add_node(node_type, node_params, node_output_type)
-            for input_node in input_nodes:
-                self._add_edge(input_node, node)
-
-            self.ref_to_node_name[node_ref] = node.name
-            self.node_name_to_ref[node.name] = node_ref
-        else:
-            node = self.nodes_map[self.ref_to_node_name[node_ref]]
-        return node
+        return self.add_operation_node(node=temp_node, input_nodes=input_nodes)
 
 
 NodeNameMap = Dict[str, str]
 GraphNodeNameMap = Tuple[QueryGraphModel, NodeNameMap]
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/model/table.py` & `featurebyte-0.4.0/featurebyte/query_graph/model/table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/__init__.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/__init__.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/agg_func.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/agg_func.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -253,20 +253,20 @@
         """
         operation_info = self._derive_node_operation_info(
             inputs=inputs, branch_state=branch_state, global_state=global_state
         )
         if operation_info.columns or operation_info.aggregations:
             # make sure node name should be included in the node operation info
             assert self.name in operation_info.all_node_names
-        # Update is_time_based based on the inputs, or if the derive_node_operation_info returns true
-        update_args = {
-            "is_time_based": any(input_.is_time_based for input_ in inputs)
-            or operation_info.is_time_based,
-        }
-        return OperationStructure(**{**operation_info.dict(), **update_args})
+
+        # update is_time_based based on the inputs, or if the derive_node_operation_info returns true
+        operation_info.is_time_based = (
+            any(input_.is_time_based for input_ in inputs) or operation_info.is_time_based
+        )
+        return operation_info
 
     def derive_sdk_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         operation_structure: OperationStructure,
         config: CodeGenerationConfig,
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/binary.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/binary.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 """
 This module contains binary operation node classes
 """
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import List, Literal, Optional, Sequence, Union
+from typing import List, Literal, Sequence
 
-from pydantic import BaseModel, Field
+from pydantic import Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.base import (
-    BaseSeriesOutputNode,
     BaseSeriesOutputWithAScalarParamNode,
     BinaryArithmeticOpNode,
     BinaryLogicalOpNode,
     BinaryRelationalOpNode,
 )
 from featurebyte.query_graph.node.metadata.operation import OperationStructure
 
@@ -143,29 +142,26 @@
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.FLOAT
 
     def generate_expression(self, left_operand: str, right_operand: str) -> str:
         return f"{left_operand}.pow({right_operand})"
 
 
-class IsInNode(BaseSeriesOutputNode):
+class IsInNode(BaseSeriesOutputWithAScalarParamNode):
     """IsInNode class"""
 
-    class Parameters(BaseModel):
-        """Parameters"""
-
-        value: Optional[Sequence[Union[bool, int, float, str]]]
-
     type: Literal[NodeType.IS_IN] = Field(NodeType.IS_IN, const=True)
-    parameters: Parameters
 
     @property
     def max_input_count(self) -> int:
         return 2
 
     def _get_required_input_columns(
         self, input_index: int, available_column_names: List[str]
     ) -> Sequence[str]:
         return self._assert_empty_required_input_columns()
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.BOOL
+
+    def generate_expression(self, left_operand: str, right_operand: str) -> str:
+        return f"{left_operand}.isin({right_operand})"
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/cleaning_operation.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/cleaning_operation.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/count_dict.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/count_dict.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/date.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/date.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/generic.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/generic.py`

 * *Files 2% similar despite different names*

```diff
@@ -464,14 +464,107 @@
         col_name = var_name_expressions[0].as_input()
         entity_columns = ValueStr.create(self.parameters.entity_columns)
         offset = ValueStr.create(self.parameters.offset)
         expression = f"{col_name}.lag(entity_columns={entity_columns}, offset={offset})"
         return [], ExpressionStr(expression)
 
 
+class ForwardAggregateParameters(BaseGroupbyParameters):
+    """
+    Forward aggregate parameters
+    """
+
+    name: str
+    window: Optional[str]
+    timestamp_col: InColumnStr
+
+
+class ForwardAggregateNode(AggregationOpStructMixin, BaseNode):
+    """
+    ForwardAggregateNode class.
+    """
+
+    type: Literal[NodeType.FORWARD_AGGREGATE] = Field(NodeType.FORWARD_AGGREGATE, const=True)
+    output_type: NodeOutputType = Field(NodeOutputType.FRAME, const=True)
+    parameters: ForwardAggregateParameters
+
+    @property
+    def max_input_count(self) -> int:
+        return 1
+
+    def _get_aggregations(
+        self,
+        columns: List[ViewDataColumn],
+        node_name: str,
+        other_node_names: Set[str],
+        output_var_type: DBVarType,
+    ) -> List[AggregationColumn]:
+        col_name_map = {col.name: col for col in columns}
+        return [
+            AggregationColumn(
+                name=self.parameters.name,
+                method=self.parameters.agg_func,
+                keys=self.parameters.keys,
+                window=self.parameters.window,
+                category=self.parameters.value_by,
+                column=col_name_map.get(self.parameters.parent),
+                filter=any(col.filter for col in columns),
+                aggregation_type=self.type,
+                node_names={node_name}.union(other_node_names),
+                node_name=node_name,
+                dtype=output_var_type,
+            )
+        ]
+
+    def _exclude_source_columns(self) -> List[str]:
+        cols = self.parameters.keys
+        return [str(col) for col in cols]
+
+    def _is_time_based(self) -> bool:
+        return True
+
+    def _get_required_input_columns(
+        self, input_index: int, available_column_names: List[str]
+    ) -> Sequence[str]:
+        return self._extract_column_str_values(self.parameters.dict(), InColumnStr)
+
+    def _derive_sdk_code(
+        self,
+        node_inputs: List[VarNameExpressionInfo],
+        var_name_generator: VariableNameGenerator,
+        operation_structure: OperationStructure,
+        config: CodeGenerationConfig,
+        context: CodeGenerationContext,
+    ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
+        var_name_expressions = self._assert_no_info_dict(node_inputs)
+        statements, var_name = self._convert_expression_to_variable(
+            var_name_expression=var_name_expressions[0],
+            var_name_generator=var_name_generator,
+            node_output_type=NodeOutputType.FRAME,
+            node_output_category=NodeOutputCategory.VIEW,
+            to_associate_with_node_name=False,
+        )
+        keys = ValueStr.create(self.parameters.keys)
+        category = ValueStr.create(self.parameters.value_by)
+        grouped = f"{var_name}.groupby(by_keys={keys}, category={category})"
+        out_var_name = var_name_generator.convert_to_variable_name(
+            variable_name_prefix="target",
+            node_name=self.name,
+        )
+        expression = get_object_class_from_function_call(
+            callable_name=f"{grouped}.forward_aggregate",
+            value_column=self.parameters.parent,
+            method=self.parameters.agg_func,
+            window=self.parameters.window,
+            target_name=self.parameters.name,
+        )
+        statements.append((out_var_name, expression))
+        return statements, out_var_name
+
+
 class GroupByNode(AggregationOpStructMixin, BaseNode):
     """GroupByNode class"""
 
     class Parameters(BaseGroupbyParameters):
         """Parameters"""
 
         windows: List[Optional[str]]
@@ -972,26 +1065,30 @@
                 if params.join_type != "left"
                 else col.node_names,
                 node_name=self.name,
             )
             for col in inputs[0].columns
             if col.name in left_col_map
         }
+        left_on_col = next(col for col in inputs[0].columns if col.name == self.parameters.left_on)
         right_columns = {}
         right_on_col = next(
             col for col in inputs[1].columns if col.name == self.parameters.right_on
         )
         for col in inputs[1].columns:
             if col.name in right_col_map:
                 if global_state.keep_all_source_columns:
                     # when keep_all_source_columns is True, we should include the right_on column in the join
                     # so that any changes on the right_on column can be tracked.
                     right_columns[col.name] = DerivedDataColumn.create(
                         name=right_col_map[col.name],  # type: ignore
-                        columns=[right_on_col, col],
+                        # the main source column must be on the right most side
+                        # this is used to decide the timestamp column source table in
+                        # `iterate_group_by_node_and_table_id_pairs`
+                        columns=[left_on_col, right_on_col, col],
                         transform=self.transform_info,
                         node_name=self.name,
                         dtype=col.dtype,
                         other_node_names=col.node_names,
                     )
                 else:
                     right_columns[col.name] = col.clone(
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/input.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/input.py`

 * *Files 0% similar despite different names*

```diff
@@ -389,14 +389,15 @@
                 SourceDataColumn(
                     name=column.name,
                     table_id=self.parameters.id,
                     table_type=self.parameters.type,
                     node_names={self.name},
                     node_name=self.name,
                     dtype=column.dtype,
+                    filter=False,
                 )
                 for column in self.parameters.columns
             ],
             output_type=NodeOutputType.FRAME,
             output_category=NodeOutputCategory.VIEW,
             row_index_lineage=(self.name,),
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/metadata/column.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/metadata/column.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/metadata/operation.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/metadata/operation.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,29 +15,30 @@
     Type,
     TypeVar,
     Union,
     overload,
 )
 from typing_extensions import Annotated  # pylint: disable=wrong-import-order
 
+import dataclasses
 from collections import defaultdict
 
-from bson import json_util
-from pydantic import Field, root_validator, validator
+from pydantic import BaseModel, Field
 
 from featurebyte.enum import AggFunc, DBVarType, StrEnum, TableDataType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 
 
 class NodeOutputCategory(StrEnum):
     """NodeOutputCategory enum used to identify node output category"""
 
     VIEW = "view"
     FEATURE = "feature"
+    TARGET = "target"
 
 
 class ViewDataColumnType(StrEnum):
     """ViewColumnType enum"""
 
     SOURCE = "source"
     DERIVED = "derived"
@@ -50,15 +51,16 @@
     POST_AGGREGATION = "post_aggregation"
 
 
 BaseColumnT = TypeVar("BaseColumnT", bound="BaseColumn")
 BaseDerivedColumnT = TypeVar("BaseDerivedColumnT", bound="BaseDerivedColumn")
 
 
-class BaseColumn(FeatureByteBaseModel):
+@dataclasses.dataclass
+class BaseColumn:
     """
     BaseColumn class
 
     name: Optional[str]
         Column name
     dtype: DBVarType
         Column table type
@@ -72,28 +74,38 @@
 
     name: Optional[str]
     dtype: DBVarType
     filter: bool
     node_names: Set[str]
     node_name: str
 
+    def _get_hash_key(self) -> Tuple[Optional[str], DBVarType, bool, str]:
+        """
+        Get the hash key of the column (used to construct the hash key of the column)
+
+        Returns
+        -------
+        Tuple[Optional[str], DBVarType, bool, str]
+        """
+        return self.name, self.dtype, self.filter, self.node_name
+
     def clone(self: BaseColumnT, **kwargs: Any) -> BaseColumnT:
         """
         Clone an existing object by overriding certain attribute(s)
 
         Parameters
         ----------
         kwargs: Any
             Keyword parameters to overwrite existing object
 
         Returns
         -------
         Self
         """
-        return type(self)(**{**self.dict(), **kwargs})
+        return dataclasses.replace(self, **kwargs)
 
     def clone_without_internal_nodes(
         self: BaseColumnT,
         proxy_node_name_map: Dict[str, "OperationStructure"],
         graph_node_name: str,
         graph_node_transform: str,
         **kwargs: Any,
@@ -142,74 +154,70 @@
         # if any of the node name are not from the proxy input names, that means the nested graph's node
         # must change the column in some way, we must include the node name
         if self.node_names.difference(proxy_node_name_map):
             node_kwargs["node_names"].add(graph_node_name)
             node_kwargs["node_name"] = graph_node_name
             if hasattr(self, "transforms"):
                 node_kwargs["transforms"] = [graph_node_transform] if graph_node_transform else []
-        return self.clone(**node_kwargs, **kwargs)
+        return dataclasses.replace(self, **{**node_kwargs, **kwargs})
 
 
+@dataclasses.dataclass
 class BaseDataColumn(BaseColumn):
     """BaseDataColumn class"""
 
     name: str
 
 
+@dataclasses.dataclass
 class BaseDerivedColumn(BaseColumn):
     """BaseDerivedColumn class"""
 
     transforms: List[str]
     columns: Sequence[BaseDataColumn]
 
-    @root_validator(pre=True)
-    @classmethod
-    def _set_filter_flag(cls, values: Dict[str, Any]) -> Dict[str, Any]:
-        if "filter" not in values:
-            values["filter"] = any(col.filter for col in values["columns"])
-        return values
-
     @staticmethod
     def insert_column(
-        column_map: Dict[str, BaseDataColumn], column: BaseDataColumn
-    ) -> Dict[str, BaseDataColumn]:
+        column_map: Dict[Tuple[str, str], BaseDataColumn], column: BaseDataColumn
+    ) -> Dict[Tuple[str, str], BaseDataColumn]:
         """
         Insert column into dictionary. If more than more columns with the same name, aggregate the node names
         (make sure we don't miss any node which is used for pruning) and combine filter flag (take the max
         value to indicate the filtered column has been used at least in one operation).
 
         Parameters
         ----------
-        column_map: Dict[str, BaseDataColumn]
+        column_map: Dict[Tuple[str, str], BaseDataColumn]
             Dictionary map
         column: BaseDataColumn
             Data column object
 
         Returns
         -------
-        Dict[str, BaseDataColumn]
+        Dict[Tuple[str, str], BaseDataColumn]
         """
-        if column.name not in column_map:
-            column_map[column.name] = column
+        key = (column.name, column.node_name)
+        if key not in column_map:
+            column_map[key] = column
         else:
-            cur_col = column_map[column.name]
-            column_map[column.name] = column.clone(
+            cur_col = column_map[key]
+            column_map[key] = column.clone(
                 node_names=cur_col.node_names.union(column.node_names),
                 node_name=cur_col.node_name
                 if len(cur_col.node_names) > len(column.node_names)
                 else column.node_name,
                 filter=cur_col.filter or column.filter,
             )
         return column_map
 
     @classmethod
     def _flatten_columns(
         cls, columns: Sequence[Union[BaseDataColumn, "BaseDerivedColumn"]]
     ) -> Tuple[Sequence[BaseDataColumn], List[str], Set[str]]:
-        col_map: Dict[str, BaseDataColumn] = {}
+        col_map: Dict[Tuple[str, str], BaseDataColumn] = {}
         transforms: List[str] = []
         node_names: Set[str] = set()
         for column in columns:
             node_names.update(column.node_names)
             if isinstance(column, BaseDerivedColumn):
                 # traverse to nested derived columns first
                 flat_columns, flat_transforms, _ = cls._flatten_columns(column.columns)
@@ -270,14 +278,15 @@
         return cls(
             name=name,
             dtype=dtype,
             columns=columns,
             transforms=transforms,
             node_names=node_names,
             node_name=node_name,
+            filter=any(col.filter for col in columns),
         )
 
     def clone_without_internal_nodes(
         self,
         proxy_node_name_map: Dict[str, "OperationStructure"],
         graph_node_name: str,
         graph_node_transform: str,
@@ -294,69 +303,76 @@
             graph_node_name=graph_node_name,
             graph_node_transform=graph_node_transform,
             columns=columns,
             **kwargs,
         )
 
 
+@dataclasses.dataclass
 class SourceDataColumn(BaseDataColumn):
     """Source column"""
 
     table_id: Optional[PydanticObjectId]
     table_type: TableDataType
-    type: Literal[ViewDataColumnType.SOURCE] = Field(ViewDataColumnType.SOURCE, const=True)
-    filter: bool = Field(default=False)
+    filter: bool
+    type: Literal[ViewDataColumnType.SOURCE] = ViewDataColumnType.SOURCE
 
     def __hash__(self) -> int:
-        col_dict = self.dict()
-        col_dict["node_names"] = sorted(col_dict["node_names"])
-        return hash(json_util.dumps(col_dict, sort_keys=True))
+        key = (*self._get_hash_key(), self.type, self.table_id, self.table_type)
+        return hash(key)
 
 
+@dataclasses.dataclass
 class DerivedDataColumn(BaseDerivedColumn):
     """Derived column"""
 
     columns: List[SourceDataColumn]
-    type: Literal[ViewDataColumnType.DERIVED] = Field(ViewDataColumnType.DERIVED, const=True)
+    type: Literal[ViewDataColumnType.DERIVED] = ViewDataColumnType.DERIVED
 
     def __hash__(self) -> int:
-        col_dict = self.dict()
-        col_dict["columns"] = sorted(
-            [json_util.dumps(col, sort_keys=True) for col in col_dict["columns"]]
-        )
-        col_dict["node_names"] = sorted(col_dict["node_names"])
-        return hash(json_util.dumps(col_dict, sort_keys=True))
+        columns_item = tuple(sorted([col.__hash__() for col in self.columns]))
+        key = (*self._get_hash_key(), self.type, columns_item)
+        return hash(key)
 
 
 ViewDataColumn = Annotated[Union[SourceDataColumn, DerivedDataColumn], Field(discriminator="type")]
 
 
+@dataclasses.dataclass
 class AggregationColumn(BaseDataColumn):
     """Aggregation column"""
 
     method: Optional[AggFunc]
-    keys: List[str]
+    keys: Sequence[str]
     window: Optional[str]
     category: Optional[str]
-    type: Literal[FeatureDataColumnType.AGGREGATION] = Field(
-        FeatureDataColumnType.AGGREGATION, const=True
-    )
     column: Optional[ViewDataColumn]
     aggregation_type: Literal[
         NodeType.GROUPBY,
         NodeType.ITEM_GROUPBY,
         NodeType.LOOKUP,
         NodeType.AGGREGATE_AS_AT,
         NodeType.REQUEST_COLUMN,
+        NodeType.FORWARD_AGGREGATE,
     ]
+    type: Literal[FeatureDataColumnType.AGGREGATION] = FeatureDataColumnType.AGGREGATION
 
     def __hash__(self) -> int:
-        col_dict = self.dict()
-        col_dict["node_names"] = sorted(col_dict["node_names"])
-        return hash(json_util.dumps(col_dict, sort_keys=True))
+        key = (
+            *self._get_hash_key(),
+            self.type,
+            # specific to aggregation column
+            self.method,
+            tuple(sorted(self.keys)),
+            self.window,
+            self.category,
+            self.column.__hash__() if self.column else None,
+            self.aggregation_type,
+        )
+        return hash(key)
 
     def clone_without_internal_nodes(
         self,
         proxy_node_name_map: Dict[str, "OperationStructure"],
         graph_node_name: str,
         graph_node_transform: str,
         **kwargs: Any,
@@ -374,38 +390,34 @@
             graph_node_name=graph_node_name,
             graph_node_transform=graph_node_transform,
             column=column,
             **kwargs,
         )
 
 
+@dataclasses.dataclass
 class PostAggregationColumn(BaseDerivedColumn):
     """Post aggregation column"""
 
     columns: List[AggregationColumn]
-    type: Literal[FeatureDataColumnType.POST_AGGREGATION] = Field(
-        FeatureDataColumnType.POST_AGGREGATION, const=True
-    )
+    type: Literal[FeatureDataColumnType.POST_AGGREGATION] = FeatureDataColumnType.POST_AGGREGATION
 
     def __hash__(self) -> int:
-        col_dict = self.dict()
-        col_dict["columns"] = sorted(
-            [json_util.dumps(col, sort_keys=True) for col in col_dict["columns"]]
-        )
-        col_dict["node_names"] = sorted(col_dict["node_names"])
-        return hash(json_util.dumps(col_dict, sort_keys=True))
+        columns_item = tuple(sorted([col.__hash__() for col in self.columns]))
+        key = (*self._get_hash_key(), self.type, columns_item)
+        return hash(key)
 
 
 FeatureDataColumn = Annotated[
     Union[AggregationColumn, PostAggregationColumn], Field(discriminator="type")
 ]
 
 
-class GroupOperationStructure(FeatureByteBaseModel):
-    """GroupOperationStructure class"""
+class GroupOperationStructure(BaseModel):
+    """GroupOperationStructure class is used to construct feature/target info's metadata attribute."""
 
     source_columns: List[SourceDataColumn] = Field(default_factory=list)
     derived_columns: List[DerivedDataColumn] = Field(default_factory=list)
     aggregations: List[AggregationColumn] = Field(default_factory=list)
     post_aggregation: Optional[PostAggregationColumn]
     row_index_lineage: Tuple[str, ...]
     is_time_based: bool = Field(default=False)
@@ -419,29 +431,39 @@
         -------
         List[PydanticObjectId]
         """
         table_ids = [col.table_id for col in self.source_columns if col.table_id]
         return list(set(table_ids))
 
 
-class OperationStructure(FeatureByteBaseModel):
+@dataclasses.dataclass
+class OperationStructure:
     """NodeOperationStructure class"""
 
     # When NodeOutputType is:
     # - NodeOutputType.VIEW -> columns represents the output columns
-    # - NodeOutputType.FEATURE -> columns represents the input columns
-    columns: List[ViewDataColumn] = Field(default_factory=list)
-    aggregations: List[FeatureDataColumn] = Field(default_factory=list)
+    # - NodeOutputType.FEATURE or TARGET -> columns represents the input columns
     output_type: NodeOutputType
     output_category: NodeOutputCategory
     row_index_lineage: Tuple[str, ...]
-    is_time_based: bool = Field(default=False)
+    columns: List[ViewDataColumn] = dataclasses.field(default_factory=list)
+    aggregations: List[FeatureDataColumn] = dataclasses.field(default_factory=list)
+    is_time_based: bool = False
 
-    def __init__(self, **kwargs: Any):
-        super().__init__(**kwargs)
+    @staticmethod
+    def _deduplicate(columns: List[Any]) -> List[Any]:
+        output: Dict[Any, None] = {}
+        for col in columns:
+            if col not in output:
+                output[col] = None
+        return list(output)
+
+    def __post_init__(self) -> None:
+        self.columns = self._deduplicate(self.columns)
+        self.aggregations = self._deduplicate(self.aggregations)
         if self.output_category == NodeOutputCategory.VIEW:
             # make sure there are no duplicated column names
             assert len(self.columns) == len(set(col.name for col in self.columns))
         elif self.output_category == NodeOutputCategory.FEATURE:
             assert len(self.aggregations) == len(set(agg.name for agg in self.aggregations))
 
     @property
@@ -511,23 +533,14 @@
         -------
         List[str]
         """
         if self.output_category == NodeOutputCategory.VIEW:
             return [col.name for col in self.columns if col.name]
         return [agg.name for agg in self.aggregations if agg.name]
 
-    @validator("columns", "aggregations")
-    @classmethod
-    def _validator(cls, value: List[Any]) -> List[Any]:
-        output: Dict[Any, None] = {}
-        for obj in value:
-            if obj not in output:
-                output[obj] = None
-        return list(output)
-
     @overload
     def _split_column_by_type(
         self, columns: List[Union[SourceDataColumn, DerivedDataColumn]]
     ) -> Tuple[List[SourceDataColumn], List[DerivedDataColumn]]:
         ...
 
     @overload
@@ -543,56 +556,80 @@
             List[Union[AggregationColumn, PostAggregationColumn]],
         ],
     ) -> Union[
         Tuple[List[SourceDataColumn], List[DerivedDataColumn]],
         Tuple[List[AggregationColumn], List[PostAggregationColumn]],
     ]:
         _ = self
-        input_column_map: Dict[str, Any] = {}
+        input_column_map: Dict[Tuple[str, str], Any] = {}
         derived_column_map: Dict[Any, None] = {}
         for column in columns:
             if isinstance(column, (DerivedDataColumn, PostAggregationColumn)):
                 derived_column_map[column] = None
                 for inner_column in column.columns:
                     input_column_map = BaseDerivedColumn.insert_column(
                         input_column_map, inner_column
                     )
             else:
                 input_column_map = BaseDerivedColumn.insert_column(input_column_map, column)
 
         return list(input_column_map.values()), list(derived_column_map)
 
+    def iterate_source_columns(self) -> Iterator[SourceDataColumn]:
+        """
+        Iterate source columns in the operation structure.
+
+        Yields
+        ------
+        SourceDataColumn
+            Source column that directly contributes to the output
+        """
+        for column in self.columns:
+            if isinstance(column, SourceDataColumn):
+                yield column
+            else:
+                assert isinstance(column, DerivedDataColumn)
+                for source_col in column.columns:
+                    yield source_col
+
+    def iterate_aggregations(self) -> Iterator[AggregationColumn]:
+        """
+        Iterate aggregations in the operation structure.
+
+        Yields
+        ------
+        AggregationColumn
+            Aggregation that directly contributes to the output
+        """
+        for aggregation in self.aggregations:
+            if isinstance(aggregation, AggregationColumn):
+                yield aggregation
+            else:
+                assert isinstance(aggregation, PostAggregationColumn)
+                for source_agg in aggregation.columns:
+                    yield source_agg
+
     def iterate_source_columns_or_aggregations(
         self,
     ) -> Iterator[Union[SourceDataColumn, AggregationColumn]]:
         """
         Iterate source columns or aggregations. For view category, it returns SourceDataColumn. For feature category,
         it returns AggregationColumn.
 
         Yields
         ------
         Union[SourceDataColumn, AggregationColumn]
             Column/Aggregation that directly contributes to the output
         """
         if self.output_category == NodeOutputCategory.VIEW:
-            for column in self.columns:
-                if isinstance(column, SourceDataColumn):
-                    yield column
-                else:
-                    assert isinstance(column, DerivedDataColumn)
-                    for source_col in column.columns:
-                        yield source_col
+            for column in self.iterate_source_columns():
+                yield column
         else:
-            for aggregation in self.aggregations:
-                if isinstance(aggregation, AggregationColumn):
-                    yield aggregation
-                else:
-                    assert isinstance(aggregation, PostAggregationColumn)
-                    for source_agg in aggregation.columns:
-                        yield source_agg
+            for aggregation in self.iterate_aggregations():
+                yield aggregation
 
     def to_group_operation_structure(self) -> GroupOperationStructure:
         """
         Convert the OperationStructure format to group structure format
 
         Returns
         -------
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/metadata/sdk_code.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/metadata/sdk_code.py`

 * *Files 9% similar despite different names*

```diff
@@ -198,14 +198,15 @@
 class ClassEnum(Enum):
     """
     ClassEnum is used to store the python package import tagging for specific classes or objects
     """
 
     # non-featurebyte related
     OBJECT_ID = ("bson", "ObjectId")
+    PD_TIMESTAMP = ("pandas", "Timestamp")
 
     # feature store
     FEATURE_STORE = ("featurebyte", "FeatureStore")
 
     # database details
     SNOWFLAKE_DETAILS = ("featurebyte", "SnowflakeDetails")
     DATABRICK_DETAILS = ("featurebyte", "DatabricksDetails")
@@ -240,14 +241,15 @@
 
     # others
     COLUMN_INFO = ("featurebyte.query_graph.model.column_info", "ColumnInfo")
     FEATURE_JOB_SETTING = ("featurebyte", "FeatureJobSetting")
     TO_TIMEDELTA = ("featurebyte", "to_timedelta")
     COLUMN_CLEANING_OPERATION = ("featurebyte", "ColumnCleaningOperation")
     REQUEST_COLUMN = ("featurebyte.api.request_column", "RequestColumn")
+    USER_DEFINED_FUNCTION = ("featurebyte", "UserDefinedFunction")
 
     def __call__(
         self, *args: Any, _method_name: Optional[str] = None, **kwargs: Any
     ) -> ObjectClass:
         module_path, class_name = self.value
         return ObjectClass(
             module_path=module_path,
@@ -272,19 +274,30 @@
         Positional arguments for the function call
     kwargs: Any
         Keyword arguments for the function call
 
     Returns
     -------
     ObjectClass
+
+    Raises
+    ------
+    ValueError
+        If module_path and class_name are not both None or both not None
     """
+    module_path = kwargs.pop("module_path", None)
+    class_name = kwargs.pop("class_name", None)
+    if (module_path and not class_name) or (class_name and not module_path):
+        raise ValueError("module_path and class_name should be both None or both not None")
     return ObjectClass(
         positional_args=args,
         keyword_args=kwargs,
         callable_name=callable_name,
+        module_path=module_path,
+        class_name=class_name,
     )
 
 
 VarNameExpressionStr = Union[VariableNameStr, ExpressionStr]
 VarNameExpressionInfo = Union[VariableNameStr, ExpressionStr, InfoDict]
 RightHandSide = Union[ValueStr, VariableNameStr, ExpressionStr, ObjectClass]
 StatementT = Union[  # pylint: disable=invalid-name
@@ -350,14 +363,15 @@
     """
     VariableNameGenerator class is used to generate the variable name given the characteristics of the
     value to be stored.
     """
 
     var_name_counter: DefaultDict[str, int] = Field(default_factory=lambda: defaultdict(int))
     node_name_to_var_name: Dict[str, VariableNameStr] = Field(default_factory=dict)
+    func_id_to_var_name: Dict[PydanticObjectId, VariableNameStr] = Field(default_factory=dict)
 
     def generate_variable_name(
         self,
         node_output_type: NodeOutputType,
         node_output_category: NodeOutputCategory,
         node_name: Optional[str],
     ) -> VariableNameStr:
@@ -391,42 +405,53 @@
                 pre_variable_name = "feat"
 
         return self.convert_to_variable_name(
             variable_name_prefix=pre_variable_name, node_name=node_name
         )
 
     def convert_to_variable_name(
-        self, variable_name_prefix: str, node_name: Optional[str]
+        self,
+        variable_name_prefix: str,
+        node_name: Optional[str],
+        function_id: Optional[PydanticObjectId] = None,
     ) -> VariableNameStr:
         """
         Convert an input variable name into a valid variable name (no name collision).
 
         Parameters
         ----------
         variable_name_prefix: str
             Variable name before name collision check
         node_name: Optional[str]
             If not None, the generated variable name will be associated with the node name. If the node name
             already exists, its associated variable name will be returned.
+        function_id: Optional[PydanticObjectId]
+            If not None, the generated variable name will be associated with the function ID. If the function ID
+            already exists, its associated variable name will be returned.
 
         Returns
         -------
         VariableNameStr
         """
+        assert not (node_name and function_id), "node_name and function_id cannot be both not None"
         if node_name is not None and node_name in self.node_name_to_var_name:
             return self.node_name_to_var_name[node_name]
+        if function_id is not None and function_id in self.func_id_to_var_name:
+            return self.func_id_to_var_name[function_id]
 
         count = self.var_name_counter[variable_name_prefix]
         self.var_name_counter[variable_name_prefix] += 1
         var_name = VariableNameStr(variable_name_prefix)
         if count:
             var_name = VariableNameStr(f"{variable_name_prefix}_{count}")
 
         if node_name is not None:
             self.node_name_to_var_name[node_name] = var_name
+        if function_id is not None:
+            self.func_id_to_var_name[function_id] = var_name
         return var_name
 
 
 class UnusedVariableFinder(ast.NodeVisitor):
     """UnusedVariableFinder class is used to find the unused variables in the generated SDK code."""
 
     def __init__(self) -> None:
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/mixin.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -178,14 +178,16 @@
         if (
             self.type == NodeType.ITEM_GROUPBY
             and NodeType.JOIN_FEATURE in branch_state.visited_node_types
         ):
             # if the output of the item_groupby will be used to join with other table,
             # this mean the output of this item_groupby is view but not feature.
             output_category = NodeOutputCategory.VIEW
+        elif self.type == NodeType.FORWARD_AGGREGATE:
+            output_category = NodeOutputCategory.TARGET
 
         # prepare output variable type
         if agg_func:
             aggregation_func_obj = construct_agg_func(agg_func)
             input_var_type = parent_columns[0].dtype if parent_columns else columns[0].dtype
             output_var_type = aggregation_func_obj.derive_output_var_type(
                 input_var_type=input_var_type, category=self.parameters.value_by
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/nested.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/nested.py`

 * *Files 0% similar despite different names*

```diff
@@ -71,19 +71,19 @@
         global_state: OperationStructureInfo,
     ) -> OperationStructure:
         # lookup the operation structure using the proxy input's node_name parameter
         proxy_input_order = self.parameters.input_order
         operation_structure = global_state.proxy_input_operation_structures[proxy_input_order]
         return OperationStructure(
             columns=[
-                col.clone(node_names=[self.name], node_name=self.name)
+                col.clone(node_names={self.name}, node_name=self.name)
                 for col in operation_structure.columns
             ],
             aggregations=[
-                agg.clone(node_names=[self.name], node_name=self.name)
+                agg.clone(node_names={self.name}, node_name=self.name)
                 for agg in operation_structure.aggregations
             ],
             output_type=operation_structure.output_type,
             output_category=operation_structure.output_category,
             row_index_lineage=operation_structure.row_index_lineage,
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/request.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/request.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/scalar.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/scalar.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/schema.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/schema.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/string.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/string.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/unary.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/unary.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/node/validator.py` & `featurebyte-0.4.0/featurebyte/query_graph/node/validator.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/pruning_util.py` & `featurebyte-0.4.0/featurebyte/query_graph/pruning_util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/asat.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/asat.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/base.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,35 +9,39 @@
 from collections import defaultdict
 from dataclasses import dataclass
 
 from bson import ObjectId
 from sqlglot import expressions
 from sqlglot.expressions import Select, alias_, select
 
-from featurebyte.enum import SourceType
+from featurebyte.enum import DBVarType, InternalName, SourceType
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
+from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     CteStatements,
     get_qualified_column_identifier,
     quoted_identifier,
 )
 from featurebyte.query_graph.sql.online_serving_util import (
-    get_online_store_table_name_from_entity_ids,
+    get_online_store_table_name,
+    get_version_placeholder,
 )
 from featurebyte.query_graph.sql.specs import (
     AggregationSpec,
     NonTileBasedAggregationSpec,
     TileBasedAggregationSpec,
 )
 
 AggregationSpecT = TypeVar("AggregationSpecT", bound=AggregationSpec)
 NonTileBasedAggregationSpecT = TypeVar(
     "NonTileBasedAggregationSpecT", bound=NonTileBasedAggregationSpec
 )
 
+LATEST_VERSION = "LATEST_VERSION"
+
 
 @dataclass
 class AggregationResult:
     """
     Representation of aggregation result from an instance of Aggregator
     """
 
@@ -309,30 +313,33 @@
     """
 
     def __init__(self, *args: Any, **kwargs: Any) -> None:
         super().__init__(*args, **kwargs)
         self.agg_result_names_by_online_store_table: dict[str, set[str]] = defaultdict(set)
         self.original_serving_names_by_online_store_table: dict[str, list[str]] = {}
         self.request_serving_names_by_online_store_table: dict[str, list[str]] = {}
+        self.dtype_by_online_store_table: dict[str, DBVarType] = {}
 
     def update(self, aggregation_spec: TileBasedAggregationSpec) -> None:
         super().update(aggregation_spec)
         if self.is_online_serving:
-            table_name = get_online_store_table_name_from_entity_ids(
-                set(aggregation_spec.entity_ids if aggregation_spec.entity_ids is not None else [])
+            table_name = get_online_store_table_name(
+                set(aggregation_spec.entity_ids if aggregation_spec.entity_ids is not None else []),
+                self.adapter.get_physical_type_from_dtype(aggregation_spec.dtype),
             )
             self.agg_result_names_by_online_store_table[table_name].add(
                 aggregation_spec.agg_result_name
             )
             self.original_serving_names_by_online_store_table[
                 table_name
             ] = aggregation_spec.original_serving_names
             self.request_serving_names_by_online_store_table[
                 table_name
             ] = aggregation_spec.serving_names
+            self.dtype_by_online_store_table[table_name] = aggregation_spec.dtype
 
     def update_aggregation_table_expr(
         self,
         table_expr: Select,
         point_in_time_column: str,
         current_columns: list[str],
         current_query_index: int,
@@ -404,27 +411,175 @@
         for (
             table_name,
             agg_result_names_set,
         ) in self.agg_result_names_by_online_store_table.items():
             agg_result_names = sorted(agg_result_names_set)
             serving_names = self._alias_original_serving_names_to_request_serving_names(table_name)
             quoted_agg_result_names = [quoted_identifier(name) for name in agg_result_names]
-            expr = select(*serving_names, *quoted_agg_result_names).from_(table_name)
+            pivoted_online_store = self._pivot_online_store_table(
+                table_name=table_name,
+                agg_result_names=agg_result_names,
+                serving_names=self.original_serving_names_by_online_store_table[table_name],
+                dtype=self.dtype_by_online_store_table[table_name],
+            )
+            expr = select(*serving_names, *quoted_agg_result_names).from_(
+                pivoted_online_store.subquery()
+            )
             join_keys = self.request_serving_names_by_online_store_table[table_name]
             query = LeftJoinableSubquery(
                 expr,
                 agg_result_names,
                 join_keys=join_keys,
             )
             left_join_queries.append(query)
 
         return self._update_with_left_joins(
             table_expr, current_query_index=current_query_index, queries=left_join_queries
         )
 
+    def _pivot_online_store_table(
+        self,
+        table_name: str,
+        agg_result_names: list[str],
+        serving_names: list[str],
+        dtype: DBVarType,
+    ) -> Select:
+        """
+        Pivot the online store table from long to wide format. See an example below.
+
+        Physical online store table's schema:
+
+        CUST_ID  AGGREGATION_RESULT_NAME  VALUE
+        ---------------------------------------
+        C1       agg_id_1_30d             1.0
+        C1       agg_id_1_90d             3.0
+        C2       agg_id_1_30d             10.0
+        C2       agg_id_1_90d             33.0
+        C3       agg_id_1_90d             100.0
+
+        Pivoted table's schema:
+
+        CUST_ID  agg_id_1_30d  agg_id_1_90d
+        -----------------------------------
+        C1       1.0           3.0
+        C2       10.0          33.0
+        C3       NULL          100.0
+
+        Parameters
+        ----------
+        table_name: str
+            Online store table name
+        agg_result_names: list[str]
+            Name of the aggregation results
+        serving_names: list[str]
+            Serving names
+        dtype: DBVarType
+            Data type of the aggregation results
+
+        Returns
+        -------
+        Select
+        """
+        literal_agg_result_names = [make_literal_value(name) for name in agg_result_names]
+        value_column = self.adapter.online_store_pivot_prepare_value_column(dtype)
+
+        # Get the latest version of the online store table
+        values_expressions = [
+            expressions.Tuple(
+                expressions=[
+                    make_literal_value(agg_result_name),
+                    expressions.Identifier(this=get_version_placeholder(agg_result_name)),
+                ]
+            )
+            for agg_result_name in agg_result_names
+        ]
+        values_alias = expressions.TableAlias(
+            this=expressions.Identifier(this="version_table"),
+            columns=[
+                quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+                quoted_identifier(LATEST_VERSION),
+            ],
+        )
+        latest_result_version = select(
+            quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+            quoted_identifier(LATEST_VERSION),
+        ).from_(expressions.Values(expressions=values_expressions, alias=values_alias))
+        latest_online_store_table = (
+            select("R.*")
+            .from_(latest_result_version.subquery(alias="L"))
+            .join(
+                table_name,
+                join_alias="R",
+                join_type="inner",
+                on=expressions.and_(
+                    expressions.EQ(
+                        this=get_qualified_column_identifier(
+                            InternalName.ONLINE_STORE_RESULT_NAME_COLUMN, "R"
+                        ),
+                        expression=get_qualified_column_identifier(
+                            InternalName.ONLINE_STORE_RESULT_NAME_COLUMN, "L"
+                        ),
+                    ),
+                    expressions.EQ(
+                        this=get_qualified_column_identifier(
+                            InternalName.ONLINE_STORE_VERSION_COLUMN, "R"
+                        ),
+                        expression=get_qualified_column_identifier(LATEST_VERSION, "L"),
+                    ),
+                ),
+            )
+        )
+
+        # Filter the latest version of the online store table to only include the required
+        # aggregation results
+        filtered_online_store = (
+            select(
+                *[quoted_identifier(serving_name) for serving_name in serving_names],
+                quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+                value_column,
+            )
+            .from_(latest_online_store_table.subquery())
+            .where(
+                expressions.In(
+                    this=quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+                    expressions=literal_agg_result_names,
+                )
+            )
+        )
+        pivots = [
+            expressions.Pivot(
+                expressions=[
+                    self.adapter.online_store_pivot_aggregation_func(
+                        quoted_identifier(InternalName.ONLINE_STORE_VALUE_COLUMN.value)
+                    )
+                ],
+                field=expressions.In(
+                    this=quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+                    expressions=literal_agg_result_names,
+                ),
+            )
+        ]
+        pivot_subquery = expressions.Subquery(this=filtered_online_store, pivots=pivots)
+        pivot_result = select(
+            *[
+                self.adapter.online_store_pivot_finalise_serving_name(serving_name)
+                for serving_name in serving_names
+            ],
+            *[
+                expressions.Alias(
+                    this=self.adapter.online_store_pivot_finalise_value_column(
+                        agg_result_name, dtype
+                    ),
+                    alias=quoted_identifier(agg_result_name),
+                )
+                for agg_result_name in agg_result_names
+            ],
+        ).from_(pivot_subquery)
+        return pivot_result
+
     def _alias_original_serving_names_to_request_serving_names(
         self, online_store_table_name: str
     ) -> list[expressions.Expression]:
         original_serving_names = self.original_serving_names_by_online_store_table[
             online_store_table_name
         ]
         request_serving_names = self.request_serving_names_by_online_store_table[
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/item.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/item.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/latest.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/latest.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/lookup.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/lookup.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/request_table.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/aggregator/window.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/aggregator/window.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/aggregate.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/aggregate.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.sql.ast.base import SQLNodeContext, TableNode
 from featurebyte.query_graph.sql.common import SQLType, quoted_identifier
 from featurebyte.query_graph.sql.specs import (
     AggregateAsAtSpec,
     AggregationSource,
+    ForwardAggregateSpec,
     ItemAggregationSpec,
     LookupSpec,
 )
 
 
 @dataclass  # type: ignore[misc]
 class Aggregate(TableNode):
@@ -180,7 +181,28 @@
         spec = ItemAggregationSpec.from_query_graph_node(
             context.query_node,
             aggregation_source=Aggregate.get_aggregation_source_from_source_node(source_node),
         )[0]
         feature_name = cast(str, spec.parameters.name)
         columns_map[feature_name] = quoted_identifier(spec.agg_result_name)
         return columns_map
+
+
+@dataclass
+class Forward(Aggregate):
+    """
+    Forward SQLNode
+    """
+
+    query_node_type = NodeType.FORWARD_AGGREGATE
+
+    @staticmethod
+    def construct_columns_map(
+        context: SQLNodeContext, source_node: TableNode
+    ) -> dict[str, Expression]:
+        columns_map = {}
+        spec = ForwardAggregateSpec.from_query_graph_node(
+            context.query_node,
+            aggregation_source=Aggregate.get_aggregation_source_from_source_node(source_node),
+        )[0]
+        columns_map[spec.parameters.name] = quoted_identifier(spec.agg_result_name)
+        return columns_map
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/base.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/binary.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/binary.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/count_dict.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/count_dict.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/datetime.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/datetime.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 Module for datetime operations related sql generation
 """
 from __future__ import annotations
 
-from typing import Literal, Union, cast
+from typing import Union, cast
 
 from dataclasses import dataclass
 
 import pandas as pd
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
@@ -56,16 +56,16 @@
         else:
             # If timezone offset is provided, apply that to the input timestamp (in UTC) to obtain
             # the local time before extracting date properties
             timezone_offset_seconds = expressions.Anonymous(
                 this="F_TIMEZONE_OFFSET_TO_SECOND",
                 expressions=[timezone_offset_expr],
             )
-            timestamp_expr = expressions.Anonymous(
-                this="DATEADD", expressions=["second", timezone_offset_seconds, input_expr_node.sql]
+            timestamp_expr = context.adapter.dateadd_second(
+                timezone_offset_seconds, input_expr_node.sql
             )
 
         sql_node = DatetimeExtractNode(
             context=context,
             table_node=table_node,
             expr=timestamp_expr,
             dt_property=context.parameters["property"],
@@ -77,45 +77,35 @@
 class DateDiffNode(ExpressionNode):
     """Node for date difference operation"""
 
     left_node: ExpressionNode
     right_node: ExpressionNode
     query_node_type = NodeType.DATE_DIFF
 
-    def with_unit(self, unit: TimedeltaSupportedUnitType) -> Expression:
-        """Construct a date difference expression with provided time unit
-
-        Parameters
-        ----------
-        unit : TimedeltaSupportedUnitType
-            Time unit
+    def total_microseconds(self) -> Expression:
+        """Construct a date difference expression in microseconds
 
         Returns
         -------
         Expression
         """
-        output_expr = expressions.Anonymous(
-            this="DATEDIFF",
-            expressions=[
-                expressions.Identifier(this=unit),
-                self.right_node.sql,
-                self.left_node.sql,
-            ],
+        output_expr = self.context.adapter.datediff_microsecond(
+            timestamp_expr_1=self.right_node.sql,
+            timestamp_expr_2=self.left_node.sql,
         )
         return output_expr
 
     @property
     def sql(self) -> Expression:
         # The behaviour of DATEDIFF given a time unit depends on the engine; some engines perform
         # rounding but others don't. To keep a consistent behaviour, always work in the highest
         # supported precision (microsecond) and convert the result back to the desired unit
         # explicitly.
-        working_unit: Literal["microsecond"] = "microsecond"
         return TimedeltaExtractNode.convert_timedelta_unit(
-            input_expr=self.with_unit(working_unit), input_unit=working_unit, output_unit="second"
+            input_expr=self.total_microseconds(), input_unit="microsecond", output_unit="second"
         )
 
     @classmethod
     def build(cls, context: SQLNodeContext) -> DateDiffNode:
         table_node, left_node, right_node = prepare_binary_op_input_nodes(context)
         node = cls(
             context=context,
@@ -133,15 +123,15 @@
     timedelta_node: Union[TimedeltaNode, DateDiffNode]
     unit: TimedeltaSupportedUnitType
     query_node_type = NodeType.TIMEDELTA_EXTRACT
 
     @property
     def sql(self) -> Expression:
         if isinstance(self.timedelta_node, DateDiffNode):
-            expr = self.timedelta_node.with_unit("microsecond")
+            expr = self.timedelta_node.total_microseconds()
             output_expr = self.convert_timedelta_unit(expr, "microsecond", self.unit)
         else:
             output_expr = self.convert_timedelta_unit(
                 self.timedelta_node.sql, self.timedelta_node.unit, self.unit
             )
         return output_expr
 
@@ -260,15 +250,15 @@
             date_add_args = [
                 self.timedelta_node.value_with_unit("microsecond"),
                 self.input_date_node.sql,
             ]
         elif isinstance(self.timedelta_node, DateDiffNode):
             # timedelta is the result of date difference
             date_add_args = [
-                self.timedelta_node.with_unit("microsecond"),
+                self.timedelta_node.total_microseconds(),
                 self.input_date_node.sql,
             ]
         else:
             # timedelta is a constant value
             quantity_expr = TimedeltaExtractNode.convert_timedelta_unit(
                 self.timedelta_node.sql, input_unit="second", output_unit="microsecond"
             )
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/generic.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/generic.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/groupby.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/groupby.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/input.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/input.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/is_in.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/is_in.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/join.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/join.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/join_feature.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/join_feature.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/literal.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/literal.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/request.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/request.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/string.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/string.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,21 +50,18 @@
     pattern: str
     case: bool
     query_node_type = NodeType.STR_CONTAINS
 
     @property
     def sql(self) -> Expression:
         if self.case:
-            return fb_expressions.Contains(
-                this=self.expr.sql,
-                pattern=make_literal_value(self.pattern),
-            )
-        return fb_expressions.Contains(
-            this=expressions.Lower(this=self.expr.sql),
-            pattern=expressions.Lower(this=make_literal_value(self.pattern)),
+            return self.context.adapter.str_contains(self.expr.sql, self.pattern)
+        return self.context.adapter.str_contains(
+            expressions.Lower(this=self.expr.sql),
+            self.pattern.lower(),
         )
 
     @classmethod
     def build(cls, context: SQLNodeContext) -> StringContains:
         table_node, input_expr_node, parameters = prepare_unary_input_nodes(context)
         sql_node = StringContains(
             context=context,
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/tile.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/tile.py`

 * *Files 2% similar despite different names*

```diff
@@ -310,14 +310,14 @@
         raise NotImplementedError()
 
     @classmethod
     def build(cls, context: SQLNodeContext) -> AggregatedTilesNode | None:
         sql_node = None
         if context.sql_type == SQLType.POST_AGGREGATION:
             agg_specs = TileBasedAggregationSpec.from_groupby_query_node(
-                context.query_node, context.adapter
+                context.graph, context.query_node, context.adapter
             )
             columns_map = {}
             for agg_spec in agg_specs:
                 columns_map[agg_spec.feature_name] = quoted_identifier(agg_spec.agg_result_name)
             sql_node = AggregatedTilesNode(context=context, columns_map=columns_map)
         return sql_node
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/track_changes.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/track_changes.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/unary.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/unary.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/ast/util.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/ast/util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/builder.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/builder.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/common.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/common.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/dataframe.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/dataframe.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/expression.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/expression.py`

 * *Files 12% similar despite different names*

```diff
@@ -60,20 +60,14 @@
 class RPad(Func):
     """RPad function"""
 
     _sql_names = ["RPAD"]
     arg_types = {"this": True, "length": True, "pad": True}
 
 
-class Contains(Func):
-    """Contains function"""
-
-    arg_types = {"this": True, "pattern": True}
-
-
 class Concat(Func):
     """Concat function"""
 
     arg_types = {"expressions": False}
     is_var_len_args = True
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/feature_compute.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/feature_compute.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,14 +12,16 @@
 from featurebyte.enum import SourceType
 from featurebyte.models.parent_serving import ParentServingPreparation
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.aggregator.asat import AsAtAggregator
+from featurebyte.query_graph.sql.aggregator.base import TileBasedAggregator
+from featurebyte.query_graph.sql.aggregator.forward import ForwardAggregator
 from featurebyte.query_graph.sql.aggregator.item import ItemAggregator
 from featurebyte.query_graph.sql.aggregator.latest import LatestAggregator
 from featurebyte.query_graph.sql.aggregator.lookup import LookupAggregator
 from featurebyte.query_graph.sql.aggregator.window import WindowAggregator
 from featurebyte.query_graph.sql.ast.base import TableNode
 from featurebyte.query_graph.sql.ast.generic import AliasNode, Project
 from featurebyte.query_graph.sql.builder import SQLOperationGraph
@@ -32,23 +34,29 @@
 )
 from featurebyte.query_graph.sql.parent_serving import construct_request_table_with_parent_entities
 from featurebyte.query_graph.sql.specs import (
     AggregateAsAtSpec,
     AggregationSpec,
     AggregationType,
     FeatureSpec,
+    ForwardAggregateSpec,
     ItemAggregationSpec,
     LookupSpec,
     NonTileBasedAggregationSpec,
     TileBasedAggregationSpec,
 )
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
 
 AggregatorType = Union[
-    LatestAggregator, LookupAggregator, WindowAggregator, ItemAggregator, AsAtAggregator
+    LatestAggregator,
+    LookupAggregator,
+    WindowAggregator,
+    ItemAggregator,
+    AsAtAggregator,
+    ForwardAggregator,
 ]
 AggregationSpecType = Union[TileBasedAggregationSpec, NonTileBasedAggregationSpec]
 
 
 class FeatureExecutionPlan:
     """Responsible for constructing the SQL to compute features by aggregating tiles"""
 
@@ -63,14 +71,15 @@
         aggregator_kwargs = {"source_type": source_type, "is_online_serving": is_online_serving}
         self.aggregators: dict[str, AggregatorType] = {
             AggregationType.LATEST: LatestAggregator(**aggregator_kwargs),
             AggregationType.LOOKUP: LookupAggregator(**aggregator_kwargs),
             AggregationType.WINDOW: WindowAggregator(**aggregator_kwargs),
             AggregationType.ITEM: ItemAggregator(**aggregator_kwargs),
             AggregationType.AS_AT: AsAtAggregator(**aggregator_kwargs),
+            AggregationType.FORWARD: ForwardAggregator(**aggregator_kwargs),
         }
         self.feature_specs: dict[str, FeatureSpec] = {}
         self.adapter = get_sql_adapter(source_type)
         self.source_type = source_type
         self.parent_serving_preparation = parent_serving_preparation
 
     @property
@@ -105,14 +114,29 @@
 
         Returns
         -------
         list[str]
         """
         return list(self.feature_specs.keys())
 
+    @property
+    def tile_based_aggregation_result_names(self) -> list[str]:
+        """Returns the list of tile based aggregation result names
+
+        Returns
+        -------
+        list[str]
+        """
+        out = set()
+        for aggregator in self.iter_aggregators():
+            if isinstance(aggregator, TileBasedAggregator):
+                for result_names in aggregator.agg_result_names_by_online_store_table.values():
+                    out.update(result_names)
+        return list(out)
+
     def iter_aggregators(self) -> Iterable[AggregatorType]:
         """Iterate over all the aggregators
 
         Yields
         ------
         BaseAggregator
             Instance of an aggregator
@@ -168,21 +192,21 @@
             Columns in the request table
 
         Returns
         -------
         tuple[expressions.Select, List[str]]
         """
         assert self.parent_serving_preparation is not None
-        table_expr, new_columns = construct_request_table_with_parent_entities(
+        parent_serving_result = construct_request_table_with_parent_entities(
             request_table_name=request_table_name,
             request_table_columns=request_table_columns,
             join_steps=self.parent_serving_preparation.join_steps,
             feature_store_details=self.parent_serving_preparation.feature_store_details,
         )
-        return table_expr, new_columns
+        return parent_serving_result.table_expr, parent_serving_result.parent_entity_columns
 
     def construct_combined_aggregation_cte(
         self,
         request_table_name: str,
         point_in_time_column: str,
         request_table_columns: Optional[list[str]],
     ) -> tuple[CteStatement, list[str]]:
@@ -291,28 +315,28 @@
         )
         return table_expr
 
     def construct_combined_sql(
         self,
         request_table_name: str,
         point_in_time_column: str,
-        request_table_columns: list[str],
+        request_table_columns: Optional[list[str]],
         prior_cte_statements: Optional[CteStatements] = None,
         exclude_post_aggregation: bool = False,
         exclude_columns: Optional[set[str]] = None,
     ) -> expressions.Select:
         """Construct combined SQL that will generate the features
 
         Parameters
         ----------
         request_table_name : str
             Name of request table to use
         point_in_time_column : str
             Point in time column
-        request_table_columns : list[str]
+        request_table_columns : Optional[list[str]]
             Request table columns
         prior_cte_statements : Optional[list[tuple[str, str]]]
             Other CTE statements to incorporate to the final SQL (namely the request data SQL and
             on-demand tile SQL)
         exclude_post_aggregation: bool
             When True, exclude post aggregation transforms and select aggregated columns as the
             output columns directly. Intended to be used by online store pre-computation.
@@ -328,14 +352,15 @@
             assert isinstance(prior_cte_statements, list)
             cte_statements.extend(prior_cte_statements)
 
         if exclude_columns is None:
             exclude_columns = set()
 
         if self.parent_serving_preparation is not None:
+            assert request_table_columns is not None
             (
                 updated_request_table_expr,
                 new_columns,
             ) = self.construct_request_table_with_parent_entities(
                 request_table_name=request_table_name,
                 request_table_columns=request_table_columns,
             )
@@ -444,14 +469,15 @@
         -------
         AggregationSpec
         """
         groupby_nodes = list(self.graph.iterate_nodes(node, NodeType.GROUPBY))
         item_groupby_nodes = list(self.graph.iterate_nodes(node, NodeType.ITEM_GROUPBY))
         lookup_nodes = list(self.graph.iterate_nodes(node, NodeType.LOOKUP))
         asat_nodes = list(self.graph.iterate_nodes(node, NodeType.AGGREGATE_AS_AT))
+        forward_aggregate_nodes = list(self.graph.iterate_nodes(node, NodeType.FORWARD_AGGREGATE))
 
         out: list[AggregationSpecType] = []
         if groupby_nodes:
             # Feature involves window aggregations. In this case, tiling applies. Even if
             # ITEM_GROUPBY nodes are involved, their results would have already been incorporated in
             # tiles, so we only need to handle GROUPBY node type here.
             for groupby_node in groupby_nodes:
@@ -466,14 +492,18 @@
             for lookup_node in lookup_nodes:
                 out.extend(self.get_non_tiling_specs(LookupSpec, lookup_node))
 
         if asat_nodes:
             for asat_node in asat_nodes:
                 out.extend(self.get_non_tiling_specs(AggregateAsAtSpec, asat_node))
 
+        if forward_aggregate_nodes:
+            for forward_aggregate_node in forward_aggregate_nodes:
+                out.extend(self.get_non_tiling_specs(ForwardAggregateSpec, forward_aggregate_node))
+
         return out
 
     def get_specs_from_groupby(self, groupby_node: Node) -> Sequence[TileBasedAggregationSpec]:
         """Update FeatureExecutionPlan with a groupby query node
 
         Parameters
         ----------
@@ -481,15 +511,15 @@
             Groupby query node
 
         Returns
         -------
         list[AggregationSpec]
         """
         return TileBasedAggregationSpec.from_groupby_query_node(
-            groupby_node, self.adapter, serving_names_mapping=self.serving_names_mapping
+            self.graph, groupby_node, self.adapter, serving_names_mapping=self.serving_names_mapping
         )
 
     def get_non_tiling_specs(
         self, spec_cls: Type[NonTileBasedAggregationSpec], node: Node
     ) -> Sequence[NonTileBasedAggregationSpec]:
         """
         Update FeatureExecutionPlan with a node that produces NonTileBasedAggregationSpec
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/feature_historical.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/feature_historical.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,47 +1,42 @@
 """
 Historical features SQL generation
 """
 from __future__ import annotations
 
-from typing import Callable, List, Optional, Tuple, Union, cast
+from typing import Callable, List, Optional, Tuple, cast
 
 import datetime
-import time
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 
 import numpy as np
 import pandas as pd
 from bson import ObjectId
 from pandas.api.types import is_datetime64_any_dtype
 from sqlglot import expressions
 
-from featurebyte.common.progress import get_ranged_progress_callback
 from featurebyte.enum import SourceType, SpecialColumnName
 from featurebyte.exception import MissingPointInTimeColumnError, TooRecentPointInTimeError
 from featurebyte.logging import get_logger
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.models.parent_serving import ParentServingPreparation
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.common import (
-    REQUEST_TABLE_NAME,
     get_fully_qualified_table_name,
     get_qualified_column_identifier,
     quoted_identifier,
     sql_to_string,
 )
 from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
-from featurebyte.query_graph.sql.parent_serving import construct_request_table_with_parent_entities
 from featurebyte.query_graph.sql.specs import NonTileBasedAggregationSpec, TileBasedAggregationSpec
 from featurebyte.session.base import BaseSession
-from featurebyte.tile.tile_cache import TileCache
 
 HISTORICAL_REQUESTS_POINT_IN_TIME_RECENCY_HOUR = 48
 NUM_FEATURES_PER_QUERY = 50
 FB_ROW_INDEX_FOR_JOIN = "__FB_ROW_INDEX_FOR_JOIN"
 
 PROGRESS_MESSAGE_COMPUTING_FEATURES = "Computing features"
 TILE_COMPUTE_PROGRESS_MAX_PERCENT = 50  #  Progress percentage to report at end of tile computation
@@ -580,80 +575,14 @@
             select_expr=output_expr,
         ),
         source_type=source_type,
     )
     return HistoricalFeatureQuerySet(feature_queries=feature_queries, output_query=output_query)
 
 
-async def compute_tiles_on_demand(
-    session: BaseSession,
-    graph: QueryGraph,
-    nodes: list[Node],
-    request_id: str,
-    request_table_name: str,
-    request_table_columns: list[str],
-    serving_names_mapping: Optional[dict[str, str]],
-    parent_serving_preparation: Optional[ParentServingPreparation] = None,
-    progress_callback: Optional[Callable[[int, str], None]] = None,
-) -> None:
-    """
-    Compute tiles on demand
-
-    Parameters
-    ----------
-    session: BaseSession
-        Session to use to make queries
-    graph: QueryGraph
-        Query graph
-    nodes: list[Node]
-        List of query graph node
-    request_id: str
-        Request ID to be used as suffix of table names when creating temporary tables
-    request_table_name: str
-        Name of request table
-    request_table_columns: list[str]
-        List of column names in the observations set
-    serving_names_mapping : dict[str, str] | None
-        Optional serving names mapping if the training events data has different serving name
-        columns than those defined in Entities
-    parent_serving_preparation: Optional[ParentServingPreparation]
-        Preparation required for serving parent features
-    progress_callback: Optional[Callable[[int, str], None]]
-        Optional progress callback function
-    """
-    tile_cache = TileCache(session=session)
-
-    if parent_serving_preparation is None:
-        effective_request_table_name = request_table_name
-    else:
-        # Lookup parent entities and join them with the request table since tile computation
-        # requires these entity columns to be present in the request table.
-        request_table_expr, _ = construct_request_table_with_parent_entities(
-            request_table_name=request_table_name,
-            request_table_columns=request_table_columns,
-            join_steps=parent_serving_preparation.join_steps,
-            feature_store_details=parent_serving_preparation.feature_store_details,
-        )
-        request_table_query = sql_to_string(request_table_expr, session.source_type)
-        effective_request_table_name = "JOINED_PARENTS_" + request_table_name
-        await session.register_table_with_query(
-            effective_request_table_name,
-            request_table_query,
-        )
-
-    await tile_cache.compute_tiles_on_demand(
-        graph=graph,
-        nodes=nodes,
-        request_id=request_id,
-        request_table_name=effective_request_table_name,
-        serving_names_mapping=serving_names_mapping,
-        progress_callback=progress_callback,
-    )
-
-
 def get_feature_names(graph: QueryGraph, nodes: list[Node]) -> list[str]:
     """
     Get feature names given a list of ndoes
 
     Parameters
     ----------
     graph: QueryGraph
@@ -663,132 +592,7 @@
 
     Returns
     -------
     list[str]
     """
     planner = FeatureExecutionPlanner(graph=graph, is_online_serving=False)
     return planner.generate_plan(nodes).feature_names
-
-
-async def get_historical_features(  # pylint: disable=too-many-locals
-    session: BaseSession,
-    graph: QueryGraph,
-    nodes: list[Node],
-    observation_set: Union[pd.DataFrame, ObservationTableModel],
-    source_type: SourceType,
-    output_table_details: TableDetails,
-    serving_names_mapping: dict[str, str] | None = None,
-    is_feature_list_deployed: bool = False,
-    parent_serving_preparation: Optional[ParentServingPreparation] = None,
-    progress_callback: Optional[Callable[[int, str], None]] = None,
-) -> None:
-    """Get historical features
-
-    Parameters
-    ----------
-    session: BaseSession
-        Session to use to make queries
-    graph : QueryGraph
-        Query graph
-    nodes : list[Node]
-        List of query graph node
-    observation_set : Union[pd.DataFrame, ObservationTableModel]
-        Observation set
-    source_type : SourceType
-        Source type information
-    serving_names_mapping : dict[str, str] | None
-        Optional serving names mapping if the observations set has different serving name columns
-        than those defined in Entities
-    is_feature_list_deployed : bool
-        Whether the feature list that triggered this historical request is deployed. If so, tile
-        tables would have already been back-filled and there is no need to check and calculate tiles
-        on demand.
-    parent_serving_preparation: Optional[ParentServingPreparation]
-        Preparation required for serving parent features
-    output_table_details: TableDetails
-        Output table details to write the results to
-    progress_callback: Optional[Callable[[int, str], None]]
-        Optional progress callback function
-    """
-    tic_ = time.time()
-
-    observation_set = get_internal_observation_set(observation_set)
-
-    # Validate request
-    validate_request_schema(observation_set)
-    validate_historical_requests_point_in_time(observation_set)
-
-    # use a unique request table name
-    request_id = session.generate_session_unique_id()
-    request_table_name = f"{REQUEST_TABLE_NAME}_{request_id}"
-    request_table_columns = observation_set.columns
-
-    # Execute feature SQL code
-    await observation_set.register_as_request_table(
-        session, request_table_name, add_row_index=len(nodes) > NUM_FEATURES_PER_QUERY
-    )
-
-    # Compute tiles on demand if required
-    if not is_feature_list_deployed:
-        tile_cache_progress_callback = (
-            get_ranged_progress_callback(
-                progress_callback,
-                0,
-                TILE_COMPUTE_PROGRESS_MAX_PERCENT,
-            )
-            if progress_callback
-            else None
-        )
-        tic = time.time()
-        # Process nodes in batches
-        tile_cache_node_groups = split_nodes(
-            graph, nodes, NUM_FEATURES_PER_QUERY, is_tile_cache=True
-        )
-        for i, _nodes in enumerate(tile_cache_node_groups):
-            logger.debug("Checking and computing tiles on demand for %d nodes", len(_nodes))
-            await compute_tiles_on_demand(
-                session=session,
-                graph=graph,
-                nodes=_nodes,
-                request_id=request_id,
-                request_table_name=request_table_name,
-                request_table_columns=request_table_columns,
-                serving_names_mapping=serving_names_mapping,
-                parent_serving_preparation=parent_serving_preparation,
-                progress_callback=get_ranged_progress_callback(
-                    tile_cache_progress_callback,
-                    100 * i / len(tile_cache_node_groups),
-                    100 * (i + 1) / len(tile_cache_node_groups),
-                )
-                if tile_cache_progress_callback
-                else None,
-            )
-
-        elapsed = time.time() - tic
-        logger.debug("Done checking and computing tiles on demand", extra={"duration": elapsed})
-
-    if progress_callback:
-        progress_callback(TILE_COMPUTE_PROGRESS_MAX_PERCENT, PROGRESS_MESSAGE_COMPUTING_FEATURES)
-
-    # Generate SQL code that computes the features
-    historical_feature_query_set = get_historical_features_query_set(
-        graph=graph,
-        nodes=nodes,
-        request_table_columns=request_table_columns,
-        serving_names_mapping=serving_names_mapping,
-        source_type=source_type,
-        output_table_details=output_table_details,
-        output_feature_names=get_feature_names(graph, nodes),
-        request_table_name=request_table_name,
-        parent_serving_preparation=parent_serving_preparation,
-    )
-    await historical_feature_query_set.execute(
-        session,
-        get_ranged_progress_callback(
-            progress_callback,
-            TILE_COMPUTE_PROGRESS_MAX_PERCENT,
-            100,
-        )
-        if progress_callback
-        else None,
-    )
-    logger.debug(f"compute_historical_features in total took {time.time() - tic_:.2f}s")
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/feature_preview.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/feature_preview.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,41 +1,43 @@
 """
-Feature preview SQL generation
+Feature or Target preview SQL generation
 """
 # pylint: disable=too-many-locals
 from __future__ import annotations
 
-from typing import Any, Optional
+from typing import Any, List, Optional, cast
 
 import time
 
 import pandas as pd
 
 from featurebyte.enum import SourceType, SpecialColumnName
 from featurebyte.logging import get_logger
 from featurebyte.models.parent_serving import ParentServingPreparation
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.sql.common import sql_to_string
+from featurebyte.query_graph.sql.common import CteStatement, sql_to_string
 from featurebyte.query_graph.sql.dataframe import construct_dataframe_sql_expr
 from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
+from featurebyte.query_graph.sql.parent_serving import construct_request_table_with_parent_entities
 from featurebyte.query_graph.sql.tile_compute import OnDemandTileComputePlan
 
 logger = get_logger(__name__)
 
 
-def get_feature_preview_sql(
+def get_feature_or_target_preview_sql(
     request_table_name: str,
     graph: QueryGraphModel,
     nodes: list[Node],
     source_type: SourceType,
     point_in_time_and_serving_name_list: Optional[list[dict[str, Any]]] = None,
     parent_serving_preparation: Optional[ParentServingPreparation] = None,
 ) -> str:
-    """Get SQL code for previewing SQL
+    """
+    Get SQL code for previewing SQL for features or targets.
 
     Parameters
     ----------
     request_table_name : str
         Name of request table to use
     graph : QueryGraphModel
         Query graph model
@@ -53,53 +55,67 @@
     -------
     str
     """
     planner = FeatureExecutionPlanner(
         graph,
         source_type=source_type,
         is_online_serving=False,
-        parent_serving_preparation=parent_serving_preparation,
     )
     execution_plan = planner.generate_plan(nodes)
 
-    if point_in_time_and_serving_name_list:
-        # build required tiles
-        tic = time.time()
-        point_in_time_list = [
-            entry[SpecialColumnName.POINT_IN_TIME] for entry in point_in_time_and_serving_name_list
-        ]
-        tile_compute_plan = OnDemandTileComputePlan(point_in_time_list, source_type=source_type)
-        for node in nodes:
-            tile_compute_plan.process_node(graph, node)
-        cte_statements = sorted(tile_compute_plan.construct_on_demand_tile_ctes())
-        elapsed = time.time() - tic
-        logger.debug(f"Constructing required tiles SQL took {elapsed:.2}s")
+    exclude_columns = set()
+    cte_statements: Optional[List[CteStatement]] = None
+    request_table_columns: Optional[List[str]] = None
 
+    if point_in_time_and_serving_name_list:
         # prepare request table
         tic = time.time()
         df_request = pd.DataFrame(point_in_time_and_serving_name_list)
         request_table_sql = construct_dataframe_sql_expr(
             df_request, [SpecialColumnName.POINT_IN_TIME]
         )
-        cte_statements.append((request_table_name, request_table_sql))
+        cte_statements = [(request_table_name, request_table_sql)]
+        request_table_columns = cast(List[str], df_request.columns.tolist())
+
+        if parent_serving_preparation is not None:
+            parent_serving_result = construct_request_table_with_parent_entities(
+                request_table_name=request_table_name,
+                request_table_columns=request_table_columns,
+                join_steps=parent_serving_preparation.join_steps,
+                feature_store_details=parent_serving_preparation.feature_store_details,
+            )
+            request_table_name = parent_serving_result.new_request_table_name
+            request_table_columns = parent_serving_result.new_request_table_columns
+            cte_statements.append((request_table_name, parent_serving_result.table_expr))
+            exclude_columns.update(parent_serving_result.parent_entity_columns)
+
         elapsed = time.time() - tic
         logger.debug(f"Constructing request table SQL took {elapsed:.2}s")
 
-        request_table_columns = df_request.columns.tolist()
-    else:
-        cte_statements = None
-        request_table_columns = None
+        # build required tiles
+        tic = time.time()
+        tile_compute_plan = OnDemandTileComputePlan(
+            request_table_name=request_table_name,
+            source_type=source_type,
+        )
+        for node in nodes:
+            tile_compute_plan.process_node(graph, node)
+        tile_compute_ctes = sorted(tile_compute_plan.construct_on_demand_tile_ctes())
+        cte_statements.extend(tile_compute_ctes)
+        elapsed = time.time() - tic
+        logger.debug(f"Constructing required tiles SQL took {elapsed:.2}s")
 
     tic = time.time()
     preview_sql = sql_to_string(
         execution_plan.construct_combined_sql(
             request_table_name=request_table_name,
             point_in_time_column=SpecialColumnName.POINT_IN_TIME,
             request_table_columns=request_table_columns,
             prior_cte_statements=cte_statements,
+            exclude_columns=exclude_columns,
         ),
         source_type=source_type,
     )
     elapsed = time.time() - tic
     logger.debug(f"Generating full SQL took {elapsed:.2}s")
 
     return preview_sql
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/groupby_helper.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/groupby_helper.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 
 from sqlglot import expressions
 
 from featurebyte.enum import SourceType
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.ast.base import TableNode
 from featurebyte.query_graph.sql.builder import SQLOperationGraph
 from featurebyte.query_graph.sql.common import SQLType, construct_cte_sql, sql_to_string
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
 
 
 class BaseGraphInterpreter:
@@ -27,14 +28,15 @@
     source_type : SourceType
         Data source type information
     """
 
     def __init__(self, query_graph: QueryGraphModel, source_type: SourceType):
         self.query_graph = query_graph
         self.source_type = source_type
+        self.adapter = get_sql_adapter(source_type)
 
     def flatten_graph(self, node_name: str) -> Tuple[QueryGraphModel, Node]:
         """
         Flatten the query graph (replace those graph node with flattened nodes)
 
         Parameters
         ----------
@@ -59,15 +61,17 @@
 
         Returns
         -------
         Tuple[str, int]
             SQL code to execute, and column count
         """
         flat_graph, flat_node = self.flatten_graph(node_name=node_name)
-        operation_structure = QueryGraph(**flat_graph.dict()).extract_operation_structure(flat_node)
+        operation_structure = QueryGraph(**flat_graph.dict()).extract_operation_structure(
+            flat_node, keep_all_source_columns=True
+        )
         sql_tree = (
             SQLOperationGraph(
                 flat_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
             )
             .build(flat_node)
             .sql
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/preview.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/preview.py`

 * *Files 0% similar despite different names*

```diff
@@ -124,15 +124,15 @@
         else:
             sql_tree = sql_node.sql_standalone
 
         assert isinstance(sql_tree, expressions.Select)
 
         # apply type conversions
         operation_structure = QueryGraph(**self.query_graph.dict()).extract_operation_structure(
-            self.query_graph.get_node_by_name(node_name)
+            self.query_graph.get_node_by_name(node_name), keep_all_source_columns=True
         )
         if skip_conversion:
             type_conversions: dict[Optional[str], DBVarType] = {}
         else:
             sql_tree, type_conversions = self._apply_type_conversions(
                 sql_tree=sql_tree, columns=operation_structure.columns
             )
@@ -248,17 +248,16 @@
         Returns
         -------
         expressions.Expression
         """
         expr = expressions.alias_(make_literal_value(None), column_name, quoted=True)
         return cast(expressions.Expression, expr)
 
-    @staticmethod
     def _percentile_expr(
-        expression: expressions.Expression, quantile: float
+        self, expression: expressions.Expression, quantile: float
     ) -> expressions.Expression:
         """
         Create expression for percentile of column
 
         Parameters
         ----------
         expression: expressions.Expression
@@ -266,21 +265,15 @@
         quantile: float
             Quantile to use for percentile expression
 
         Returns
         -------
         expressions.Expression
         """
-        order_expr = expressions.Order(expressions=[expressions.Ordered(this=expression)])
-        return expressions.WithinGroup(
-            this=expressions.Anonymous(
-                this="percentile_cont", expressions=[parse_one(f"{quantile}")]
-            ),
-            expression=order_expr,
-        )
+        return self.adapter.get_percentile_expr(expression, quantile)
 
     @staticmethod
     def _tz_offset_expr(timestamp_tz_expr: expressions.Expression) -> expressions.Expression:
         """
         Create expression for timezone offset of a timestamp_tz expr
 
         Parameters
@@ -738,15 +731,15 @@
 
         Returns
         -------
         Tuple[str, dict[Optional[str], DBVarType], List[str], List[ViewDataColumn]]
             SQL code, type conversions to apply on result, row indices, columns
         """
         operation_structure = QueryGraph(**self.query_graph.dict()).extract_operation_structure(
-            self.query_graph.get_node_by_name(node_name)
+            self.query_graph.get_node_by_name(node_name), keep_all_source_columns=True
         )
 
         sql_tree, type_conversions = self._construct_sample_sql(
             node_name=node_name,
             num_rows=num_rows,
             seed=seed,
             from_timestamp=from_timestamp,
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/interpreter/tile.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/interpreter/tile.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/materialisation.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/materialisation.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/parent_serving.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/parent_serving.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 SQL generation for looking up parent entities
 """
 from __future__ import annotations
 
-from typing import List, Tuple
+from typing import List
+
+from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Select, select
 
 from featurebyte.enum import SpecialColumnName, TableDataType
 from featurebyte.models.parent_serving import JoinStep
 from featurebyte.query_graph.graph import QueryGraph
@@ -15,20 +17,41 @@
 from featurebyte.query_graph.node.schema import FeatureStoreDetails
 from featurebyte.query_graph.sql.aggregator.lookup import LookupAggregator
 from featurebyte.query_graph.sql.builder import SQLOperationGraph
 from featurebyte.query_graph.sql.common import SQLType, get_qualified_column_identifier
 from featurebyte.query_graph.sql.specs import AggregationSource, LookupSpec
 
 
+@dataclass
+class ParentEntityLookupResult:
+    """
+    Result of updating a request table with parent entities
+
+    table_expr: Select
+        Expression of the updated request table
+    parent_entity_columns: list[str]
+        Parent entity column names that were joined
+    new_request_table_name: str
+        Name of the updated request table
+    new_request_table_columns: list[str]
+        Column names of the updated request table
+    """
+
+    table_expr: Select
+    parent_entity_columns: List[str]
+    new_request_table_name: str
+    new_request_table_columns: List[str]
+
+
 def construct_request_table_with_parent_entities(
     request_table_name: str,
     request_table_columns: list[str],
     join_steps: list[JoinStep],
     feature_store_details: FeatureStoreDetails,
-) -> Tuple[Select, List[str]]:
+) -> ParentEntityLookupResult:
     """
     Construct a query to join parent entities into the request table
 
     Parameters
     ----------
     request_table_name: str
         Request table name
@@ -38,16 +61,15 @@
         The list of join steps to be applied. Each step joins a parent entity into the request
         table. Subsequent joins can use the newly joined columns as the join key.
     feature_store_details: FeatureStoreDetails
         Information about the feature store
 
     Returns
     -------
-    Tuple[Select, List[str]]
-        Tuple of sql query and list of parent entities column names that are joined
+    ParentEntityLookupResult
     """
     table_expr = select(
         *[get_qualified_column_identifier(col, "REQ") for col in request_table_columns]
     ).from_(expressions.alias_(request_table_name, "REQ"))
 
     current_columns = request_table_columns[:]
     new_columns = []
@@ -57,15 +79,20 @@
             join_step=join_step,
             feature_store_details=feature_store_details,
             current_columns=current_columns,
         )
         current_columns.append(join_step.parent_serving_name)
         new_columns.append(join_step.parent_serving_name)
 
-    return table_expr, new_columns
+    return ParentEntityLookupResult(
+        table_expr=table_expr,
+        parent_entity_columns=new_columns,
+        new_request_table_name="JOINED_PARENTS_" + request_table_name,
+        new_request_table_columns=current_columns,
+    )
 
 
 def _apply_join_step(
     table_expr: Select,
     join_step: JoinStep,
     feature_store_details: FeatureStoreDetails,
     current_columns: list[str],
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/scd_helper.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/scd_helper.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/specs.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/specs.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,40 +1,45 @@
 """
 Module for data structures that describe different types of aggregations that form features
 """
 from __future__ import annotations
 
-from typing import Any, List, Optional, Type, TypeVar, cast
+from typing import Any, List, Optional, Tuple, Type, TypeVar, cast
 
 import hashlib
 import json
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 
 import pandas as pd
 from bson import ObjectId
 from sqlglot.expressions import Expression, Select
 
 from featurebyte.enum import DBVarType, SourceType, StrEnum
+from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.generic import (
     AggregateAsAtNode,
     AggregateAsAtParameters,
     EventLookupParameters,
+    ForwardAggregateNode,
+    ForwardAggregateParameters,
     GroupByNode,
     ItemGroupbyNode,
     ItemGroupbyParameters,
     LookupNode,
     SCDLookupParameters,
 )
 from featurebyte.query_graph.node.mixin import BaseGroupbyParameters
 from featurebyte.query_graph.sql.adapter import BaseAdapter
 from featurebyte.query_graph.sql.common import apply_serving_names_mapping
 from featurebyte.query_graph.sql.tiling import InputColumn, get_aggregator
+from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
+from featurebyte.query_graph.transform.pruning import prune_query_graph
 
 NonTileBasedAggregationSpecT = TypeVar(
     "NonTileBasedAggregationSpecT", bound="NonTileBasedAggregationSpec"
 )
 
 FB_INTERNAL_COLUMN_PREFIX = "_fb_internal"
 
@@ -46,14 +51,15 @@
     """
 
     LATEST = "latest"
     LOOKUP = "lookup"
     WINDOW = "window"
     ITEM = "item"
     AS_AT = "as_at"
+    FORWARD = "forward"
 
 
 @dataclass  # type: ignore[misc]
 class AggregationSpec(ABC):
     """
     Base class of all aggregation specifications
     """
@@ -127,14 +133,17 @@
     aggregation_id: str
     keys: list[str]
     value_by: str | None
     merge_expr: str
     feature_name: str
     is_order_dependent: bool
     tile_value_columns: list[str]
+    dtype: DBVarType
+    pruned_graph: QueryGraphModel
+    pruned_node: Node
 
     @property
     def agg_result_name(self) -> str:
         """Column name of the aggregated result
 
         Returns
         -------
@@ -152,22 +161,25 @@
         if self.window is None:
             return AggregationType.LATEST
         return AggregationType.WINDOW
 
     @classmethod
     def from_groupby_query_node(
         cls,
+        graph: QueryGraphModel,
         groupby_node: Node,
         adapter: BaseAdapter,
         serving_names_mapping: dict[str, str] | None = None,
     ) -> list[TileBasedAggregationSpec]:
         """Construct an AggregationSpec from a query graph and groupby node
 
         Parameters
         ----------
+        graph : QueryGraphModel
+            Query graph
         groupby_node : Node
             Query graph node with groupby type
         adapter : BaseAdapter
             Instance of BaseAdapter
         serving_names_mapping : dict[str, str]
             Mapping from original serving name to new serving name
 
@@ -197,14 +209,19 @@
             spec.tile_column_name
             for spec in aggregator.tile(parent_column, params["aggregation_id"])
         ]
         for window, feature_name in zip(params["windows"], params["names"]):
             params = groupby_node.parameters.dict()
             if window is not None:
                 window = int(pd.Timedelta(window).total_seconds())
+            pruned_graph, pruned_node, dtype = cls._get_aggregation_column_type(
+                graph=graph,
+                groupby_node=groupby_node,
+                feature_name=feature_name,
+            )
             agg_spec = cls(
                 window=window,
                 frequency=params["frequency"],
                 time_modulo_frequency=params["time_modulo_frequency"],
                 blind_spot=params["blind_spot"],
                 tile_table_id=tile_table_id,
                 aggregation_id=aggregation_id,
@@ -213,19 +230,64 @@
                 serving_names_mapping=serving_names_mapping,
                 value_by=params["value_by"],
                 merge_expr=aggregator.merge(aggregation_id),
                 feature_name=feature_name,
                 is_order_dependent=aggregator.is_order_dependent,
                 tile_value_columns=tile_value_columns,
                 entity_ids=params["entity_ids"],
+                dtype=dtype,
+                pruned_graph=pruned_graph,
+                pruned_node=pruned_node,
             )
             aggregation_specs.append(agg_spec)
 
         return aggregation_specs
 
+    @classmethod
+    def _get_aggregation_column_type(
+        cls,
+        graph: QueryGraphModel,
+        groupby_node: Node,
+        feature_name: str,
+    ) -> Tuple[QueryGraphModel, Node, DBVarType]:
+        """Get the column type of the aggregation
+
+        Parameters
+        ----------
+        graph : QueryGraphModel
+            Query graph
+        groupby_node : Node
+            Groupby node
+        feature_name : str
+            Feature name of interest. Should be one of the features generated by the groupby node.
+
+        Returns
+        -------
+        DBVarType
+        """
+        project_node = graph.add_operation(
+            NodeType.PROJECT,
+            node_params={"columns": [feature_name]},
+            node_output_type=NodeOutputType.SERIES,
+            input_nodes=[groupby_node],
+        )
+        pruned_graph, node_name_map, _ = prune_query_graph(graph=graph, node=project_node)
+        pruned_node = pruned_graph.get_node_by_name(node_name_map[project_node.name])
+        op_struct = (
+            OperationStructureExtractor(graph=pruned_graph)
+            .extract(node=pruned_node)
+            .operation_structure_map[pruned_node.name]
+        )
+        aggregations = op_struct.aggregations
+        assert (
+            len(aggregations) == 1
+        ), f"Expect exactly one aggregation but got: {[agg.name for agg in aggregations]}"
+        aggregation = aggregations[0]
+        return pruned_graph, pruned_node, aggregation.dtype
+
 
 @dataclass
 class AggregationSource:
     """
     Represents the source of an aggregation. The aggregation is to be done via lookup,
     aggregate_asat, etc.
     """
@@ -636,14 +698,59 @@
                 event_parameters=params.event_parameters,
             )
             specs.append(spec)
         return specs
 
 
 @dataclass
+class ForwardAggregateSpec(NonTileBasedAggregationSpec):
+    """
+    ForwardAggregateSpec contains all information required to generate sql for a forward aggregate target.
+    """
+
+    parameters: ForwardAggregateParameters
+
+    @property
+    def agg_result_name(self) -> str:
+        return self.get_agg_result_name_from_groupby_parameters(self.parameters)
+
+    @property
+    def aggregation_type(self) -> AggregationType:
+        return AggregationType.FORWARD
+
+    def get_source_hash_parameters(self) -> dict[str, Any]:
+        params: dict[str, Any] = {"source_expr": self.source_expr.sql()}
+        parameters_dict = self.parameters.dict(exclude={"parent", "agg_func", "name"})
+        if parameters_dict.get("entity_ids") is not None:
+            parameters_dict["entity_ids"] = [
+                str(entity_id) for entity_id in parameters_dict["entity_ids"]
+            ]
+        params["parameters"] = parameters_dict
+        return params
+
+    @classmethod
+    def construct_specs(
+        cls: Type[NonTileBasedAggregationSpecT],
+        node: Node,
+        aggregation_source: AggregationSource,
+        serving_names_mapping: Optional[dict[str, str]],
+    ) -> list[ForwardAggregateSpec]:
+        assert isinstance(node, ForwardAggregateNode)
+        return [
+            ForwardAggregateSpec(
+                parameters=node.parameters,
+                aggregation_source=aggregation_source,
+                entity_ids=cast(List[ObjectId], node.parameters.entity_ids),
+                serving_names=node.parameters.serving_names,
+                serving_names_mapping=serving_names_mapping,
+            )
+        ]
+
+
+@dataclass
 class FeatureSpec:
     """
     Feature specification
     """
 
     feature_name: str
     feature_expr: Expression
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/template.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/template.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/tile_compute.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/tile_compute.py`

 * *Files 22% similar despite different names*

```diff
@@ -3,47 +3,71 @@
 """
 from __future__ import annotations
 
 from typing import Optional, cast
 
 import pandas as pd
 from sqlglot import expressions
-from sqlglot.expressions import Select, select
+from sqlglot.expressions import Expression, Select, select
 
-from featurebyte.enum import InternalName, SourceType
+from featurebyte.enum import InternalName, SourceType, SpecialColumnName
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
+from featurebyte.query_graph.sql.ast.datetime import TimedeltaExtractNode
+from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import CteStatements, quoted_identifier
 from featurebyte.query_graph.sql.interpreter import GraphInterpreter, TileGenSql
-from featurebyte.query_graph.sql.template import SqlExpressionTemplate
-from featurebyte.query_graph.sql.tile_util import update_maximum_window_size_dict
+from featurebyte.query_graph.sql.tile_util import (
+    construct_entity_table_query,
+    get_earliest_tile_start_date_expr,
+    get_previous_job_epoch_expr,
+    update_maximum_window_size_dict,
+)
 
 
 class OnDemandTileComputePlan:
     """Responsible for generating SQL to compute tiles for preview purpose
 
     Feature preview uses the same SQL query as historical feature requests. As a result, we need to
     build temporary tile tables that are required by the feature query. Actual tile tables are wide
     and consist of tile values from different transforms (aggregation_id). Based on the current
     implementation, for feature preview each groupby node has its own tile SQLs, so we need to
     perform some manipulation to construct the wide tile tables.
 
     Parameters
     ----------
-    point_in_time_list : list[str]
-        List of point in time values specified when calling preview
+    request_table_name : str
+        Name of request table to use
+    source_type : SourceType
+        Source type information
     """
 
-    def __init__(self, point_in_time_list: list[str], source_type: SourceType):
-        self.point_in_time_list = point_in_time_list
+    def __init__(
+        self,
+        request_table_name: str,
+        source_type: SourceType,
+    ):
         self.processed_agg_ids: set[str] = set()
         self.max_window_size_by_tile_id: dict[str, Optional[int]] = {}
         self.tile_infos: list[TileGenSql] = []
+        self.request_table_name = request_table_name
         self.source_type = source_type
 
+    @property
+    def adapter(self) -> BaseAdapter:
+        """
+        Returns an instance of BaseAdapter based on the source type
+
+        Returns
+        -------
+        BaseAdapter
+        """
+        return get_sql_adapter(self.source_type)
+
     def process_node(self, graph: QueryGraphModel, node: Node) -> None:
         """Update state given a query graph node
 
         Parameters
         ----------
         graph : QueryGraphModel
             Query graph
@@ -73,22 +97,21 @@
         dict[str, expressions.Expression]
         """
 
         tile_sqls: dict[str, Select] = {}
         prev_aliases: dict[str, str] = {}
 
         for tile_info in self.tile_infos:
-            # Convert template SQL with concrete start and end timestamps, based on the requested
-            # point-in-time and feature window sizes
-            tile_sql_expr = get_tile_sql_from_point_in_time(
-                sql_template=tile_info.sql_template,
-                point_in_time_list=self.point_in_time_list,
-                frequency=tile_info.frequency,
-                time_modulo_frequency=tile_info.time_modulo_frequency,
-                blind_spot=tile_info.blind_spot,
+            # Construct tile SQL using an entity table (a table with entity column(s) as the primary
+            # key representing the entities of interest) created from the request table and feature
+            # window sizes
+            tile_sql_expr = get_tile_sql(
+                adapter=self.adapter,
+                tile_info=tile_info,
+                request_table_name=self.request_table_name,
                 window=self.get_max_window_size(tile_info.tile_table_id),
             )
 
             # Build wide tile table by joining tile sqls with the same tile_table_id
             tile_table_id = tile_info.tile_table_id
             agg_id = tile_info.aggregation_id
             assert isinstance(tile_sql_expr, expressions.Subqueryable)
@@ -195,15 +218,15 @@
         Source type information
 
     Returns
     -------
     list[TileGenSql]
     """
     interpreter = GraphInterpreter(graph, source_type=source_type)
-    tile_gen_info = interpreter.construct_tile_gen_sql(node, is_on_demand=False)
+    tile_gen_info = interpreter.construct_tile_gen_sql(node, is_on_demand=True)
     return tile_gen_info
 
 
 def get_epoch_seconds(datetime_like: str) -> int:
     """Convert datetime string to UNIX timestamp
 
     Parameters
@@ -230,128 +253,102 @@
     Returns
     -------
     pd.Timestamp
     """
     return pd.Timestamp(num_seconds, unit="s")
 
 
-def compute_start_end_date_from_point_in_time(
-    point_in_time_list: list[str],
-    frequency: int,
-    time_modulo_frequency: int,
-    blind_spot: int,
-    num_tiles: Optional[int],
-) -> tuple[pd.Timestamp, pd.Timestamp]:
-    """Compute start and end dates to fill in the placeholders in tile SQL template
-
-    Parameters
-    ----------
-    point_in_time_list : list[str]
-        List of point in time. This will determine the range of data required to build tiles
-    frequency : int
-        Frequency in feature job setting
-    time_modulo_frequency : int
-        Time modulo frequency in feature job setting
-    blind_spot : int
-        Blind spot in feature job setting
-    num_tiles : int
-        Number of tiles required. This is calculated from feature window size.
-
-    Returns
-    -------
-    tuple[pd.Timestamp, pd.Timestamp]
-        Tuple of start and end dates
-    """
-    # Convert point in time to UNIX timestamp
-    point_in_time_epoch_seconds_list = [
-        get_epoch_seconds(point_in_time) for point_in_time in point_in_time_list
-    ]
-
-    def _compute_end_date_epoch_seconds(point_in_time_epoch_seconds: int) -> int:
-        """
-        Compute end date based on point in time in epoch seconds
-
-        Parameters
-        ----------
-        point_in_time_epoch_seconds : int
-            Point in time in epoch seconds
-
-        Returns
-        -------
-        int
-            End date
-        """
-        # Calculate the time of the latest feature job before point in time
-        last_job_index = (point_in_time_epoch_seconds - time_modulo_frequency) // frequency
-        last_job_time_epoch_seconds = last_job_index * frequency + time_modulo_frequency
-        # Compute end date in epoch seconds
-        return last_job_time_epoch_seconds - blind_spot
-
-    # Compute end date of latest point in time
-    latest_end_date_epoch_seconds = _compute_end_date_epoch_seconds(
-        max(point_in_time_epoch_seconds_list)
-    )
-
-    # Compute start date of earliest point in time based on number of tiles required
-    earliest_end_date_epoch_seconds = _compute_end_date_epoch_seconds(
-        min(point_in_time_epoch_seconds_list)
-    )
-    if num_tiles is not None:
-        start_date_epoch_seconds = earliest_end_date_epoch_seconds - num_tiles * frequency
-        start_date = epoch_seconds_to_timestamp(start_date_epoch_seconds)
-    else:
-        # In this case, we need a timestamp that is earlier than all possible event timestamps and
-        # aligns with the tile boundary. This computes the earliest possible timestamp at around the
-        # beginning of epoch.
-        start_date = epoch_seconds_to_timestamp(time_modulo_frequency - blind_spot)
-
-    return start_date, epoch_seconds_to_timestamp(latest_end_date_epoch_seconds)
-
-
-def get_tile_sql_from_point_in_time(
-    sql_template: SqlExpressionTemplate,
-    point_in_time_list: list[str],
-    frequency: int,
-    time_modulo_frequency: int,
-    blind_spot: int,
+def get_tile_sql(
+    adapter: BaseAdapter,
+    tile_info: TileGenSql,
+    request_table_name: str,
     window: Optional[int],
 ) -> Select:
-    """Fill in start date and end date placeholders for template tile SQL
+    """
+    Construct the SQL query that would compute the tiles for a given TileGenSql.
+
+    TileGenSql already contains the template for the tile SQL. This function fills in the entity
+    table placeholder so that the SQL is complete.
 
     Parameters
     ----------
-    sql_template : SqlExpressionTemplate
-        Tile SQL template expression
-    point_in_time_list : list[str]
-        List of point in time in the request
-    frequency : int
-        Frequency in feature job setting
-    time_modulo_frequency : int
-        Time modulo frequency in feature job setting
-    blind_spot : int
-        Blind spot in feature job setting
+    adapter : BaseAdapter
+        Instance of BaseAdapter for generating engine specific SQL
+    tile_info : TileGenSql
+        Tile table information
+    request_table_name : str
+        Name of the request table
     window : Optional[int]
-        Window size. If None, it means the feature requires unbounded window.
+        Window size in seconds. None for features with an unbounded window.
 
     Returns
     -------
     Select
     """
+
+    def get_tile_boundary(point_in_time_expr: Expression) -> Expression:
+        previous_job_epoch_expr = get_previous_job_epoch_expr(
+            adapter.to_epoch_seconds(point_in_time_expr), tile_info
+        )
+        return expressions.Anonymous(
+            this="TO_TIMESTAMP",
+            expressions=[expressions.Sub(this=previous_job_epoch_expr, expression=blind_spot)],
+        )
+
+    blind_spot = make_literal_value(tile_info.blind_spot)
+    time_modulo_frequency = make_literal_value(tile_info.time_modulo_frequency)
+
     if window:
-        num_tiles = int(window // frequency)
+        num_tiles = int(window // tile_info.frequency)
     else:
         num_tiles = None
-    start_date, end_date = compute_start_end_date_from_point_in_time(
-        point_in_time_list,
-        frequency=frequency,
-        time_modulo_frequency=time_modulo_frequency,
-        blind_spot=blind_spot,
-        num_tiles=num_tiles,
+
+    # Tile end date is determined from the latest point in time per entity
+    end_date_expr = get_tile_boundary(
+        expressions.Max(this=expressions.Identifier(this=SpecialColumnName.POINT_IN_TIME.value))
     )
-    out_expr = sql_template.render(
-        {
-            InternalName.TILE_START_DATE_SQL_PLACEHOLDER: str(start_date),
-            InternalName.TILE_END_DATE_SQL_PLACEHOLDER: str(end_date),
-        },
-        as_str=False,
+
+    if num_tiles:
+        minus_num_tiles_in_microseconds = expressions.Mul(
+            this=TimedeltaExtractNode.convert_timedelta_unit(
+                expressions.Mul(
+                    this=make_literal_value(num_tiles),
+                    expression=make_literal_value(tile_info.frequency),
+                ),
+                "second",
+                "microsecond",
+            ),
+            expression=make_literal_value(-1),
+        )
+        # Tile start date is determined from the earliest point in time per entity minus the largest
+        # feature window
+        start_date_expr = adapter.dateadd_microsecond(
+            minus_num_tiles_in_microseconds,
+            get_tile_boundary(
+                expressions.Min(
+                    this=expressions.Identifier(this=SpecialColumnName.POINT_IN_TIME.value)
+                )
+            ),
+        )
+    else:
+        start_date_expr = get_earliest_tile_start_date_expr(
+            adapter=adapter,
+            time_modulo_frequency=time_modulo_frequency,
+            blind_spot=blind_spot,
+        )
+
+    entity_table_expr = construct_entity_table_query(
+        tile_info=tile_info,
+        entity_source_expr=select().from_(quoted_identifier(request_table_name)),
+        start_date_expr=start_date_expr,
+        end_date_expr=end_date_expr,
+    )
+
+    return cast(
+        Select,
+        tile_info.sql_template.render(
+            {
+                InternalName.ENTITY_TABLE_SQL_PLACEHOLDER: entity_table_expr.subquery(),
+            },
+            as_str=False,
+        ),
     )
-    return cast(Select, out_expr)
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/sql/tiling.py` & `featurebyte-0.4.0/featurebyte/query_graph/sql/tiling.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/base.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/base.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/flattening.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/flattening.py`

 * *Files 5% similar despite different names*

```diff
@@ -47,19 +47,16 @@
                 input_node_name = nested_node_name_map[nested_input_node_name]
                 input_nodes.append(global_state.graph.get_node_by_name(input_node_name))
 
             if isinstance(nested_node, ProxyInputNode):
                 input_order = nested_node.parameters.input_order
                 nested_node_name_map[nested_node.name] = graph_input_nodes[input_order].name
             else:
-                inserted_node = global_state.graph.add_operation(
-                    node_type=nested_node.type,
-                    node_params=nested_node.parameters.dict(),
-                    node_output_type=nested_node.output_type,
-                    input_nodes=input_nodes,
+                inserted_node = global_state.graph.add_operation_node(
+                    node=nested_node, input_nodes=input_nodes
                 )
                 nested_node_name_map[nested_node.name] = inserted_node.name
 
             # node.parameters.output_node_name refers to the node name in the nested graph
             # we should map it into the flattened nested graph's node name for comparison.
             if nested_node.name == nested_flat_node_name_map[node.parameters.output_node_name]:
                 global_state.node_name_map[node.name] = nested_node_name_map[nested_node.name]
@@ -74,19 +71,16 @@
             for input_node_name in self.graph.get_input_node_names(node=node)
         ]
         if isinstance(node, BaseGraphNode) and self._should_flatten(global_state, node):
             self._flatten_nested_graph(
                 global_state=global_state, node=node, graph_input_nodes=input_nodes
             )
         else:
-            inserted_node = global_state.graph.add_operation(
-                node_type=node.type,
-                node_params=node.parameters.dict(),
-                node_output_type=node.output_type,
-                input_nodes=input_nodes,
+            inserted_node = global_state.graph.add_operation_node(
+                node=node, input_nodes=input_nodes
             )
             global_state.node_name_map[node.name] = inserted_node.name
 
     def transform(
         self, skip_flattening_graph_node_types: Optional[Set[GraphNodeType]] = None
     ) -> GraphNodeNameMap:
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/operation_structure.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/operation_structure.py`

 * *Files 0% similar despite different names*

```diff
@@ -139,15 +139,15 @@
 
     def extract(
         self,
         node: Node,
         proxy_input_operation_structures: Optional[List[OperationStructure]] = None,
         **kwargs: Any,
     ) -> OperationStructureInfo:
-        state_params: Dict[str, Any] = {"keep_all_source_columns": False}
+        state_params: Dict[str, Any] = {"keep_all_source_columns": True}
         if "keep_all_source_columns" in kwargs:
             # if this parameter is set, then the operation structure will keep all the source columns
             # even if they are not directly used in the operation (for example, event timestamp & entity columns
             # used in group by node)
             state_params["keep_all_source_columns"] = kwargs["keep_all_source_columns"]
         if proxy_input_operation_structures:
             state_params["proxy_input_operation_structures"] = proxy_input_operation_structures
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/pruning.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/pruning.py`

 * *Files 11% similar despite different names*

```diff
@@ -50,23 +50,21 @@
 
 
 def prune_query_graph(
     graph: QueryGraphModel,
     node: Node,
     target_columns: Optional[List[str]] = None,
     proxy_input_operation_structures: Optional[List[OperationStructure]] = None,
-    aggressive: bool = False,
+    operation_structure_info: Optional[OperationStructureInfo] = None,
 ) -> Tuple[QueryGraphModel, NodeNameMap, str]:
     """
-    Prune the query graph given target node. There are 2 modes in graph pruning:
-    - non-aggressive: prune the query graph by doing a graph traversal and keeps all the travelled nodes
-    - aggressive: in addition to the graph traversal, further prune the graph by removing the nodes that does not
-      contribute to final output and prunes the node parameters by removing unused parameters.
+    Prune the query graph given target node. In addition to the removing unused nodes, this function
+    further prune the graph by removing the node parameters that do not contribute to final output.
 
-    For aggressive graph pruning, there are 2 major steps:
+    There are 2 major steps in this graph pruning function:
     - graph structure pruning is performed first by removing useless graph node
     - node parameter pruning is performed then to prune the node parameters on the structure-pruned graph
 
     Parameters
     ----------
     graph: QueryGraphModel
         Query graph to be pruned
@@ -75,54 +73,49 @@
     target_columns: Optional[List[str]]
         Subset of the output columns of the target node, used to further prune the graph. If this parameter
         is provided, the mapped node (of the given node in the pruned graph) could be removed.
     proxy_input_operation_structures: Optional[List[OperationStructure]]
         All ProxyInputNode operation structures for nested graph pruning to map (operation structure of
         ProxyInputNode in the nested graph) to (the operation structure that the proxy input node refers
         to in the external graph)
-    aggressive: bool
-        Whether to enable aggressive pruning mode. For non-aggressive mode, all travelled nodes will be kept.
-        For aggressive mode, node could be removed if it does not contribute to the final output and node
-        parameters could be pruned if it is not used.
+    operation_structure_info: Optional[OperationStructureInfo]
+        Operation structure info for the given graph and node. If not provided, it will be extracted from the
+        graph.
 
     Returns
     -------
     Tuple[QueryGraphModel, NodeNameMap, str]
     """
-    pruned_graph, node_name_map = GraphStructurePruningExtractor(graph=graph).extract(
+    pruned_graph, node_name_map = GraphStructurePruningExtractor(
+        graph=graph, operation_structure_info=operation_structure_info
+    ).extract(
         node=node,
         target_columns=target_columns,
         proxy_input_operation_structures=proxy_input_operation_structures,
-        aggressive=aggressive,
     )
-    if aggressive:
-        # if aggressive mode enabled, further prune the node parameters
-        # first get the output node name in the pruned graph, use `map_and_resolve_node_name` as the target node
-        # could be pruned if `target_columns` is used (means that not all output columns of the target node are
-        # required).
-        output_node_name = map_and_resolve_node_name(
-            graph=graph, node_name_map=node_name_map, node_name=node.name
-        )
-        mapped_node = pruned_graph.get_node_by_name(node_name_map[output_node_name])
-        output_graph, pruned_node_name_map = NodeParametersPruningExtractor(
-            graph=pruned_graph
-        ).extract(
-            node=mapped_node,
-            target_columns=target_columns,
-            proxy_input_operation_structures=proxy_input_operation_structures,
-        )
+    # first get the output node name in the pruned graph, use `map_and_resolve_node_name` as the target node
+    # could be pruned if `target_columns` is used (means that not all output columns of the target node are
+    # required).
+    output_node_name = map_and_resolve_node_name(
+        graph=graph, node_name_map=node_name_map, node_name=node.name
+    )
+    mapped_node = pruned_graph.get_node_by_name(node_name_map[output_node_name])
+    output_graph, pruned_node_name_map = NodeParametersPruningExtractor(graph=pruned_graph).extract(
+        node=mapped_node,
+        target_columns=target_columns,
+        proxy_input_operation_structures=proxy_input_operation_structures,
+    )
 
-        # node_name_map => map (original graph node name) to (structure-pruned graph node name)
-        # pruned_node_name_map => map (structure-pruned graph node name) to (parameters-pruned graph node name)
-        # output_node_name_map => map (original graph node name) to (parameters-pruned graph node name)
-        output_node_name_map = {
-            key: pruned_node_name_map[value] for key, value in node_name_map.items()
-        }
-        return output_graph, output_node_name_map, output_node_name_map[output_node_name]
-    return pruned_graph, node_name_map, node_name_map[node.name]
+    # node_name_map => map (original graph node name) to (structure-pruned graph node name)
+    # pruned_node_name_map => map (structure-pruned graph node name) to (parameters-pruned graph node name)
+    # output_node_name_map => map (original graph node name) to (parameters-pruned graph node name)
+    output_node_name_map = {
+        key: pruned_node_name_map[value] for key, value in node_name_map.items()
+    }
+    return output_graph, output_node_name_map, output_node_name_map[output_node_name]
 
 
 class NodeParametersPruningGlobalState(OperationStructureInfo):
     """NodeParametersPruningGlobalState class"""
 
     def __init__(
         self,
@@ -201,19 +194,16 @@
 
             node = node.prune(
                 target_node_input_order_pairs=target_node_input_order_pairs,
                 input_operation_structures=input_op_structs,
             )
 
         # add the pruned node back to a graph to reconstruct a parameters-pruned graph
-        node_pruned = global_state.graph.add_operation(
-            node_type=node.type,
-            node_params=node.parameters.dict(),
-            node_output_type=node.output_type,
-            input_nodes=mapped_input_nodes,
+        node_pruned = global_state.graph.add_operation_node(
+            node=node, input_nodes=mapped_input_nodes
         )
         global_state.node_name_map[node.name] = node_pruned.name
         return super()._post_compute(
             branch_state=branch_state,
             global_state=global_state,
             node=node_pruned,
             inputs=inputs,
@@ -254,15 +244,14 @@
     def __init__(
         self,
         node_names: Set[str],
         target_node_name: str,
         graph: Optional[QueryGraphModel] = None,
         node_name_map: Optional[NodeNameMap] = None,
         target_columns: Optional[List[str]] = None,
-        aggressive: bool = False,
         **kwargs: Any,
     ):
         super().__init__(**kwargs)
 
         # variables to store some internal pruning info
         self.node_names = node_names
 
@@ -270,46 +259,44 @@
         self.graph = graph or QueryGraphModel()
         self.node_name_map = node_name_map or {}
 
         # variables to track output node & target columns
         self.target_columns = target_columns
         self.target_node_name = target_node_name
 
-        # variables to control pruning behavior
-        self.aggressive = aggressive
-
 
 class GraphStructurePruningExtractor(
     BaseGraphExtractor[GraphNodeNameMap, GraphPruningBranchState, GraphPruningGlobalState]
 ):
     """
     GraphStructurePruningExtractor is used to prune the graph structure (remove redundant nodes).
     This pruning operation travels the graph from the target node back to input nodes (uni-direction).
-    For non-aggressive pruning, all the travelled nodes will be kept. For aggressive pruning, the travelled
-    nodes will be removed if it does not contribute to the final output.
     """
 
+    def __init__(
+        self,
+        graph: QueryGraphModel,
+        operation_structure_info: Optional[OperationStructureInfo] = None,
+    ):
+        super().__init__(graph=graph)
+        self.operation_structure_info = operation_structure_info
+
     def _pre_compute(
         self,
         branch_state: GraphPruningBranchState,
         global_state: GraphPruningGlobalState,
         node: Node,
         input_node_names: List[str],
     ) -> Tuple[List[str], bool]:
-        if (
-            global_state.aggressive
-            and isinstance(node, BasePrunableNode)
-            and node.name not in global_state.node_names
-        ):
+        if isinstance(node, BasePrunableNode) and node.name not in global_state.node_names:
             if isinstance(node, BaseGraphNode) and not node.is_prunable:
                 # graph node is not prunable
                 return input_node_names, False
 
             # prune the graph structure if
-            # - pruning mode is aggressive (means that travelled node can be removed)
             # - node is prunable
             # - node does not contribute to the final output
             selected_node_name = node.resolve_node_pruned(input_node_names)
             return [selected_node_name], True
         return input_node_names, False
 
     def _in_compute(
@@ -340,15 +327,14 @@
 
     @classmethod
     def _prune_nested_graph(
         cls,
         node: BaseGraphNode,
         target_columns: Optional[List[str]],
         proxy_input_operation_structures: List[OperationStructure],
-        aggressive: bool,
     ) -> Node:
         output_node_name = node.parameters.output_node_name
         graph = node.parameters.graph
         if node.parameters.type not in GraphNodeType.view_graph_node_types():
             # skip view graph node pruning (as it will be pruned in view construction service)
             # if we prune the view graph node here, it will cause the issue when the feature is created
             # from the item table column only. In this case, the proxy input node which represents the
@@ -356,15 +342,14 @@
             # requires all the proxy input nodes to be present in the nested graph.
             nested_target_node = graph.get_node_by_name(output_node_name)
             graph, _, output_node_name = prune_query_graph(
                 graph=graph,
                 node=nested_target_node,
                 target_columns=target_columns,
                 proxy_input_operation_structures=proxy_input_operation_structures,
-                aggressive=aggressive,
             )
 
         return node.clone(
             parameters={
                 "graph": graph,
                 "output_node_name": output_node_name,
                 "type": node.parameters.type,
@@ -401,49 +386,48 @@
         # construct mapped input_node_names (from original graph to pruned graph)
         mapped_input_nodes = []
         for input_node_name in input_node_names:
             mapped_input_node_name = global_state.node_name_map[input_node_name]
             mapped_input_nodes.append(global_state.graph.get_node_by_name(mapped_input_node_name))
 
         # add the node back to the pruned graph
-        if global_state.aggressive and isinstance(node, BaseGraphNode):
+        if isinstance(node, BaseGraphNode):
             proxy_input_operation_structures = [
                 global_state.operation_structure_map[node_name]
                 for node_name in self.graph.get_input_node_names(node=node)
             ]
             target_columns = self._prepare_target_columns(node=node, global_state=global_state)
             node = self._prune_nested_graph(
                 node=node,
                 target_columns=target_columns,
                 proxy_input_operation_structures=proxy_input_operation_structures,
-                aggressive=global_state.aggressive,
             )
 
-        node_pruned = global_state.graph.add_operation(
-            node_type=node.type,
-            node_params=node.parameters.dict(),
-            node_output_type=node.output_type,
-            input_nodes=mapped_input_nodes,
+        node_pruned = global_state.graph.add_operation_node(
+            node=node, input_nodes=mapped_input_nodes
         )
 
         # update the containers to store the mapped node name & processed nodes information
         global_state.node_name_map[node.name] = node_pruned.name
 
     def extract(
         self,
         node: Node,
         target_columns: Optional[List[str]] = None,
         proxy_input_operation_structures: Optional[List[OperationStructure]] = None,
-        aggressive: bool = False,
         **kwargs: Any,
     ) -> GraphNodeNameMap:
-        op_struct_info = OperationStructureExtractor(graph=self.graph).extract(
-            node=node,
-            proxy_input_operation_structures=proxy_input_operation_structures,
-        )
+        if self.operation_structure_info is None:
+            op_struct_info = OperationStructureExtractor(graph=self.graph).extract(
+                node=node,
+                proxy_input_operation_structures=proxy_input_operation_structures,
+            )
+        else:
+            op_struct_info = self.operation_structure_info
+
         operation_structure = op_struct_info.operation_structure_map[node.name]
         temp_node_name = "temp"
         if target_columns:
             # subset the operation structure info by keeping only selected columns (using project node)
             temp_node = ProjectNode(
                 name=temp_node_name,
                 parameters={"columns": target_columns},
@@ -457,15 +441,14 @@
 
         global_state = GraphPruningGlobalState(
             node_names=operation_structure.all_node_names.difference([temp_node_name]),
             edges_map=op_struct_info.edges_map,
             target_node_name=node.name,
             target_columns=target_columns,
             operation_structure_map=op_struct_info.operation_structure_map,
-            aggressive=aggressive,
         )
         branch_state = GraphPruningBranchState()
         self._extract(
             node=node,
             branch_state=branch_state,
             global_state=global_state,
             topological_order_map=self.graph.node_topological_order_map,
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/reconstruction.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/reconstruction.py`

 * *Files 2% similar despite different names*

```diff
@@ -166,15 +166,15 @@
         node=input_node,
         target_columns=list(
             temp_node.get_required_input_columns(
                 input_index=0,
                 available_column_names=input_operation_structure.output_column_names,
             )
         ),
-        aggressive=True,
+        operation_structure_info=operation_structure_info,
     )
 
     # flatten the pruned graph before further operations
     flat_graph, node_name_map = GraphFlatteningTransformer(graph=pruned_graph).transform()
     mapped_input_node_name = node_name_map[pruned_input_node_name]
     additional_parameters = node_cls.derive_parameters_post_prune(
         graph=graph,
@@ -229,19 +229,16 @@
             inserted_node = add_pruning_sensitive_operation(
                 graph=global_state.graph,
                 node_cls=PRUNING_SENSITIVE_NODE_MAP[node.type],  # type: ignore
                 node_params=node_to_insert.parameters.dict(),
                 input_node=input_nodes[0],
             )
         else:
-            inserted_node = global_state.graph.add_operation(
-                node_type=node.type,
-                node_params=node_to_insert.parameters.dict(),
-                node_output_type=node_to_insert.output_type,
-                input_nodes=input_nodes,
+            inserted_node = global_state.graph.add_operation_node(
+                node=node_to_insert, input_nodes=input_nodes
             )
 
         # update node name mapping between original graph & reconstructed graph
         global_state.node_name_map[node.name] = inserted_node.name
 
     def transform(
         self, node_name_to_replacement_node: Dict[str, NodeT], regenerate_groupby_hash: bool
```

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/transform/sdk_code.py` & `featurebyte-0.4.0/featurebyte/query_graph/transform/sdk_code.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/query_graph/util.py` & `featurebyte-0.4.0/featurebyte/query_graph/util.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/batch_feature_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/batch_feature_table/api.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
 from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate, BatchFeatureTableList
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import BatchFeatureTableInfo
 from featurebyte.schema.task import Task
 
 router = APIRouter(prefix="/batch_feature_table")
 
 
 @router.post("", response_model=Task, status_code=HTTPStatus.CREATED)
@@ -142,7 +143,24 @@
     """
     controller = request.state.app_container.batch_feature_table_controller
     result: StreamingResponse = await controller.download_materialized_table(
         document_id=batch_feature_table_id,
         get_credential=request.state.get_credential,
     )
     return result
+
+
+@router.patch("/{batch_feature_table_id}/description", response_model=BatchFeatureTableModel)
+async def update_batch_feature_table_description(
+    request: Request,
+    batch_feature_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> BatchFeatureTableModel:
+    """
+    Update batch_feature_table description
+    """
+    controller = request.state.app_container.batch_feature_table_controller
+    batch_feature_table: BatchFeatureTableModel = await controller.update_description(
+        document_id=batch_feature_table_id,
+        description=data.description,
+    )
+    return batch_feature_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/batch_request_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/batch_request_table/api.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate, BatchRequestTableList
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import BatchRequestTableInfo
 from featurebyte.schema.task import Task
 
 router = APIRouter(prefix="/batch_request_table")
 
 
 @router.post("", response_model=Task, status_code=HTTPStatus.CREATED)
@@ -144,7 +145,24 @@
     """
     controller = request.state.app_container.batch_request_table_controller
     result: StreamingResponse = await controller.download_materialized_table(
         document_id=batch_request_table_id,
         get_credential=request.state.get_credential,
     )
     return result
+
+
+@router.patch("/{batch_request_table_id}/description", response_model=BatchRequestTableModel)
+async def update_batch_request_table_description(
+    request: Request,
+    batch_request_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> BatchRequestTableModel:
+    """
+    Update batch_request_table description
+    """
+    controller = request.state.app_container.batch_request_table_controller
+    batch_request_table: BatchRequestTableModel = await controller.update_description(
+        document_id=batch_request_table_id,
+        description=data.description,
+    )
+    return batch_request_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/batch_request_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/batch_request_table/controller.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from featurebyte.routes.common.base_materialized_table import BaseMaterializedTableController
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate, BatchRequestTableList
 from featurebyte.schema.info import BatchRequestTableInfo
 from featurebyte.schema.task import Task
 from featurebyte.service.batch_feature_table import BatchFeatureTableService
 from featurebyte.service.batch_request_table import BatchRequestTableService
-from featurebyte.service.info import InfoService
+from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.preview import PreviewService
 from featurebyte.service.validator.materialized_table_delete import check_delete_batch_request_table
 
 
 class BatchRequestTableController(
     BaseMaterializedTableController[
         BatchRequestTableModel, BatchRequestTableService, BatchRequestTableList
@@ -27,24 +27,24 @@
     BatchRequestTable Controller
     """
 
     paginated_document_class = BatchRequestTableList
 
     def __init__(
         self,
-        service: BatchRequestTableService,
+        batch_request_table_service: BatchRequestTableService,
         preview_service: PreviewService,
         batch_feature_table_service: BatchFeatureTableService,
-        info_service: InfoService,
         task_controller: TaskController,
+        feature_store_service: FeatureStoreService,
     ):
-        super().__init__(service=service, preview_service=preview_service)
+        super().__init__(service=batch_request_table_service, preview_service=preview_service)
         self.batch_feature_table_service = batch_feature_table_service
-        self.info_service = info_service
         self.task_controller = task_controller
+        self.feature_store_service = feature_store_service
 
     async def create_batch_request_table(
         self,
         data: BatchRequestTableCreate,
     ) -> Task:
         """
         Create BatchRequestTable by submitting a materialization task
@@ -80,11 +80,21 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         BatchRequestTableInfo
         """
-        info_document = await self.info_service.get_batch_request_table_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        batch_request_table = await self.service.get_document(document_id=document_id)
+        feature_store = await self.feature_store_service.get_document(
+            document_id=batch_request_table.location.feature_store_id
+        )
+        return BatchRequestTableInfo(
+            name=batch_request_table.name,
+            type=batch_request_table.request_input.type,
+            feature_store_name=feature_store.name,
+            table_details=batch_request_table.location.table_details,
+            created_at=batch_request_table.created_at,
+            updated_at=batch_request_table.updated_at,
+            description=batch_request_table.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/catalog/api.py` & `featurebyte-0.4.0/featurebyte/routes/catalog/api.py`

 * *Files 10% similar despite different names*

```diff
@@ -19,14 +19,15 @@
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
 from featurebyte.schema.catalog import CatalogCreate, CatalogList, CatalogUpdate
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import CatalogInfo
 
 router = APIRouter(prefix="/catalog")
 
 
 @router.post("", response_model=CatalogModel, status_code=HTTPStatus.CREATED)
 async def create_catalog(request: Request, data: CatalogCreate) -> CatalogModel:
@@ -152,7 +153,24 @@
     """
     controller = request.state.app_container.catalog_controller
     info = await controller.get_info(
         document_id=catalog_id,
         verbose=verbose,
     )
     return cast(CatalogInfo, info)
+
+
+@router.patch("/{catalog_id}/description", response_model=CatalogModel)
+async def update_catalog_description(
+    request: Request,
+    catalog_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> CatalogModel:
+    """
+    Update catalog description
+    """
+    controller = request.state.app_container.catalog_controller
+    catalog: CatalogModel = await controller.update_description(
+        document_id=catalog_id,
+        description=data.description,
+    )
+    return catalog
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/catalog/controller.py` & `featurebyte-0.4.0/featurebyte/routes/catalog/controller.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,34 +11,25 @@
     CatalogCreate,
     CatalogList,
     CatalogServiceUpdate,
     CatalogUpdate,
 )
 from featurebyte.schema.info import CatalogInfo
 from featurebyte.service.catalog import CatalogService
-from featurebyte.service.info import InfoService
 
 
 class CatalogController(
     BaseDocumentController[CatalogModel, CatalogService, CatalogList],
 ):
     """
     Catalog Controller
     """
 
     paginated_document_class = CatalogList
 
-    def __init__(
-        self,
-        service: CatalogService,
-        info_service: InfoService,
-    ):
-        super().__init__(service)
-        self.info_service = info_service
-
     async def create_catalog(
         self,
         data: CatalogCreate,
     ) -> CatalogModel:
         """
         Create Catalog at persistent
 
@@ -96,11 +87,15 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         InfoDocument
         """
-        info_document = await self.info_service.get_catalog_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        catalog = await self.service.get_document(document_id=document_id)
+        return CatalogInfo(
+            name=catalog.name,
+            created_at=catalog.created_at,
+            updated_at=catalog.updated_at,
+            description=catalog.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/common/base.py` & `featurebyte-0.4.0/featurebyte/routes/common/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,34 +12,39 @@
 from featurebyte.models.relationship_analysis import derive_primary_entity
 from featurebyte.schema.common.base import PaginationMixin
 from featurebyte.service.batch_feature_table import BatchFeatureTableService
 from featurebyte.service.batch_request_table import BatchRequestTableService
 from featurebyte.service.catalog import CatalogService
 from featurebyte.service.context import ContextService
 from featurebyte.service.credential import CredentialService
-from featurebyte.service.deployment import DeploymentService
+from featurebyte.service.deployment import AllDeploymentService, DeploymentService
 from featurebyte.service.dimension_table import DimensionTableService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.event_table import EventTableService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_job_setting_analysis import FeatureJobSettingAnalysisService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.item_table import ItemTableService
-from featurebyte.service.mixin import Document
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE, Document
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.periodic_task import PeriodicTaskService
 from featurebyte.service.relationship import ParentT, RelationshipService
 from featurebyte.service.relationship_info import RelationshipInfoService
 from featurebyte.service.scd_table import SCDTableService
 from featurebyte.service.semantic import SemanticService
+from featurebyte.service.static_source_table import StaticSourceTableService
 from featurebyte.service.table import TableService
+from featurebyte.service.target import TargetService
+from featurebyte.service.target_namespace import TargetNamespaceService
+from featurebyte.service.target_table import TargetTableService
+from featurebyte.service.user_defined_function import UserDefinedFunctionService
 
 PaginatedDocument = TypeVar("PaginatedDocument", bound=PaginationMixin)
 DocumentServiceT = TypeVar(
     "DocumentServiceT",
     CredentialService,
     FeatureStoreService,
     ContextService,
@@ -57,16 +62,22 @@
     FeatureJobSettingAnalysisService,
     CatalogService,
     RelationshipInfoService,
     PeriodicTaskService,
     ObservationTableService,
     HistoricalFeatureTableService,
     BatchRequestTableService,
+    StaticSourceTableService,
     BatchFeatureTableService,
     DeploymentService,
+    TargetService,
+    TargetNamespaceService,
+    TargetTableService,
+    UserDefinedFunctionService,
+    AllDeploymentService,
 )
 
 
 class BaseDocumentController(Generic[Document, DocumentServiceT, PaginatedDocument]):
     """
     BaseDocumentController for API routes
     """
@@ -105,15 +116,15 @@
             exception_detail=exception_detail,
         )
         return cast(Document, document)
 
     async def list(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         **kwargs: Any,
     ) -> PaginatedDocument:
         """
         List documents stored at persistent (GitDB or MongoDB)
 
@@ -131,29 +142,29 @@
             Additional keyword arguments
 
         Returns
         -------
         PaginationDocument
             List of documents fulfilled the filtering condition
         """
-        document_data = await self.service.list_documents(
+        document_data = await self.service.list_documents_as_dict(
             page=page,
             page_size=page_size,
             sort_by=sort_by,
             sort_dir=sort_dir,
             **kwargs,
         )
         return cast(PaginatedDocument, self.paginated_document_class(**document_data))
 
     async def list_audit(
         self,
         document_id: ObjectId,
         query_filter: Optional[QueryFilter] = None,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         **kwargs: Any,
     ) -> AuditDocumentList:
         """
         List audit records stored at persistent (GitDB or MongoDB)
 
@@ -211,14 +222,40 @@
             List of historical values for a field in the document
         """
         document_data = await self.service.list_document_field_history(
             document_id=document_id, field=field
         )
         return document_data
 
+    async def update_description(
+        self,
+        document_id: ObjectId,
+        description: Optional[str],
+    ) -> Document:
+        """
+        Update document description
+
+        Parameters
+        ----------
+        document_id: ObjectId
+            Document ID
+        description: Optional[str]
+            Document description
+
+        Returns
+        -------
+        Document
+            Document model with updated description
+        """
+        await self.service.update_document_description(
+            document_id=document_id,
+            description=description,
+        )
+        return await self.get(document_id=document_id)
+
 
 class RelationshipMixin(Generic[Document, ParentT]):
     """
     RelationshipMixin contains methods to add & remove parent relationship
     """
 
     relationship_service: RelationshipService
@@ -265,68 +302,69 @@
         document = await self.relationship_service.remove_relationship(
             parent_id=parent_id,
             child_id=child_id,
         )
         return cast(Document, document)
 
 
-class DerivePrimaryEntityMixin:
+class DerivePrimaryEntityHelper:
     """Mixin class to derive primary entity from a list of entities"""
 
-    entity_service: EntityService
+    def __init__(self, entity_service: EntityService):
+        self.entity_service = entity_service
 
     async def get_entity_id_to_entity(
         self, doc_list: list[dict[str, Any]]
-    ) -> dict[ObjectId, dict[str, Any]]:
+    ) -> dict[ObjectId, EntityModel]:
         """
         Construct entity ID to entity dictionary mapping
 
         Parameters
         ----------
         doc_list: list[dict[str, Any]]
             List of document dictionary (document should contain entity_ids field)
 
         Returns
         -------
-        dict[ObjectId, dict[str, Any]]
-            Dictionary mapping entity ID to entity dictionary
+        dict[ObjectId, EntityModel]
+            Dictionary mapping entity ID to entity model
         """
         entity_ids = set()
         for doc in doc_list:
             entity_ids.update(doc["entity_ids"])
 
-        entity_id_to_entity = {}
-        async for entity_dict in self.entity_service.list_documents_iterator(
+        entity_id_to_entity: dict[ObjectId, EntityModel] = {}
+        async for entity in self.entity_service.list_documents_iterator(
             query_filter={"_id": {"$in": list(entity_ids)}}
         ):
-            entity_id_to_entity[entity_dict["_id"]] = entity_dict
+            entity_id_to_entity[entity.id] = entity
         return entity_id_to_entity
 
     async def derive_primary_entity_ids(
         self,
         entity_ids: Sequence[ObjectId],
-        entity_id_to_entity: Optional[dict[ObjectId, dict[str, Any]]] = None,
+        entity_id_to_entity: Optional[dict[ObjectId, EntityModel]] = None,
     ) -> list[ObjectId]:
         """
         Derive primary entity IDs from a list of entity IDs
 
         Parameters
         ----------
         entity_ids: Sequence[ObjectId]
             List of entity IDs
-        entity_id_to_entity: Optional[dict[ObjectId, dict[str, Any]]]
+        entity_id_to_entity: Optional[dict[ObjectId, EntityModel]]
             Dictionary mapping entity ID to entity dictionary
 
         Returns
         -------
         list[ObjectId]
         """
         if entity_id_to_entity is None:
             entity_id_to_entity = {
-                entity_dict["_id"]: entity_dict
-                async for entity_dict in self.entity_service.list_documents_iterator(
+                entity.id: entity
+                async for entity in self.entity_service.list_documents_iterator(
                     query_filter={"_id": {"$in": entity_ids}},
                 )
             }
 
-        entities = [EntityModel(**entity_id_to_entity[entity_id]) for entity_id in entity_ids]
+        entities = [entity_id_to_entity[entity_id] for entity_id in entity_ids]
         return [entity.id for entity in derive_primary_entity(entities=entities)]
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/common/base_materialized_table.py` & `featurebyte-0.4.0/featurebyte/routes/common/base_materialized_table.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,36 +6,44 @@
 from bson import ObjectId
 from starlette.responses import StreamingResponse
 
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.models.observation_table import ObservationTableModel
+from featurebyte.models.static_source_table import StaticSourceTableModel
+from featurebyte.models.target_table import TargetTableModel
 from featurebyte.routes.common.base import BaseDocumentController, PaginatedDocument
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.task import Task
 from featurebyte.service.batch_feature_table import BatchFeatureTableService
 from featurebyte.service.batch_request_table import BatchRequestTableService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.preview import PreviewService
+from featurebyte.service.static_source_table import StaticSourceTableService
+from featurebyte.service.target_table import TargetTableService
 
 MaterializedTableDocumentT = TypeVar(
     "MaterializedTableDocumentT",
     ObservationTableModel,
     HistoricalFeatureTableModel,
     BatchRequestTableModel,
     BatchFeatureTableModel,
+    StaticSourceTableModel,
+    TargetTableModel,
 )
 MaterializedTableDocumentServiceT = TypeVar(
     "MaterializedTableDocumentServiceT",
     ObservationTableService,
     HistoricalFeatureTableService,
     BatchRequestTableService,
     BatchFeatureTableService,
+    StaticSourceTableService,
+    TargetTableService,
 )
 
 
 class BaseMaterializedTableController(
     BaseDocumentController[
         MaterializedTableDocumentT, MaterializedTableDocumentServiceT, PaginatedDocument
     ]
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/common/base_table.py` & `featurebyte-0.4.0/featurebyte/routes/common/base_table.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,15 +14,14 @@
 from featurebyte.models.item_table import ItemTableModel
 from featurebyte.models.scd_table import SCDTableModel
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.routes.common.base import BaseDocumentController, PaginatedDocument
 from featurebyte.schema.table import TableServiceUpdate, TableUpdate
 from featurebyte.service.dimension_table import DimensionTableService
 from featurebyte.service.event_table import EventTableService
-from featurebyte.service.info import InfoService
 from featurebyte.service.item_table import ItemTableService
 from featurebyte.service.scd_table import SCDTableService
 from featurebyte.service.semantic import SemanticService
 from featurebyte.service.table_columns_info import TableColumnsInfoService, TableDocumentService
 from featurebyte.service.table_status import TableStatusService
 
 TableDocumentT = TypeVar(
@@ -48,21 +47,19 @@
 
     def __init__(
         self,
         service: TableDocumentService,
         table_columns_info_service: TableColumnsInfoService,
         table_status_service: TableStatusService,
         semantic_service: SemanticService,
-        info_service: InfoService,
     ):
         super().__init__(service)  # type: ignore[arg-type]
         self.table_column_info_service = table_columns_info_service
         self.table_status_service = table_status_service
         self.semantic_service = semantic_service
-        self.info_service = info_service
 
     @abstractmethod
     async def _get_column_semantic_map(self, document: TableDocumentT) -> dict[str, Any]:
         """
         Construct column name to semantic mapping
 
         Parameters
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/common/schema.py` & `featurebyte-0.4.0/featurebyte/routes/common/schema.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/context/api.py` & `featurebyte-0.4.0/featurebyte/routes/context/api.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.context import ContextCreate, ContextList, ContextUpdate
 
 router = APIRouter(prefix="/context")
 
 
 @router.post("", response_model=ContextModel, status_code=HTTPStatus.CREATED)
 async def create_context(request: Request, data: ContextCreate) -> ContextModel:
@@ -102,7 +103,24 @@
         page=page,
         page_size=page_size,
         sort_by=sort_by,
         sort_dir=sort_dir,
         search=search,
     )
     return audit_doc_list
+
+
+@router.patch("/{context_id}/description", response_model=ContextModel)
+async def update_context_description(
+    request: Request,
+    context_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> ContextModel:
+    """
+    Update context description
+    """
+    controller = request.state.app_container.context_controller
+    context: ContextModel = await controller.update_description(
+        document_id=context_id,
+        description=data.description,
+    )
+    return context
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/context/controller.py` & `featurebyte-0.4.0/featurebyte/routes/context/controller.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/credential/api.py` & `featurebyte-0.4.0/featurebyte/routes/credential/api.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
-from featurebyte.schema.common.base import DeleteResponse
+from featurebyte.schema.common.base import DeleteResponse, DescriptionUpdate
 from featurebyte.schema.credential import (
     CredentialCreate,
     CredentialList,
     CredentialRead,
     CredentialUpdate,
 )
 from featurebyte.schema.info import CredentialInfo
@@ -169,7 +169,24 @@
     Delete credential
     """
     controller = request.state.app_container.credential_controller
     await controller.delete_credential(
         credential_id=credential_id,
     )
     return DeleteResponse()
+
+
+@router.patch("/{credential_id}/description", response_model=CredentialRead)
+async def update_credential_description(
+    request: Request,
+    credential_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> CredentialRead:
+    """
+    Update credential description
+    """
+    controller = request.state.app_container.credential_controller
+    credential: CredentialRead = await controller.update_description(
+        document_id=credential_id,
+        description=data.description,
+    )
+    return credential
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/credential/controller.py` & `featurebyte-0.4.0/featurebyte/routes/credential/controller.py`

 * *Files 13% similar despite different names*

```diff
@@ -13,33 +13,31 @@
     CredentialList,
     CredentialRead,
     CredentialServiceUpdate,
     CredentialUpdate,
 )
 from featurebyte.schema.info import CredentialInfo
 from featurebyte.service.credential import CredentialService
-from featurebyte.service.info import InfoService
+from featurebyte.service.feature_store import FeatureStoreService
 
 
 class CredentialController(
     BaseDocumentController[CredentialModel, CredentialService, CredentialList],
 ):
     """
     Credential controller
     """
 
     paginated_document_class = CredentialList
 
     def __init__(
-        self,
-        service: CredentialService,
-        info_service: InfoService,
+        self, credential_service: CredentialService, feature_store_service: FeatureStoreService
     ):
-        super().__init__(service)
-        self.info_service = info_service
+        super().__init__(credential_service)
+        self.feature_store_service = feature_store_service
 
     async def create_credential(
         self,
         data: CredentialCreate,
     ) -> CredentialRead:
         """
         Create credential
@@ -51,15 +49,15 @@
 
         Returns
         -------
         CredentialRead
             CredentialRead object
         """
         document = await self.service.create_document(data=data)
-        return CredentialRead(**document.json_dict())
+        return CredentialRead(**document.dict(by_alias=True))
 
     async def update_credential(
         self,
         credential_id: PydanticObjectId,
         data: CredentialUpdate,
     ) -> CredentialRead:
         """
@@ -75,18 +73,18 @@
         Returns
         -------
         CredentialRead
             CredentialRead object
         """
         document = await self.service.update_document(
             document_id=credential_id,
-            data=CredentialServiceUpdate(**data.dict()),
+            data=CredentialServiceUpdate(**data.dict(by_alias=True)),
         )
         assert document is not None
-        return CredentialRead(**document.json_dict())
+        return CredentialRead(**document.dict(by_alias=True))
 
     async def delete_credential(
         self,
         credential_id: PydanticObjectId,
     ) -> None:
         """
         Delete credential
@@ -113,11 +111,24 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         CredentialInfo
         """
-        info_document = await self.info_service.get_credential_info(
-            document_id=credential_id, verbose=verbose
+        _ = verbose
+        credential = await self.service.get_document(document_id=credential_id)
+        return CredentialInfo(
+            name=credential.name,
+            feature_store_info=await self.feature_store_service.get_feature_store_info(
+                document_id=credential.feature_store_id, verbose=verbose
+            ),
+            database_credential_type=credential.database_credential.type
+            if credential.database_credential
+            else None,
+            storage_credential_type=credential.storage_credential.type
+            if credential.storage_credential
+            else None,
+            created_at=credential.created_at,
+            updated_at=credential.updated_at,
+            description=credential.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/deployment/api.py` & `featurebyte-0.4.0/featurebyte/routes/deployment/api.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.deployment import (
     AllDeploymentList,
     DeploymentCreate,
     DeploymentList,
     DeploymentSummary,
     DeploymentUpdate,
     OnlineFeaturesResponseModel,
@@ -163,15 +164,15 @@
     sort_by: Optional[str] = SortByQuery,
     sort_dir: Optional[str] = SortDirQuery,
     enabled: Optional[bool] = Query(default=None),
 ) -> AllDeploymentList:
     """
     List All Deployments (Regardless of Catalog)
     """
-    controller = request.state.app_container.deployment_controller
+    controller = request.state.app_container.all_deployment_controller
     deployment_list: AllDeploymentList = await controller.list_all_deployments(
         page=page,
         page_size=page_size,
         sort_by=sort_by,
         sort_dir=sort_dir,
         enabled=enabled,
     )
@@ -181,10 +182,27 @@
 @router.get("/summary/", response_model=DeploymentSummary)
 async def get_deployment_summary(
     request: Request,
 ) -> DeploymentSummary:
     """
     Get Deployment Summary
     """
-    controller = request.state.app_container.deployment_controller
+    controller = request.state.app_container.all_deployment_controller
     deployment_summary: DeploymentSummary = await controller.get_deployment_summary()
     return deployment_summary
+
+
+@router.patch("/{deployment_id}/description", response_model=DeploymentModel)
+async def update_deployment_description(
+    request: Request,
+    deployment_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> DeploymentModel:
+    """
+    Update deployment description
+    """
+    controller = request.state.app_container.deployment_controller
+    deployment: DeploymentModel = await controller.update_description(
+        document_id=deployment_id,
+        description=data.description,
+    )
+    return deployment
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/deployment/controller.py` & `featurebyte-0.4.0/featurebyte/routes/deployment/controller.py`

 * *Files 10% similar despite different names*

```diff
@@ -28,47 +28,45 @@
 from featurebyte.schema.info import DeploymentInfo
 from featurebyte.schema.task import Task
 from featurebyte.schema.worker.task.deployment_create_update import (
     CreateDeploymentPayload,
     DeploymentCreateUpdateTaskPayload,
     UpdateDeploymentPayload,
 )
-from featurebyte.service.catalog import CatalogService
+from featurebyte.service.catalog import AllCatalogService, CatalogService
 from featurebyte.service.context import ContextService
-from featurebyte.service.deployment import DeploymentService
-from featurebyte.service.feature_list import FeatureListService
-from featurebyte.service.info import InfoService
+from featurebyte.service.deployment import AllDeploymentService, DeploymentService
+from featurebyte.service.feature_list import AllFeatureListService, FeatureListService
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
 from featurebyte.service.online_serving import OnlineServingService
 
 
 class DeploymentController(
     BaseDocumentController[DeploymentModel, DeploymentService, DeploymentList]
 ):
     """
     Deployment Controller
     """
 
     paginated_document_class = DeploymentList
 
     def __init__(
         self,
-        service: DeploymentService,
+        deployment_service: DeploymentService,
         catalog_service: CatalogService,
         context_service: ContextService,
         feature_list_service: FeatureListService,
         online_serving_service: OnlineServingService,
-        info_service: InfoService,
         task_controller: TaskController,
     ):
-        super().__init__(service)
+        super().__init__(deployment_service)
         self.catalog_service = catalog_service
         self.context_service = context_service
         self.feature_list_service = feature_list_service
         self.online_serving_service = online_serving_service
-        self.info_service = info_service
         self.task_controller = task_controller
 
     async def create_deployment(self, data: DeploymentCreate) -> Task:
         """
         Create deployment.
 
         Parameters
@@ -139,18 +137,32 @@
         verbose: bool
             Whether to return verbose info
 
         Returns
         -------
         DeploymentInfo
         """
-        info_document = await self.info_service.get_deployment_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        deployment = await self.service.get_document(document_id=document_id)
+        feature_list = await self.feature_list_service.get_document(
+            document_id=deployment.feature_list_id
+        )
+        return DeploymentInfo(
+            name=deployment.name,
+            feature_list_name=feature_list.name,
+            feature_list_version=feature_list.version.to_str(),
+            num_feature=len(feature_list.feature_ids),
+            enabled=deployment.enabled,
+            serving_endpoint=(
+                f"/deployment/{deployment.id}/online_features" if deployment.enabled else None
+            ),
+            created_at=deployment.created_at,
+            updated_at=deployment.updated_at,
+            description=deployment.description,
         )
-        return info_document
 
     async def compute_online_features(
         self,
         deployment_id: ObjectId,
         data: OnlineFeaturesRequestPayload,
         get_credential: Any,
     ) -> OnlineFeaturesResponseModel:
@@ -186,54 +198,75 @@
         except (FeatureListNotOnlineEnabledError, RuntimeError) as exc:
             raise HTTPException(
                 status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=exc.args[0]
             ) from exc
         assert result is not None, result
         return result
 
+
+class AllDeploymentController(
+    BaseDocumentController[DeploymentModel, AllDeploymentService, DeploymentList]
+):
+    """
+    All Deployment Controller
+    """
+
+    paginated_document_class = DeploymentList
+
+    def __init__(
+        self,
+        all_deployment_service: AllDeploymentService,
+        all_catalog_service: AllCatalogService,
+        all_feature_list_service: AllFeatureListService,
+        task_controller: TaskController,
+    ):
+        super().__init__(all_deployment_service)
+        self.catalog_service = all_catalog_service
+        self.feature_list_service = all_feature_list_service
+        self.task_controller = task_controller
+
     async def get_deployment_summary(self) -> DeploymentSummary:
         """
         Get summary of all deployments.
 
         Returns
         -------
         DeploymentSummary
             Summary of all deployments.
         """
         feature_list_ids = set()
         feature_ids = set()
         with self.service.allow_use_raw_query_filter():
-            deployment_data = await self.service.list_documents(
+            deployment_data = await self.service.list_documents_as_dict(
                 page=1,
                 page_size=0,
                 query_filter={"enabled": True},
                 use_raw_query_filter=True,
             )
 
         for doc in deployment_data["data"]:
             deployment_model = DeploymentModel(**doc)
             feature_list_ids.add(deployment_model.feature_list_id)
 
         with self.feature_list_service.allow_use_raw_query_filter():
-            async for doc in self.feature_list_service.list_documents_iterator(
+            async for feature_list in self.feature_list_service.list_documents_iterator(
                 query_filter={"_id": {"$in": list(feature_list_ids)}},
                 use_raw_query_filter=True,
             ):
-                feature_list_model = FeatureListModel(**doc)
-                feature_ids.update(set(feature_list_model.feature_ids))
+                feature_ids.update(set(feature_list.feature_ids))
 
         return DeploymentSummary(
             num_feature_list=len(feature_list_ids),
             num_feature=len(feature_ids),
         )
 
     async def list_all_deployments(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         enabled: bool | None = None,
     ) -> AllDeploymentList:
         """
         List all deployments across all catalogs.
 
@@ -251,36 +284,36 @@
             Whether to return only enabled deployments
 
         Returns
         -------
         AllDeploymentList
         """
         with self.service.allow_use_raw_query_filter():
-            deployment_data = await self.service.list_documents(
+            deployment_data = await self.service.list_documents_as_dict(
                 page=page,
                 page_size=page_size,
                 sort_by=sort_by,
                 sort_dir=sort_dir,
                 query_filter={"enabled": enabled} if enabled is not None else {},
                 use_raw_query_filter=True,
             )
 
         feature_list_ids = {doc["feature_list_id"] for doc in deployment_data["data"]}
         with self.feature_list_service.allow_use_raw_query_filter():
-            feature_list_documents = await self.feature_list_service.list_documents(
+            feature_list_documents = await self.feature_list_service.list_documents_as_dict(
                 page_size=0,
                 query_filter={"_id": {"$in": list(feature_list_ids)}},
                 use_raw_query_filter=True,
             )
             deployment_id_to_feature_list = {
                 doc["_id"]: FeatureListModel(**doc) for doc in feature_list_documents["data"]
             }
 
         catalog_ids = {doc["catalog_id"] for doc in deployment_data["data"]}
-        catalog_documents = await self.catalog_service.list_documents(
+        catalog_documents = await self.catalog_service.list_documents_as_dict(
             page_size=0, query_filter={"_id": {"$in": list(catalog_ids)}}
         )
         deployment_id_to_catalog_name = {
             doc["_id"]: doc["name"] for doc in catalog_documents["data"]
         }
 
         output = []
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/dimension_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/dimension_table/api.py`

 * *Files 19% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.dimension_table import (
     DimensionTableCreate,
     DimensionTableList,
     DimensionTableUpdate,
 )
 from featurebyte.schema.info import DimensionTableInfo
 
@@ -136,7 +137,24 @@
     """
     controller = request.state.app_container.dimension_table_controller
     info = await controller.get_info(
         document_id=dimension_table_id,
         verbose=verbose,
     )
     return cast(DimensionTableInfo, info)
+
+
+@router.patch("/{dimension_table_id}/description", response_model=DimensionTableModel)
+async def update_dimension_table_description(
+    request: Request,
+    dimension_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> DimensionTableModel:
+    """
+    Update dimension_table description
+    """
+    controller = request.state.app_container.dimension_table_controller
+    dimension_table: DimensionTableModel = await controller.update_description(
+        document_id=dimension_table_id,
+        description=data.description,
+    )
+    return dimension_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/dimension_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/dimension_table/controller.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,26 +9,46 @@
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.dimension_table import DimensionTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
 from featurebyte.schema.dimension_table import DimensionTableList, DimensionTableServiceUpdate
 from featurebyte.schema.info import DimensionTableInfo
 from featurebyte.service.dimension_table import DimensionTableService
+from featurebyte.service.semantic import SemanticService
+from featurebyte.service.table_columns_info import TableColumnsInfoService, TableDocumentService
+from featurebyte.service.table_info import TableInfoService
+from featurebyte.service.table_status import TableStatusService
 
 
 class DimensionTableController(
     BaseTableDocumentController[DimensionTableModel, DimensionTableService, DimensionTableList]
 ):
     """
     DimensionTable controller
     """
 
     paginated_document_class = DimensionTableList
     document_update_schema_class = DimensionTableServiceUpdate
 
+    def __init__(
+        self,
+        dimension_table_service: TableDocumentService,
+        table_columns_info_service: TableColumnsInfoService,
+        table_status_service: TableStatusService,
+        semantic_service: SemanticService,
+        table_info_service: TableInfoService,
+    ):
+        super().__init__(
+            dimension_table_service,
+            table_columns_info_service,
+            table_status_service,
+            semantic_service,
+        )
+        self.table_info_service = table_info_service
+
     async def _get_column_semantic_map(self, document: DimensionTableModel) -> dict[str, Any]:
         dimension_id_semantic = await self.semantic_service.get_or_create_document(
             name=SemanticType.DIMENSION_ID
         )
         return {document.dimension_id_column: dimension_id_semantic}
 
     async def get_info(self, document_id: ObjectId, verbose: bool) -> DimensionTableInfo:
@@ -42,11 +62,15 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         DimensionTableInfo
         """
-        info_document = await self.info_service.get_dimension_table_info(
-            document_id=document_id, verbose=verbose
+        dimension_table = await self.service.get_document(document_id=document_id)
+        table_dict = await self.table_info_service.get_table_info(
+            data_document=dimension_table, verbose=verbose
+        )
+        return DimensionTableInfo(
+            **table_dict,
+            dimension_id_column=dimension_table.dimension_id_column,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/entity/api.py` & `featurebyte-0.4.0/featurebyte/routes/entity/api.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,26 +6,27 @@
 from typing import List, Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.entity import EntityModel, EntityNameHistoryEntry, ParentEntity
+from featurebyte.models.entity import EntityModel, EntityNameHistoryEntry
 from featurebyte.models.persistent import AuditDocumentList
 from featurebyte.routes.common.schema import (
     AuditLogSortByQuery,
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.entity import EntityCreate, EntityList, EntityUpdate
 from featurebyte.schema.info import EntityInfo
 
 router = APIRouter(prefix="/entity")
 
 
 @router.post("", response_model=EntityModel, status_code=HTTPStatus.CREATED)
@@ -86,40 +87,14 @@
     entity: EntityModel = await controller.update_entity(
         entity_id=entity_id,
         data=data,
     )
     return entity
 
 
-@router.post("/{entity_id}/parent", response_model=EntityModel, status_code=HTTPStatus.CREATED)
-async def add_parent(
-    request: Request, entity_id: PydanticObjectId, data: ParentEntity
-) -> EntityModel:
-    """
-    Create entity relationship
-    """
-    controller = request.state.app_container.entity_controller
-    entity: EntityModel = await controller.create_relationship(data=data, child_id=entity_id)
-    return entity
-
-
-@router.delete("/{entity_id}/parent/{parent_entity_id}", response_model=EntityModel)
-async def remove_parent(
-    request: Request, entity_id: PydanticObjectId, parent_entity_id: PydanticObjectId
-) -> EntityModel:
-    """
-    Remove entity relationship
-    """
-    controller = request.state.app_container.entity_controller
-    entity: EntityModel = await controller.remove_relationship(
-        parent_id=parent_entity_id, child_id=entity_id
-    )
-    return entity
-
-
 @router.get("/audit/{entity_id}", response_model=AuditDocumentList)
 async def list_entity_audit_logs(
     request: Request,
     entity_id: PydanticObjectId,
     page: int = PageQuery,
     page_size: int = PageSizeQuery,
     sort_by: Optional[str] = AuditLogSortByQuery,
@@ -178,7 +153,24 @@
     """
     controller = request.state.app_container.entity_controller
     info = await controller.get_info(
         document_id=entity_id,
         verbose=verbose,
     )
     return cast(EntityInfo, info)
+
+
+@router.patch("/{entity_id}/description", response_model=EntityModel)
+async def update_entity_description(
+    request: Request,
+    entity_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> EntityModel:
+    """
+    Update entity description
+    """
+    controller = request.state.app_container.entity_controller
+    entity: EntityModel = await controller.update_description(
+        document_id=entity_id,
+        description=data.description,
+    )
+    return entity
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/entity/controller.py` & `featurebyte-0.4.0/featurebyte/routes/entity/controller.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,42 +1,39 @@
 """
 Entity API route controller
 """
 from __future__ import annotations
 
 from bson.objectid import ObjectId
 
-from featurebyte.models.entity import EntityModel, ParentEntity
-from featurebyte.routes.common.base import BaseDocumentController, RelationshipMixin
+from featurebyte.models.entity import EntityModel
+from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.schema.entity import EntityCreate, EntityList, EntityServiceUpdate, EntityUpdate
 from featurebyte.schema.info import EntityInfo
+from featurebyte.service.catalog import CatalogService
 from featurebyte.service.entity import EntityService
-from featurebyte.service.info import InfoService
 from featurebyte.service.relationship import EntityRelationshipService
 
 
-class EntityController(
-    BaseDocumentController[EntityModel, EntityService, EntityList],
-    RelationshipMixin[EntityModel, ParentEntity],
-):
+class EntityController(BaseDocumentController[EntityModel, EntityService, EntityList]):
     """
     Entity Controller
     """
 
     paginated_document_class = EntityList
 
     def __init__(
         self,
-        service: EntityService,
+        entity_service: EntityService,
         entity_relationship_service: EntityRelationshipService,
-        info_service: InfoService,
+        catalog_service: CatalogService,
     ):
-        super().__init__(service)
+        super().__init__(entity_service)
         self.relationship_service = entity_relationship_service
-        self.info_service = info_service
+        self.catalog_service = catalog_service
 
     async def create_entity(
         self,
         data: EntityCreate,
     ) -> EntityModel:
         """
         Create Entity at persistent
@@ -80,24 +77,34 @@
 
     async def get_info(
         self,
         document_id: ObjectId,
         verbose: bool,
     ) -> EntityInfo:
         """
-        Get document info given document ID
+        Get entity info
 
         Parameters
         ----------
         document_id: ObjectId
             Document ID
         verbose: bool
-            Flag to control verbose level
+            Verbose or not
 
         Returns
         -------
-        InfoDocument
+        EntityInfo
         """
-        info_document = await self.info_service.get_entity_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        entity = await self.service.get_document(document_id=document_id)
+
+        # get catalog info
+        catalog = await self.catalog_service.get_document(entity.catalog_id)
+
+        return EntityInfo(
+            name=entity.name,
+            created_at=entity.created_at,
+            updated_at=entity.updated_at,
+            serving_names=entity.serving_names,
+            catalog_name=catalog.name,
+            description=catalog.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/event_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/event_table/api.py`

 * *Files 20% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.event_table import EventTableCreate, EventTableList, EventTableUpdate
 from featurebyte.schema.info import EventTableInfo
 
 router = APIRouter(prefix="/event_table")
 
 
 @router.post("", response_model=EventTableModel, status_code=HTTPStatus.CREATED)
@@ -154,7 +155,24 @@
     """
     controller = request.state.app_container.event_table_controller
     info = await controller.get_info(
         document_id=event_table_id,
         verbose=verbose,
     )
     return cast(EventTableInfo, info)
+
+
+@router.patch("/{event_table_id}/description", response_model=EventTableModel)
+async def update_event_table_description(
+    request: Request,
+    event_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> EventTableModel:
+    """
+    Update event_table description
+    """
+    controller = request.state.app_container.event_table_controller
+    event_table: EventTableModel = await controller.update_description(
+        document_id=event_table_id,
+        description=data.description,
+    )
+    return event_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/event_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/event_table/controller.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,26 +9,43 @@
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.event_table import EventTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
 from featurebyte.schema.event_table import EventTableList, EventTableServiceUpdate
 from featurebyte.schema.info import EventTableInfo
 from featurebyte.service.event_table import EventTableService
+from featurebyte.service.semantic import SemanticService
+from featurebyte.service.table_columns_info import TableColumnsInfoService, TableDocumentService
+from featurebyte.service.table_info import TableInfoService
+from featurebyte.service.table_status import TableStatusService
 
 
 class EventTableController(
     BaseTableDocumentController[EventTableModel, EventTableService, EventTableList]
 ):
     """
     EventTable controller
     """
 
     paginated_document_class = EventTableList
     document_update_schema_class = EventTableServiceUpdate
 
+    def __init__(
+        self,
+        event_table_service: TableDocumentService,
+        table_columns_info_service: TableColumnsInfoService,
+        table_status_service: TableStatusService,
+        semantic_service: SemanticService,
+        table_info_service: TableInfoService,
+    ):
+        super().__init__(
+            event_table_service, table_columns_info_service, table_status_service, semantic_service
+        )
+        self.table_info_service = table_info_service
+
     async def _get_column_semantic_map(self, document: EventTableModel) -> dict[str, Any]:
         event_timestamp = await self.semantic_service.get_or_create_document(
             name=SemanticType.EVENT_TIMESTAMP
         )
         event_id = await self.semantic_service.get_or_create_document(name=SemanticType.EVENT_ID)
         assert document.event_id_column is not None
         return {
@@ -47,11 +64,17 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         EventTableInfo
         """
-        info_document = await self.info_service.get_event_table_info(
-            document_id=document_id, verbose=verbose
+        event_table = await self.service.get_document(document_id=document_id)
+        table_dict = await self.table_info_service.get_table_info(
+            data_document=event_table, verbose=verbose
+        )
+        return EventTableInfo(
+            **table_dict,
+            event_id_column=event_table.event_id_column,
+            event_timestamp_column=event_table.event_timestamp_column,
+            default_feature_job_setting=event_table.default_feature_job_setting,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature/api.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,26 +18,26 @@
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
     VersionQuery,
 )
-from featurebyte.schema.common.base import DeleteResponse
+from featurebyte.schema.common.base import DeleteResponse, DescriptionUpdate
 from featurebyte.schema.feature import (
     BatchFeatureCreate,
     FeatureCreate,
     FeatureModelResponse,
     FeatureNewVersionCreate,
     FeaturePaginatedList,
-    FeaturePreview,
     FeatureSQL,
     FeatureUpdate,
 )
 from featurebyte.schema.info import FeatureInfo
+from featurebyte.schema.preview import FeatureOrTargetPreview
 from featurebyte.schema.task import Task
 
 router = APIRouter(prefix="/feature")
 
 
 @router.post("", response_model=FeatureModelResponse, status_code=HTTPStatus.CREATED)
 async def create_feature(
@@ -168,15 +168,15 @@
     )
     return cast(FeatureInfo, info)
 
 
 @router.post("/preview", response_model=Dict[str, Any])
 async def get_feature_preview(
     request: Request,
-    feature_preview: FeaturePreview,
+    feature_preview: FeatureOrTargetPreview,
 ) -> Dict[str, Any]:
     """
     Retrieve Feature preview
     """
     controller = request.state.app_container.feature_controller
     return cast(
         Dict[str, Any],
@@ -210,10 +210,26 @@
     """
     Retrieve feature job status
     """
     controller = request.state.app_container.feature_controller
     result = await controller.get_feature_job_logs(
         feature_id=feature_id,
         hour_limit=hour_limit,
-        get_credential=request.state.get_credential,
     )
     return cast(Dict[str, Any], result)
+
+
+@router.patch("/{feature_id}/description", response_model=FeatureModelResponse)
+async def update_feature_description(
+    request: Request,
+    feature_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureModelResponse:
+    """
+    Update feature description
+    """
+    controller = request.state.app_container.feature_controller
+    feature: FeatureModelResponse = await controller.update_description(
+        document_id=feature_id,
+        description=data.description,
+    )
+    return feature
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature/controller.py` & `featurebyte-0.4.0/featurebyte/service/version.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,448 +1,370 @@
 """
-Feature API route controller
+VersionService class
 """
 from __future__ import annotations
 
-from typing import Any, Dict, Literal, Optional, Union, cast
-
-from http import HTTPStatus
-from pprint import pformat
+from typing import Any, Optional
 
 from bson.objectid import ObjectId
-from fastapi.exceptions import HTTPException
 
+from featurebyte.enum import TableDataType
 from featurebyte.exception import (
-    DocumentDeletionError,
-    MissingPointInTimeColumnError,
-    RequiredEntityNotProvidedError,
-)
-from featurebyte.feature_manager.model import ExtendedFeatureModel
-from featurebyte.models.base import VersionIdentifier
-from featurebyte.models.feature import DefaultVersionMode, FeatureModel, FeatureReadiness
-from featurebyte.routes.common.base import (
-    BaseDocumentController,
-    DerivePrimaryEntityMixin,
-    PaginatedDocument,
+    DocumentError,
+    NoChangesInFeatureVersionError,
+    NoFeatureJobSettingInSourceError,
 )
-from featurebyte.routes.task.controller import TaskController
-from featurebyte.schema.feature import (
-    BatchFeatureCreate,
-    FeatureCreate,
-    FeatureModelResponse,
-    FeatureNewVersionCreate,
-    FeaturePaginatedList,
-    FeaturePreview,
-    FeatureServiceCreate,
-    FeatureSQL,
-    FeatureUpdate,
+from featurebyte.models.event_table import EventTableModel
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_list import FeatureListModel
+from featurebyte.models.proxy_table import ProxyTableModel
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.model.feature_job_setting import (
+    FeatureJobSetting,
+    TableFeatureJobSetting,
 )
-from featurebyte.schema.info import FeatureInfo
-from featurebyte.schema.task import Task
-from featurebyte.schema.worker.task.batch_feature_create import BatchFeatureCreateTaskPayload
-from featurebyte.service.entity import EntityService
+from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
+from featurebyte.query_graph.node.generic import GroupByNode
+from featurebyte.schema.feature import FeatureNewVersionCreate, FeatureServiceCreate
+from featurebyte.schema.feature_list import FeatureListNewVersionCreate, FeatureListServiceCreate
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
+from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
-from featurebyte.service.feature_readiness import FeatureReadinessService
-from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
-from featurebyte.service.info import InfoService
-from featurebyte.service.mixin import Document
-from featurebyte.service.preview import PreviewService
-from featurebyte.service.version import VersionService
+from featurebyte.service.table import TableService
+from featurebyte.service.view_construction import ViewConstructionService
 
 
-# pylint: disable=too-many-instance-attributes
-class FeatureController(
-    BaseDocumentController[FeatureModelResponse, FeatureService, FeaturePaginatedList],
-    DerivePrimaryEntityMixin,
-):
+class VersionService:
     """
-    Feature controller
+    VersionService class is responsible for creating new feature version
     """
 
-    paginated_document_class = FeaturePaginatedList
-
     def __init__(
         self,
-        service: FeatureService,
+        table_service: TableService,
+        feature_service: FeatureService,
         feature_namespace_service: FeatureNamespaceService,
-        entity_service: EntityService,
         feature_list_service: FeatureListService,
-        feature_readiness_service: FeatureReadinessService,
-        preview_service: PreviewService,
-        version_service: VersionService,
-        info_service: InfoService,
-        feature_store_warehouse_service: FeatureStoreWarehouseService,
-        task_controller: TaskController,
+        feature_list_namespace_service: FeatureListNamespaceService,
+        view_construction_service: ViewConstructionService,
     ):
-        # pylint: disable=too-many-arguments
-        super().__init__(service)
+        self.table_service = table_service
+        self.feature_service = feature_service
         self.feature_namespace_service = feature_namespace_service
-        self.entity_service = entity_service
         self.feature_list_service = feature_list_service
-        self.feature_readiness_service = feature_readiness_service
-        self.preview_service = preview_service
-        self.version_service = version_service
-        self.info_service = info_service
-        self.feature_store_warehouse_service = feature_store_warehouse_service
-        self.task_controller = task_controller
+        self.feature_list_namespace_service = feature_list_namespace_service
+        self.view_construction_service = view_construction_service
 
-    async def submit_batch_feature_create_task(self, data: BatchFeatureCreate) -> Optional[Task]:
-        """
-        Submit Feature Create Task
+    async def _prepare_group_by_node_name_to_replacement_node(
+        self,
+        feature: FeatureModel,
+        table_feature_job_settings: Optional[list[TableFeatureJobSetting]],
+        use_source_settings: bool,
+    ) -> dict[str, Node]:
+        """
+        Prepare group by node name to replacement node mapping by using the provided feature job setting
+        to construct new group by node. If the new group by node is the same as the existing one, then
+        the mapping will be empty.
 
         Parameters
         ----------
-        data: BatchFeatureCreate
-            Batch Feature creation payload
+        feature: FeatureModel
+            Feature model
+        table_feature_job_settings: Optional[list[TableFeatureJobSetting]]
+            Feature job setting
+        use_source_settings: bool
+            Whether to use source table's default feature job setting
 
         Returns
         -------
-        Optional[Task]
-            Task object
+        dict[str, Node]
+
+        Raises
+        ------
+        NoFeatureJobSettingInSourceError
+            If the source table does not have a default feature job setting
         """
-        payload = BatchFeatureCreateTaskPayload(
-            **{
-                **data.json_dict(),
-                "catalog_id": self.service.catalog_id,
+        node_name_to_replacement_node: dict[str, Node] = {}
+        table_feature_job_settings = table_feature_job_settings or []
+        if table_feature_job_settings or use_source_settings:
+            table_name_to_feature_job_setting: dict[str, FeatureJobSetting] = {
+                data_feature_job_setting.table_name: data_feature_job_setting.feature_job_setting
+                for data_feature_job_setting in table_feature_job_settings
             }
-        )
-        task_id = await self.task_controller.task_manager.submit(payload=payload)
-        return await self.task_controller.task_manager.get_task(task_id=str(task_id))
-
-    async def create_feature(
-        self, data: Union[FeatureCreate, FeatureNewVersionCreate]
-    ) -> FeatureModelResponse:
+            table_id_to_table: dict[ObjectId, ProxyTableModel] = {
+                # pylint: disable=abstract-class-instantiated
+                doc["_id"]: ProxyTableModel(**doc)  # type: ignore
+                async for doc in self.table_service.list_documents_as_dict_iterator(
+                    query_filter={"_id": {"$in": feature.table_ids}}
+                )
+            }
+            for (
+                group_by_node,
+                table_id,
+            ) in feature.graph.iterate_group_by_node_and_table_id_pairs(target_node=feature.node):
+                # prepare feature job setting
+                assert table_id is not None, "Table ID should not be None."
+                table = table_id_to_table[table_id]
+                feature_job_setting: Optional[FeatureJobSetting] = None
+                if use_source_settings:
+                    # use the event table source's default feature job setting if table is event table
+                    # otherwise, do not create a replacement node for the group by node
+                    if table.type == TableDataType.EVENT_TABLE:
+                        assert isinstance(table, EventTableModel)
+                        feature_job_setting = table.default_feature_job_setting
+                        if not feature_job_setting:
+                            raise NoFeatureJobSettingInSourceError(
+                                f"No feature job setting found in source id {table_id}"
+                            )
+                else:
+                    # use the provided feature job setting
+                    assert table.name is not None, "Table name should not be None."
+                    feature_job_setting = table_name_to_feature_job_setting.get(table.name)
+
+                if feature_job_setting:
+                    # input node will be used when we need to support updating specific
+                    # GroupBy node given event table ID
+                    parameters = {
+                        **group_by_node.parameters.dict(),
+                        **feature_job_setting.to_seconds(),
+                    }
+                    if group_by_node.parameters.dict() != parameters:
+                        node_name_to_replacement_node[group_by_node.name] = GroupByNode(
+                            **{**group_by_node.dict(), "parameters": parameters}
+                        )
+
+        return node_name_to_replacement_node
+
+    @staticmethod
+    def _create_new_feature_version_from(
+        feature: FeatureModel, node_name_to_replacement_node: dict[str, Node]
+    ) -> Optional[FeatureModel]:
         """
-        Create Feature at persistent (GitDB or MongoDB)
+        Create a new feature version from the provided feature model and node name to replacement node mapping.
 
         Parameters
         ----------
-        data: FeatureCreate | FeatureNewVersionCreate
-            Feature creation payload
+        feature: FeatureModel
+            Feature model
+        node_name_to_replacement_node: dict[str, Node]
+            Node name to replacement node mapping
 
         Returns
         -------
-        FeatureModelResponse
-            Newly created feature object
+        Optional[FeatureModel]
         """
-        if isinstance(data, FeatureCreate):
-            document = await self.service.create_document(
-                data=FeatureServiceCreate(**data.json_dict())
-            )
-        else:
-            document = await self.version_service.create_new_feature_version(data=data)
-
-        # update feature namespace readiness due to introduction of new feature
-        await self.feature_readiness_service.update_feature_namespace(
-            feature_namespace_id=document.feature_namespace_id,
-            return_document=False,
+        # reconstruct view graph node to remove unused column cleaning operations
+        graph, node_name_map = feature.graph.reconstruct(
+            node_name_to_replacement_node=node_name_to_replacement_node,
+            regenerate_groupby_hash=True,
         )
-        return await self.get(document_id=document.id)
+        node_name = node_name_map[feature.node_name]
 
-    async def get(
-        self,
-        document_id: ObjectId,
-        exception_detail: str | None = None,
-    ) -> Document:
-        document = await self.service.get_document(
-            document_id=document_id,
-            exception_detail=exception_detail,
-        )
-        output = FeatureModelResponse(
-            **document.dict(by_alias=True),
-            primary_entity_ids=await self.derive_primary_entity_ids(entity_ids=document.entity_ids),
+        # prune the graph to remove unused nodes
+        pruned_graph, node_name_map = QueryGraph(**graph.dict(by_alias=True)).prune(
+            target_node=graph.get_node_by_name(node_name),
         )
-        return cast(Document, output)
+        pruned_node_name = node_name_map[node_name]
 
-    async def list(
-        self,
-        page: int = 1,
-        page_size: int = 10,
-        sort_by: str | None = "created_at",
-        sort_dir: Literal["asc", "desc"] = "desc",
-        **kwargs: Any,
-    ) -> PaginatedDocument:
-        document_data = await self.service.list_documents(
-            page=page,
-            page_size=page_size,
-            sort_by=sort_by,
-            sort_dir=sort_dir,
-            **kwargs,
-        )
-        entity_id_to_entity = await self.get_entity_id_to_entity(doc_list=document_data["data"])
-
-        output = []
-        for feature in document_data["data"]:
-            primary_entity_ids = await self.derive_primary_entity_ids(
-                entity_ids=feature["entity_ids"], entity_id_to_entity=entity_id_to_entity
-            )
-            output.append(
-                FeatureModelResponse(
-                    **feature,
-                    primary_entity_ids=primary_entity_ids,
-                )
+        # only return a new feature version if the graph is changed
+        reference_hash_before = feature.graph.node_name_to_ref[feature.node_name]
+        reference_hash_after = graph.node_name_to_ref[node_name]
+        if reference_hash_before != reference_hash_after:
+            return FeatureModel(
+                **{
+                    **feature.dict(),
+                    "graph": pruned_graph,
+                    "node_name": pruned_node_name,
+                    "_id": ObjectId(),
+                }
             )
+        return None
 
-        document_data["data"] = output
-        return cast(PaginatedDocument, self.paginated_document_class(**document_data))
-
-    async def update_feature(
+    async def _create_new_feature_version(
         self,
-        feature_id: ObjectId,
-        data: FeatureUpdate,
+        feature: FeatureModel,
+        table_feature_job_settings: Optional[list[TableFeatureJobSetting]],
+        table_cleaning_operations: Optional[list[TableCleaningOperation]],
+        use_source_settings: bool,
     ) -> FeatureModel:
-        """
-        Update Feature at persistent
+        node_name_to_replacement_node: dict[str, Node] = {}
 
-        Parameters
-        ----------
-        feature_id: ObjectId
-            Feature ID
-        data: FeatureUpdate
-            Feature update payload
-
-        Returns
-        -------
-        FeatureModel
-            Feature object with updated attribute(s)
-        """
-        if data.readiness:
-            await self.feature_readiness_service.update_feature(
-                feature_id=feature_id,
-                readiness=FeatureReadiness(data.readiness),
-                ignore_guardrails=bool(data.ignore_guardrails),
-                return_document=False,
-            )
-        return await self.get(document_id=feature_id)
-
-    async def delete_feature(self, feature_id: ObjectId) -> None:
-        """
-        Delete Feature at persistent
-
-        Parameters
-        ----------
-        feature_id: ObjectId
-            Feature ID
-
-        Raises
-        ------
-        DocumentDeletionError
-            * If the feature is not in draft readiness
-            * If the feature is the default feature and the default version mode is manual
-            * If the feature is in any saved feature list
-        """
-        feature = await self.service.get_document(document_id=feature_id)
-        feature_namespace = await self.feature_namespace_service.get_document(
-            document_id=feature.feature_namespace_id
+        # prepare group by node replacement
+        group_by_node_replacement = await self._prepare_group_by_node_name_to_replacement_node(
+            feature=feature,
+            table_feature_job_settings=table_feature_job_settings,
+            use_source_settings=use_source_settings,
         )
+        node_name_to_replacement_node.update(group_by_node_replacement)
 
-        if feature.readiness != FeatureReadiness.DRAFT:
-            raise DocumentDeletionError("Only feature with draft readiness can be deleted.")
-
-        if (
-            feature_namespace.default_feature_id == feature_id
-            and feature_namespace.default_version_mode == DefaultVersionMode.MANUAL
-        ):
-            raise DocumentDeletionError(
-                "Feature is the default feature of the feature namespace and the default version mode is manual. "
-                "Please set another feature as the default feature or change the default version mode to auto."
-            )
-
-        if feature.feature_list_ids:
-            feature_list_info = []
-            async for feature_list in self.feature_list_service.list_documents_iterator(
-                query_filter={"_id": {"$in": feature.feature_list_ids}}
-            ):
-                feature_list_info.append(
-                    {
-                        "id": str(feature_list["_id"]),
-                        "name": feature_list["name"],
-                        "version": VersionIdentifier(**feature_list["version"]).to_str(),
-                    }
+        # prepare view graph node replacement
+        if table_cleaning_operations or use_source_settings:
+            view_node_name_replacement = (
+                await self.view_construction_service.prepare_view_node_name_to_replacement_node(
+                    query_graph=feature.graph,
+                    target_node=feature.node,
+                    table_cleaning_operations=table_cleaning_operations or [],
+                    use_source_settings=use_source_settings,
                 )
-
-            raise DocumentDeletionError(
-                f"Feature is still in use by feature list(s). Please remove the following feature list(s) first:\n"
-                f"{pformat(feature_list_info)}"
             )
+            node_name_to_replacement_node.update(view_node_name_replacement)
 
-        # use transaction to ensure atomicity
-        async with self.service.persistent.start_transaction():
-            # delete feature from the persistent
-            await self.service.delete_document(document_id=feature_id)
-            await self.feature_readiness_service.update_feature_namespace(
-                feature_namespace_id=feature.feature_namespace_id,
-                deleted_feature_ids=[feature_id],
-                return_document=False,
-            )
-            feature_namespace = await self.feature_namespace_service.get_document(
-                document_id=feature.feature_namespace_id
+        # create a new feature version if the new feature graph is different from the source feature graph
+        new_feature_version = self._create_new_feature_version_from(
+            feature=feature, node_name_to_replacement_node=node_name_to_replacement_node
+        )
+        if new_feature_version:
+            return new_feature_version
+
+        # no new feature is created, raise the error message accordingly
+        if table_feature_job_settings or table_cleaning_operations:
+            actions = []
+            if table_feature_job_settings:
+                actions.append("feature job setting")
+            if table_cleaning_operations:
+                actions.append("table cleaning operation(s)")
+
+            actions_str = " and ".join(actions).capitalize()
+            do_str = "do" if len(actions) > 1 else "does"
+            error_message = (
+                f"{actions_str} {do_str} not result a new feature version. "
+                "This is because the new feature version is the same as the source feature."
             )
-            if not feature_namespace.feature_ids:
-                # delete feature namespace if it has no more feature
-                await self.feature_namespace_service.delete_document(
-                    document_id=feature.feature_namespace_id
-                )
+            raise NoChangesInFeatureVersionError(error_message)
+        raise NoChangesInFeatureVersionError("No change detected on the new feature version.")
 
-    async def list_features(
+    async def create_new_feature_version(
         self,
-        page: int = 1,
-        page_size: int = 10,
-        sort_by: str | None = "created_at",
-        sort_dir: Literal["asc", "desc"] = "desc",
-        search: str | None = None,
-        name: str | None = None,
-        version: str | None = None,
-        feature_list_id: ObjectId | None = None,
-        feature_namespace_id: ObjectId | None = None,
-    ) -> FeaturePaginatedList:
+        data: FeatureNewVersionCreate,
+    ) -> FeatureModel:
         """
-        List documents stored at persistent (GitDB or MongoDB)
+        Create new feature version based on given source feature
+        (newly created feature will be saved to the persistent)
 
         Parameters
         ----------
-        page: int
-            Page number
-        page_size: int
-            Number of items per page
-        sort_by: str | None
-            Key used to sort the returning documents
-        sort_dir: "asc" or "desc"
-            Sorting the returning documents in ascending order or descending order
-        search: str | None
-            Search token to be used in filtering
-        name: str | None
-            Feature name to be used in filtering
-        version: str | None
-            Feature version to be used in filtering
-        feature_list_id: ObjectId | None
-            Feature list ID to be used in filtering
-        feature_namespace_id: ObjectId | None
-            Feature namespace ID to be used in filtering
+        data: FeatureNewVersionCreate
+            Version creation payload
 
         Returns
         -------
-        FeaturePaginatedList
-            List of documents fulfilled the filtering condition
+        FeatureModel
         """
-        params: Dict[str, Any] = {"search": search, "name": name}
-        if version:
-            params["version"] = VersionIdentifier.from_str(version).dict()
-
-        if feature_list_id:
-            feature_list_document = await self.feature_list_service.get_document(
-                document_id=feature_list_id
-            )
-            params["query_filter"] = {"_id": {"$in": feature_list_document.feature_ids}}
-
-        if feature_namespace_id:
-            query_filter = params.get("query_filter", {}).copy()
-            query_filter["feature_namespace_id"] = feature_namespace_id
-            params["query_filter"] = query_filter
-
-        return await self.list(
-            page=page,
-            page_size=page_size,
-            sort_by=sort_by,
-            sort_dir=sort_dir,
-            **params,
+        feature = await self.feature_service.get_document(document_id=data.source_feature_id)
+        new_feature = await self._create_new_feature_version(
+            feature,
+            data.table_feature_job_settings,
+            data.table_cleaning_operations,
+            use_source_settings=False,
+        )
+        return await self.feature_service.create_document(
+            data=FeatureServiceCreate(**new_feature.dict(by_alias=True))
         )
 
-    async def preview(self, feature_preview: FeaturePreview, get_credential: Any) -> dict[str, Any]:
-        """
-        Preview a Feature
-
-        Parameters
-        ----------
-        feature_preview: FeaturePreview
-            FeaturePreview object
-        get_credential: Any
-            Get credential handler function
-
-        Returns
-        -------
-        dict[str, Any]
-            Dataframe converted to json string
-
-        Raises
-        ------
-        HTTPException
-            Invalid request payload
-        """
-        try:
-            return await self.preview_service.preview_feature(
-                feature_preview=feature_preview, get_credential=get_credential
-            )
-        except (MissingPointInTimeColumnError, RequiredEntityNotProvidedError) as exc:
-            raise HTTPException(
-                status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=exc.args[0]
-            ) from exc
-
-    async def get_info(
-        self,
-        document_id: ObjectId,
-        verbose: bool,
-    ) -> FeatureInfo:
+    async def create_new_feature_version_using_source_settings(
+        self, document_id: ObjectId
+    ) -> FeatureModel:
         """
-        Get document info given document ID
+        Create new feature version based on feature job settings & cleaning operation from the source table
+        (newly created feature won't be saved to the persistent)
 
         Parameters
         ----------
         document_id: ObjectId
-            Document ID
-        verbose: bool
-            Flag to control verbose level
+            Feature id used to create new version
 
         Returns
         -------
-        InfoDocument
+        FeatureModel
         """
-        info_document = await self.info_service.get_feature_info(
-            document_id=document_id, verbose=verbose
+        feature = await self.feature_service.get_document(document_id=document_id)
+        new_feature = await self._create_new_feature_version(
+            feature,
+            table_feature_job_settings=None,
+            table_cleaning_operations=None,
+            use_source_settings=True,
         )
-        return info_document
+        return new_feature
 
-    async def sql(self, feature_sql: FeatureSQL) -> str:
-        """
-        Get Feature SQL
+    async def _create_new_feature_list_version(
+        self,
+        feature_list: FeatureListModel,
+        feature_namespaces: list[dict[str, Any]],
+        data: FeatureListNewVersionCreate,
+    ) -> FeatureListModel:
+        feat_name_to_default_id_map = {
+            feat_namespace["name"]: feat_namespace["default_feature_id"]
+            for feat_namespace in feature_namespaces
+        }
+        feature_id_to_name_map = {
+            feat_id: feature_namespace["name"]
+            for feature_namespace in feature_namespaces
+            for feat_id in feature_namespace["feature_ids"]
+        }
+        specified_feature_map = {feat_info.name: feat_info.version for feat_info in data.features}
+        features = []
+        for feat_id in feature_list.feature_ids:
+            feat_name = feature_id_to_name_map[feat_id]
+            if feat_name in specified_feature_map:
+                version = specified_feature_map.pop(feat_name)
+                feature = await self.feature_service.get_document_by_name_and_version(
+                    name=feat_name, version=version
+                )
+                features.append(feature)
+            else:
+                # use default feature id for non-specified features
+                feat_id = feat_name_to_default_id_map[feat_name]
+                features.append(await self.feature_service.get_document(document_id=feat_id))
+
+        if specified_feature_map:
+            names = [f'"{name}"' for name in specified_feature_map.keys()]
+            raise DocumentError(
+                f"Features ({', '.join(names)}) are not in the original FeatureList"
+            )
 
-        Parameters
-        ----------
-        feature_sql: FeatureSQL
-            FeatureSQL object
+        feature_ids = [feat.id for feat in features]
+        if set(feature_list.feature_ids) == set(feature_ids):
+            raise DocumentError("No change detected on the new feature list version.")
 
-        Returns
-        -------
-        str
-            Dataframe converted to json string
-        """
-        return await self.preview_service.feature_sql(feature_sql=feature_sql)
+        return FeatureListModel(
+            **{
+                **feature_list.dict(),
+                "_id": ObjectId(),
+                "feature_ids": feature_ids,
+                "features": features,
+            }
+        )
 
-    async def get_feature_job_logs(
-        self, feature_id: ObjectId, hour_limit: int, get_credential: Any
-    ) -> dict[str, Any]:
+    async def create_new_feature_list_version(
+        self,
+        data: FeatureListNewVersionCreate,
+    ) -> FeatureListModel:
         """
-        Retrieve table preview for query graph node
+        Create new feature list version based on given source feature list & new version mode
 
         Parameters
         ----------
-        feature_id: ObjectId
-            Feature Id
-        hour_limit: int
-            Limit in hours on the job history to fetch
-        get_credential: Any
-            Get credential handler function
+        data: FeatureListNewVersionCreate
+            Version creation payload
 
         Returns
         -------
-        dict[str, Any]
-            Dataframe converted to json string
+        FeatureListModel
         """
-        feature = await self.service.get_document(feature_id)
-        return await self.feature_store_warehouse_service.get_feature_job_logs(
-            feature_store_id=feature.tabular_source.feature_store_id,
-            features=[ExtendedFeatureModel(**feature.dict())],
-            hour_limit=hour_limit,
-            get_credential=get_credential,
+        feature_list = await self.feature_list_service.get_document(
+            document_id=data.source_feature_list_id
+        )
+        feature_list_namespace = await self.feature_list_namespace_service.get_document(
+            document_id=feature_list.feature_list_namespace_id,
+        )
+        feature_namespaces = await self.feature_namespace_service.list_documents_as_dict(
+            query_filter={"_id": {"$in": feature_list_namespace.feature_namespace_ids}},
+            page_size=0,
+        )
+        new_feature_list = await self._create_new_feature_list_version(
+            feature_list, feature_namespaces["data"], data
+        )
+        return await self.feature_list_service.create_document(
+            data=FeatureListServiceCreate(**new_feature_list.dict(by_alias=True)),
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_job_setting_analysis/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature_job_setting_analysis/api.py`

 * *Files 14% similar despite different names*

```diff
@@ -19,14 +19,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.feature_job_setting_analysis import (
     FeatureJobSettingAnalysisBacktest,
     FeatureJobSettingAnalysisCreate,
     FeatureJobSettingAnalysisList,
 )
 from featurebyte.schema.info import FeatureJobSettingAnalysisInfo
 from featurebyte.schema.task import Task
@@ -167,7 +168,28 @@
     Run Backtest on Feature Job Setting Analysis
     """
     controller = request.state.app_container.feature_job_setting_analysis_controller
     task_submit: Task = await controller.backtest(
         data=data,
     )
     return task_submit
+
+
+@router.patch(
+    "/{feature_job_setting_analysis_id}/description", response_model=FeatureJobSettingAnalysisModel
+)
+async def update_feature_job_setting_analysis_description(
+    request: Request,
+    feature_job_setting_analysis_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureJobSettingAnalysisModel:
+    """
+    Update feature_job_setting_analysis description
+    """
+    controller = request.state.app_container.feature_job_setting_analysis_controller
+    feature_job_setting_analysis: FeatureJobSettingAnalysisModel = (
+        await controller.update_description(
+            document_id=feature_job_setting_analysis_id,
+            description=data.description,
+        )
+    )
+    return feature_job_setting_analysis
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_job_setting_analysis/controller.py` & `featurebyte-0.4.0/featurebyte/routes/feature_job_setting_analysis/controller.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,26 +6,28 @@
 import tempfile
 from io import BytesIO
 
 import pdfkit
 from bson import ObjectId
 from fastapi.responses import StreamingResponse
 
+from featurebyte import FeatureJobSetting
 from featurebyte.models.feature_job_setting_analysis import FeatureJobSettingAnalysisModel
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.feature_job_setting_analysis import (
     FeatureJobSettingAnalysisBacktest,
     FeatureJobSettingAnalysisCreate,
     FeatureJobSettingAnalysisList,
 )
 from featurebyte.schema.info import FeatureJobSettingAnalysisInfo
 from featurebyte.schema.task import Task
+from featurebyte.service.catalog import CatalogService
+from featurebyte.service.event_table import EventTableService
 from featurebyte.service.feature_job_setting_analysis import FeatureJobSettingAnalysisService
-from featurebyte.service.info import InfoService
 
 
 class FeatureJobSettingAnalysisController(
     BaseDocumentController[
         FeatureJobSettingAnalysisModel,
         FeatureJobSettingAnalysisService,
         FeatureJobSettingAnalysisList,
@@ -35,21 +37,23 @@
     FeatureJobSettingAnalysis controller
     """
 
     paginated_document_class = FeatureJobSettingAnalysisList
 
     def __init__(
         self,
-        service: FeatureJobSettingAnalysisService,
+        feature_job_setting_analysis_service: FeatureJobSettingAnalysisService,
         task_controller: TaskController,
-        info_service: InfoService,
+        event_table_service: EventTableService,
+        catalog_service: CatalogService,
     ):
-        super().__init__(service)
+        super().__init__(feature_job_setting_analysis_service)
         self.task_controller = task_controller
-        self.info_service = info_service
+        self.event_table_service = event_table_service
+        self.catalog_service = catalog_service
 
     async def get_info(
         self,
         document_id: ObjectId,
         verbose: bool,
     ) -> FeatureJobSettingAnalysisInfo:
         """
@@ -62,18 +66,39 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         FeatureJobSettingAnalysisInfo
         """
-        info_document = await self.info_service.get_feature_job_setting_analysis_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        feature_job_setting_analysis = await self.service.get_document(document_id=document_id)
+        recommended_setting = (
+            feature_job_setting_analysis.analysis_result.recommended_feature_job_setting
+        )
+        event_table = await self.event_table_service.get_document(
+            document_id=feature_job_setting_analysis.event_table_id
+        )
+
+        # get catalog info
+        catalog = await self.catalog_service.get_document(feature_job_setting_analysis.catalog_id)
+
+        return FeatureJobSettingAnalysisInfo(
+            created_at=feature_job_setting_analysis.created_at,
+            event_table_name=event_table.name,
+            analysis_options=feature_job_setting_analysis.analysis_options,
+            analysis_parameters=feature_job_setting_analysis.analysis_parameters,
+            recommendation=FeatureJobSetting(
+                blind_spot=f"{recommended_setting.blind_spot}s",
+                time_modulo_frequency=f"{recommended_setting.job_time_modulo_frequency}s",
+                frequency=f"{recommended_setting.frequency}s",
+            ),
+            catalog_name=catalog.name,
+            description=catalog.description,
         )
-        return info_document
 
     async def create_feature_job_setting_analysis(
         self,
         data: FeatureJobSettingAnalysisCreate,
     ) -> Task:
         """
         Create Feature JobSetting Analysis and store in persistent
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_list/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature_list/api.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,73 +8,91 @@
 
 import json
 from http import HTTPStatus
 
 from fastapi import APIRouter, File, Form, Query, Request, UploadFile
 
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.persistent import AuditDocumentList
 from featurebyte.routes.common.schema import (
     AuditLogSortByQuery,
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
     VersionQuery,
 )
-from featurebyte.schema.common.base import DeleteResponse
+from featurebyte.schema.common.base import DeleteResponse, DescriptionUpdate
 from featurebyte.schema.feature_list import (
     FeatureListCreate,
+    FeatureListCreateWithBatchFeatureCreation,
     FeatureListGetHistoricalFeatures,
+    FeatureListModelResponse,
     FeatureListNewVersionCreate,
     FeatureListPaginatedList,
     FeatureListPreview,
     FeatureListSQL,
     FeatureListUpdate,
 )
 from featurebyte.schema.info import FeatureListInfo
+from featurebyte.schema.task import Task
 
 router = APIRouter(prefix="/feature_list")
 
 
-@router.post("", response_model=FeatureListModel, status_code=HTTPStatus.CREATED)
+@router.post("", response_model=FeatureListModelResponse, status_code=HTTPStatus.CREATED)
 async def create_feature_list(
     request: Request, data: Union[FeatureListCreate, FeatureListNewVersionCreate]
-) -> FeatureListModel:
+) -> FeatureListModelResponse:
     """
     Create FeatureList
     """
     controller = request.state.app_container.feature_list_controller
-    feature_list: FeatureListModel = await controller.create_feature_list(data=data)
+    feature_list: FeatureListModelResponse = await controller.create_feature_list(data=data)
     return feature_list
 
 
-@router.get("/{feature_list_id}", response_model=FeatureListModel)
-async def get_feature_list(request: Request, feature_list_id: PydanticObjectId) -> FeatureListModel:
+@router.post("/batch", response_model=Task, status_code=HTTPStatus.CREATED)
+async def submit_feature_create_with_batch_feature_create_task(
+    request: Request, data: FeatureListCreateWithBatchFeatureCreation
+) -> Task:
+    """
+    Submit FeatureList create with batch feature create task
+    """
+    controller = request.state.app_container.feature_list_controller
+    task: Task = await controller.submit_feature_list_create_with_batch_feature_create_task(
+        data=data
+    )
+    return task
+
+
+@router.get("/{feature_list_id}", response_model=FeatureListModelResponse)
+async def get_feature_list(
+    request: Request, feature_list_id: PydanticObjectId
+) -> FeatureListModelResponse:
     """
     Get FeatureList
     """
     controller = request.state.app_container.feature_list_controller
-    feature_list: FeatureListModel = await controller.get(document_id=feature_list_id)
+    feature_list: FeatureListModelResponse = await controller.get(document_id=feature_list_id)
     return feature_list
 
 
-@router.patch("/{feature_list_id}", response_model=FeatureListModel)
+@router.patch("/{feature_list_id}", response_model=FeatureListModelResponse)
 async def update_feature_list(
     request: Request, feature_list_id: PydanticObjectId, data: FeatureListUpdate
-) -> FeatureListModel:
+) -> FeatureListModelResponse:
     """
     Update FeatureList
     """
     controller = request.state.app_container.feature_list_controller
-    feature_list: FeatureListModel = await controller.update_feature_list(
+    feature_list: FeatureListModelResponse = await controller.update_feature_list(
         feature_list_id=feature_list_id,
         data=data,
     )
     return feature_list
 
 
 @router.delete("/{feature_list_id}", status_code=HTTPStatus.OK)
@@ -221,10 +239,26 @@
     """
     Retrieve feature job status
     """
     controller = request.state.app_container.feature_list_controller
     result = await controller.get_feature_job_logs(
         feature_list_id=feature_list_id,
         hour_limit=hour_limit,
-        get_credential=request.state.get_credential,
     )
     return cast(Dict[str, Any], result)
+
+
+@router.patch("/{feature_list_id}/description", response_model=FeatureListModelResponse)
+async def update_feature_list_description(
+    request: Request,
+    feature_list_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureListModelResponse:
+    """
+    Update feature_list description
+    """
+    controller = request.state.app_container.feature_list_controller
+    feature_list: FeatureListModelResponse = await controller.update_description(
+        document_id=feature_list_id,
+        description=data.description,
+    )
+    return feature_list
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_list/controller.py` & `featurebyte-0.4.0/featurebyte/routes/feature_list/controller.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,51 +1,59 @@
 """
 FeatureList API route controller
 """
 from __future__ import annotations
 
-from typing import Any, Dict, Literal, Union
+from typing import Any, Dict, Literal, Optional, Union
 
 from http import HTTPStatus
 
 from bson.objectid import ObjectId
 from fastapi import UploadFile
 from fastapi.exceptions import HTTPException
 
 from featurebyte.common.utils import dataframe_from_arrow_stream
 from featurebyte.exception import (
     DocumentDeletionError,
+    DocumentNotFoundError,
     MissingPointInTimeColumnError,
     RequiredEntityNotProvidedError,
     TooRecentPointInTimeError,
 )
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.models.base import VersionIdentifier
-from featurebyte.models.feature import DefaultVersionMode, FeatureReadiness
 from featurebyte.models.feature_list import FeatureListModel, FeatureListStatus
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
 from featurebyte.routes.common.base import BaseDocumentController
+from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.feature_list import (
     FeatureListCreate,
+    FeatureListCreateWithBatchFeatureCreation,
     FeatureListGetHistoricalFeatures,
+    FeatureListModelResponse,
     FeatureListNewVersionCreate,
     FeatureListPaginatedList,
     FeatureListPreview,
     FeatureListServiceCreate,
     FeatureListSQL,
     FeatureListUpdate,
 )
 from featurebyte.schema.info import FeatureListInfo
+from featurebyte.schema.task import Task
+from featurebyte.schema.worker.task.feature_list_batch_feature_create import (
+    FeatureListCreateWithBatchFeatureCreationTaskPayload,
+)
 from featurebyte.service.deploy import DeployService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
+from featurebyte.service.feature_preview import FeaturePreviewService
 from featurebyte.service.feature_readiness import FeatureReadinessService
-from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
-from featurebyte.service.info import InfoService
-from featurebyte.service.preview import PreviewService
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
+from featurebyte.service.tile_job_log import TileJobLogService
 from featurebyte.service.version import VersionService
 
 
 # pylint: disable=too-many-instance-attributes
 class FeatureListController(
     BaseDocumentController[FeatureListModel, FeatureListService, FeatureListPaginatedList]
 ):
@@ -54,94 +62,135 @@
     """
 
     paginated_document_class = FeatureListPaginatedList
 
     # pylint: disable=too-many-arguments
     def __init__(
         self,
-        service: FeatureListService,
+        feature_list_service: FeatureListService,
         feature_list_namespace_service: FeatureListNamespaceService,
         feature_service: FeatureService,
         feature_readiness_service: FeatureReadinessService,
         deploy_service: DeployService,
-        preview_service: PreviewService,
+        feature_preview_service: FeaturePreviewService,
         version_service: VersionService,
-        info_service: InfoService,
-        feature_store_warehouse_service: FeatureStoreWarehouseService,
+        tile_job_log_service: TileJobLogService,
+        task_controller: TaskController,
     ):
-        super().__init__(service)
+        super().__init__(feature_list_service)
         self.feature_list_namespace_service = feature_list_namespace_service
         self.feature_service = feature_service
         self.feature_readiness_service = feature_readiness_service
         self.deploy_service = deploy_service
-        self.preview_service = preview_service
+        self.feature_preview_service = feature_preview_service
         self.version_service = version_service
-        self.info_service = info_service
-        self.feature_store_warehouse_service = feature_store_warehouse_service
+        self.tile_job_log_service = tile_job_log_service
+        self.task_controller = task_controller
+
+    async def submit_feature_list_create_with_batch_feature_create_task(
+        self, data: FeatureListCreateWithBatchFeatureCreation
+    ) -> Optional[Task]:
+        """
+        Submit feature list creation with batch feature creation task
+
+        Parameters
+        ----------
+        data: FeatureListCreateWithBatchFeatureCreation
+            Feature list creation with batch feature creation payload
+
+        Returns
+        -------
+        Optional[Task]
+            Task object
+        """
+        payload = FeatureListCreateWithBatchFeatureCreationTaskPayload(
+            **{
+                **data.dict(by_alias=True),
+                "user_id": self.service.user.id,
+                "catalog_id": self.service.catalog_id,
+            }
+        )
+        task_id = await self.task_controller.task_manager.submit(payload=payload)
+        return await self.task_controller.task_manager.get_task(task_id=str(task_id))
 
     async def create_feature_list(
         self, data: Union[FeatureListCreate, FeatureListNewVersionCreate]
-    ) -> FeatureListModel:
+    ) -> FeatureListModelResponse:
         """
         Create FeatureList at persistent (GitDB or MongoDB)
 
         Parameters
         ----------
         data: FeatureListCreate | FeatureListNewVersionCreate
             Feature list creation payload
 
         Returns
         -------
-        FeatureListModel
+        FeatureListModelResponse
             Newly created feature list object
         """
         if isinstance(data, FeatureListCreate):
             document = await self.service.create_document(
-                data=FeatureListServiceCreate(**data.json_dict())
+                data=FeatureListServiceCreate(**data.dict(by_alias=True))
             )
         else:
             document = await self.version_service.create_new_feature_list_version(data=data)
 
         # update feature namespace readiness due to introduction of new feature list
         await self.feature_readiness_service.update_feature_list_namespace(
             feature_list_namespace_id=document.feature_list_namespace_id,
             return_document=False,
         )
-        return document
+        return await self.get(document_id=document.id)
+
+    async def get(
+        self, document_id: ObjectId, exception_detail: str | None = None
+    ) -> FeatureListModelResponse:
+        document = await self.service.get_document(
+            document_id=document_id,
+            exception_detail=exception_detail,
+        )
+        namespace = await self.feature_list_namespace_service.get_document(
+            document_id=document.feature_list_namespace_id
+        )
+        output = FeatureListModelResponse(
+            **document.dict(by_alias=True),
+            is_default=namespace.default_feature_list_id == document.id,
+        )
+        return output
 
     async def update_feature_list(
         self,
         feature_list_id: ObjectId,
         data: FeatureListUpdate,
-    ) -> FeatureListModel:
+    ) -> FeatureListModelResponse:
         """
         Update FeatureList at persistent
 
         Parameters
         ----------
         feature_list_id: ObjectId
             FeatureList ID
         data: FeatureListUpdate
             FeatureList update payload
 
         Returns
         -------
-        FeatureListModel
+        FeatureListModelResponse
             FeatureList object with updated attribute(s)
         """
         if data.make_production_ready:
             feature_list = await self.get(document_id=feature_list_id)
             for feature_id in feature_list.feature_ids:
                 await self.feature_readiness_service.update_feature(
                     feature_id=feature_id,
                     readiness=FeatureReadiness.PRODUCTION_READY,
                     ignore_guardrails=bool(data.ignore_guardrails),
                     return_document=False,
                 )
-
         return await self.get(document_id=feature_list_id)
 
     async def delete_feature_list(self, feature_list_id: ObjectId) -> None:
         """
         Delete FeatureList at persistent
 
         Raises
@@ -167,35 +216,29 @@
         ):
             raise DocumentDeletionError(
                 "Feature list is the default feature list of the feature list namespace and the "
                 "default version mode is manual. Please set another feature list as the default feature list "
                 "or change the default version mode to auto."
             )
 
-        # use transaction to ensure atomicity
-        async with self.service.persistent.start_transaction():
-            await self.service.delete_document(document_id=feature_list_id)
+        await self.service.delete_document(document_id=feature_list_id)
+        try:
             await self.feature_readiness_service.update_feature_list_namespace(
                 feature_list_namespace_id=feature_list.feature_list_namespace_id,
                 deleted_feature_list_ids=[feature_list_id],
                 return_document=False,
             )
-            feature_list_namespace = await self.feature_list_namespace_service.get_document(
-                document_id=feature_list.feature_list_namespace_id
-            )
-            if not feature_list_namespace.feature_list_ids:
-                # delete feature list namespace if it has no more feature list
-                await self.feature_list_namespace_service.delete_document(
-                    document_id=feature_list.feature_list_namespace_id
-                )
+        except DocumentNotFoundError:
+            # if feature list namespace is deleted, do nothing
+            pass
 
     async def list_feature_lists(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         search: str | None = None,
         name: str | None = None,
         version: str | None = None,
         feature_list_namespace_id: ObjectId | None = None,
     ) -> FeatureListPaginatedList:
@@ -222,31 +265,59 @@
             Feature list namespace ID to be used in filtering
 
         Returns
         -------
         FeatureListPaginatedList
             List of documents fulfilled the filtering condition
         """
+        # pylint: disable=too-many-locals
         params: Dict[str, Any] = {"search": search, "name": name}
         if version:
             params["version"] = VersionIdentifier.from_str(version).dict()
 
         if feature_list_namespace_id:
             query_filter = params.get("query_filter", {}).copy()
             query_filter["feature_list_namespace_id"] = feature_list_namespace_id
             params["query_filter"] = query_filter
 
-        return await self.list(
+        # list documents from persistent
+        document_data = await self.service.list_documents_as_dict(
             page=page,
             page_size=page_size,
             sort_by=sort_by,
             sort_dir=sort_dir,
             **params,
         )
 
+        # prepare mappings to add additional attributes
+        namespace_ids = {
+            document["feature_list_namespace_id"] for document in document_data["data"]
+        }
+        namespace_id_to_default_id = {}
+        async for namespace in self.feature_list_namespace_service.list_documents_as_dict_iterator(
+            query_filter={"_id": {"$in": list(namespace_ids)}}
+        ):
+            namespace_id_to_default_id[namespace["_id"]] = namespace["default_feature_list_id"]
+
+        # prepare output
+        output = []
+        for feature_list in document_data["data"]:
+            default_feature_list_id = namespace_id_to_default_id.get(
+                feature_list["feature_list_namespace_id"]
+            )
+            output.append(
+                FeatureListModelResponse(
+                    **feature_list,
+                    is_default=default_feature_list_id == feature_list["_id"],
+                )
+            )
+
+        document_data["data"] = output
+        return self.paginated_document_class(**document_data)
+
     async def preview(
         self, featurelist_preview: FeatureListPreview, get_credential: Any
     ) -> dict[str, Any]:
         """
         Preview a Feature List
 
         Parameters
@@ -263,15 +334,15 @@
 
         Raises
         ------
         HTTPException
             Invalid request payload
         """
         try:
-            return await self.preview_service.preview_featurelist(
+            return await self.feature_preview_service.preview_featurelist(
                 featurelist_preview=featurelist_preview, get_credential=get_credential
             )
         except (MissingPointInTimeColumnError, RequiredEntityNotProvidedError) as exc:
             raise HTTPException(
                 status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=exc.args[0]
             ) from exc
 
@@ -290,15 +361,15 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         InfoDocument
         """
-        info_document = await self.info_service.get_feature_list_info(
+        info_document = await self.service.get_feature_list_info(
             document_id=document_id, verbose=verbose
         )
         return info_document
 
     async def sql(self, featurelist_sql: FeatureListSQL) -> str:
         """
         Preview a Feature List
@@ -309,15 +380,15 @@
             FeatureListSQL object
 
         Returns
         -------
         str
             SQL statements
         """
-        return await self.preview_service.featurelist_sql(featurelist_sql=featurelist_sql)
+        return await self.feature_preview_service.featurelist_sql(featurelist_sql=featurelist_sql)
 
     async def get_historical_features_sql(
         self,
         observation_set: UploadFile,
         featurelist_get_historical_features: FeatureListGetHistoricalFeatures,
     ) -> str:
         """
@@ -337,52 +408,48 @@
 
         Raises
         ------
         HTTPException
             Invalid request payload
         """
         try:
-            return await self.preview_service.get_historical_features_sql(
+            return await self.feature_preview_service.get_historical_features_sql(
                 observation_set=dataframe_from_arrow_stream(observation_set.file),
                 featurelist_get_historical_features=featurelist_get_historical_features,
             )
         except (MissingPointInTimeColumnError, TooRecentPointInTimeError) as exc:
             raise HTTPException(
                 status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=exc.args[0]
             ) from exc
 
     async def get_feature_job_logs(
-        self, feature_list_id: ObjectId, hour_limit: int, get_credential: Any
+        self, feature_list_id: ObjectId, hour_limit: int
     ) -> dict[str, Any]:
         """
         Retrieve data preview for query graph node
 
         Parameters
         ----------
         feature_list_id: ObjectId
             FeatureList Id
         hour_limit: int
             Limit in hours on the job history to fetch
-        get_credential: Any
-            Get credential handler function
 
         Returns
         -------
         dict[str, Any]
             Dataframe converted to json string
         """
         feature_list = await self.service.get_document(feature_list_id)
         assert feature_list.feature_clusters
         assert len(feature_list.feature_clusters) == 1
 
         features = []
-        async for doc in self.feature_service.list_documents_iterator(
+        async for doc in self.feature_service.list_documents_as_dict_iterator(
             query_filter={"_id": {"$in": feature_list.feature_ids}}
         ):
             features.append(ExtendedFeatureModel(**doc))
 
-        return await self.feature_store_warehouse_service.get_feature_job_logs(
-            feature_store_id=feature_list.feature_clusters[0].feature_store_id,
+        return await self.tile_job_log_service.get_feature_job_logs(
             features=features,
             hour_limit=hour_limit,
-            get_credential=get_credential,
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_list_namespace/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature_list_namespace/api.py`

 * *Files 13% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.feature_list_namespace import (
     FeatureListNamespaceList,
     FeatureListNamespaceModelResponse,
     FeatureListNamespaceUpdate,
 )
 from featurebyte.schema.info import FeatureListNamespaceInfo
 
@@ -126,7 +127,26 @@
     """
     controller = request.state.app_container.feature_list_namespace_controller
     info = await controller.get_info(
         document_id=feature_list_namespace_id,
         verbose=verbose,
     )
     return cast(FeatureListNamespaceInfo, info)
+
+
+@router.patch(
+    "/{feature_list_namespace_id}/description", response_model=FeatureListNamespaceModelResponse
+)
+async def update_feature_list_namespace_description(
+    request: Request,
+    feature_list_namespace_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureListNamespaceModelResponse:
+    """
+    Update feature_list_namespace description
+    """
+    controller = request.state.app_container.feature_list_namespace_controller
+    feature_list_namespace: FeatureListNamespaceModelResponse = await controller.update_description(
+        document_id=feature_list_namespace_id,
+        description=data.description,
+    )
+    return feature_list_namespace
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_list_namespace/controller.py` & `featurebyte-0.4.0/featurebyte/routes/feature_list_namespace/controller.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,19 +4,19 @@
 from __future__ import annotations
 
 from typing import Any, Literal, cast
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
-from featurebyte.models.feature import DefaultVersionMode
 from featurebyte.models.feature_list import FeatureListNamespaceModel
+from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.routes.common.base import (
     BaseDocumentController,
-    DerivePrimaryEntityMixin,
+    DerivePrimaryEntityHelper,
     PaginatedDocument,
 )
 from featurebyte.schema.feature_list_namespace import (
     FeatureListNamespaceList,
     FeatureListNamespaceModelResponse,
     FeatureListNamespaceServiceUpdate,
     FeatureListNamespaceUpdate,
@@ -24,83 +24,85 @@
 from featurebyte.schema.info import FeatureListNamespaceInfo
 from featurebyte.service.default_version_mode import DefaultVersionModeService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_list_status import FeatureListStatusService
 from featurebyte.service.feature_readiness import FeatureReadinessService
-from featurebyte.service.info import InfoService
-from featurebyte.service.mixin import Document
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE, Document
 
 
 class FeatureListNamespaceController(
     BaseDocumentController[
         FeatureListNamespaceModelResponse, FeatureListNamespaceService, FeatureListNamespaceList
-    ],
-    DerivePrimaryEntityMixin,
+    ]
 ):
     """
     FeatureList controller
     """
 
     paginated_document_class = FeatureListNamespaceList
 
     def __init__(
         self,
-        service: FeatureListNamespaceService,
+        feature_list_namespace_service: FeatureListNamespaceService,
         entity_service: EntityService,
         feature_list_service: FeatureListService,
         default_version_mode_service: DefaultVersionModeService,
         feature_readiness_service: FeatureReadinessService,
         feature_list_status_service: FeatureListStatusService,
-        info_service: InfoService,
+        derive_primary_entity_helper: DerivePrimaryEntityHelper,
     ):
-        super().__init__(service)
+        super().__init__(feature_list_namespace_service)
         self.entity_service = entity_service
         self.feature_list_service = feature_list_service
         self.default_version_mode_service = default_version_mode_service
         self.feature_readiness_service = feature_readiness_service
         self.feature_list_status_service = feature_list_status_service
-        self.info_service = info_service
+        self.derive_primary_entity_helper = derive_primary_entity_helper
 
     async def get(
         self,
         document_id: ObjectId,
         exception_detail: str | None = None,
     ) -> Document:
         document = await self.service.get_document(
             document_id=document_id, exception_detail=exception_detail
         )
         output = FeatureListNamespaceModelResponse(
-            **document.json_dict(),
-            primary_entity_ids=await self.derive_primary_entity_ids(entity_ids=document.entity_ids),
+            **document.dict(by_alias=True),
+            primary_entity_ids=await self.derive_primary_entity_helper.derive_primary_entity_ids(
+                entity_ids=document.entity_ids
+            ),
         )
         return cast(Document, output)
 
     async def list(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         **kwargs: Any,
     ) -> PaginatedDocument:
-        document_data = await self.service.list_documents(
+        document_data = await self.service.list_documents_as_dict(
             page=page,
             page_size=page_size,
             sort_by=sort_by,
             sort_dir=sort_dir,
             **kwargs,
         )
 
         # compute primary entity ids of each feature list namespace
-        entity_id_to_entity = await self.get_entity_id_to_entity(doc_list=document_data["data"])
+        entity_id_to_entity = await self.derive_primary_entity_helper.get_entity_id_to_entity(
+            doc_list=document_data["data"]
+        )
         output = []
         for feature_list_namespace in document_data["data"]:
-            primary_entity_ids = await self.derive_primary_entity_ids(
+            primary_entity_ids = await self.derive_primary_entity_helper.derive_primary_entity_ids(
                 entity_ids=feature_list_namespace["entity_ids"],
                 entity_id_to_entity=entity_id_to_entity,
             )
             output.append(
                 FeatureListNamespaceModelResponse(
                     **feature_list_namespace,
                     primary_entity_ids=primary_entity_ids,
@@ -185,11 +187,11 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         InfoDocument
         """
-        info_document = await self.info_service.get_feature_list_namespace_info(
+        info_document = await self.service.get_feature_list_namespace_info(
             document_id=document_id, verbose=verbose
         )
         return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_namespace/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature_namespace/api.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.feature_namespace import (
     FeatureNamespaceList,
     FeatureNamespaceModelResponse,
     FeatureNamespaceUpdate,
 )
 from featurebyte.schema.info import FeatureNamespaceInfo
 
@@ -122,7 +123,24 @@
     """
     controller = request.state.app_container.feature_namespace_controller
     info = await controller.get_info(
         document_id=feature_namespace_id,
         verbose=verbose,
     )
     return cast(FeatureNamespaceInfo, info)
+
+
+@router.patch("/{feature_namespace_id}/description", response_model=FeatureNamespaceModelResponse)
+async def update_feature_namespace_description(
+    request: Request,
+    feature_namespace_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureNamespaceModelResponse:
+    """
+    Update feature_namespace description
+    """
+    controller = request.state.app_container.feature_namespace_controller
+    feature_namespace: FeatureNamespaceModelResponse = await controller.update_description(
+        document_id=feature_namespace_id,
+        description=data.description,
+    )
+    return feature_namespace
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_namespace/controller.py` & `featurebyte-0.4.0/featurebyte/routes/feature_namespace/controller.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,119 +1,127 @@
 """
 FeatureNamespace API route controller
 """
 from __future__ import annotations
 
-from typing import Any, Literal, cast
+from typing import Any, Literal, Optional, cast
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
-from featurebyte.models.feature import DefaultVersionMode, FeatureModel
+from featurebyte.models.base import VersionIdentifier
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
+from featurebyte.routes.catalog.catalog_name_injector import CatalogNameInjector
 from featurebyte.routes.common.base import (
     BaseDocumentController,
-    DerivePrimaryEntityMixin,
+    DerivePrimaryEntityHelper,
     PaginatedDocument,
 )
 from featurebyte.schema.feature_namespace import (
     FeatureNamespaceList,
     FeatureNamespaceModelResponse,
     FeatureNamespaceServiceUpdate,
     FeatureNamespaceUpdate,
 )
-from featurebyte.schema.info import FeatureNamespaceInfo
+from featurebyte.schema.info import EntityBriefInfoList, FeatureNamespaceInfo, TableBriefInfoList
 from featurebyte.service.default_version_mode import DefaultVersionModeService
-from featurebyte.service.entity import EntityService
+from featurebyte.service.entity import EntityService, get_primary_entity_from_entities
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
 from featurebyte.service.feature_readiness import FeatureReadinessService
-from featurebyte.service.info import InfoService
-from featurebyte.service.mixin import Document
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE, Document
+from featurebyte.service.table import TableService
 
 
 class FeatureNamespaceController(
     BaseDocumentController[
         FeatureNamespaceModelResponse, FeatureNamespaceService, FeatureNamespaceList
-    ],
-    DerivePrimaryEntityMixin,
+    ]
 ):
     """
     FeatureName controller
     """
 
     paginated_document_class = FeatureNamespaceList
 
     def __init__(
         self,
-        service: FeatureNamespaceService,
+        feature_namespace_service: FeatureNamespaceService,
         entity_service: EntityService,
         feature_service: FeatureService,
         default_version_mode_service: DefaultVersionModeService,
         feature_readiness_service: FeatureReadinessService,
-        info_service: InfoService,
+        table_service: TableService,
+        derive_primary_entity_helper: DerivePrimaryEntityHelper,
+        catalog_name_injector: CatalogNameInjector,
     ):
-        super().__init__(service)
+        super().__init__(feature_namespace_service)
         self.entity_service = entity_service
         self.feature_service = feature_service
         self.default_version_mode_service = default_version_mode_service
         self.feature_readiness_service = feature_readiness_service
-        self.info_service = info_service
+        self.table_service = table_service
+        self.derive_primary_entity_helper = derive_primary_entity_helper
+        self.catalog_name_injector = catalog_name_injector
 
     async def get(
         self,
         document_id: ObjectId,
         exception_detail: str | None = None,
     ) -> Document:
         document = await self.service.get_document(
             document_id=document_id,
             exception_detail=exception_detail,
         )
         default_feature = await self.feature_service.get_document(
             document_id=document.default_feature_id
         )
         output = FeatureNamespaceModelResponse(
-            **document.json_dict(),
+            **document.dict(by_alias=True),
             primary_table_ids=default_feature.primary_table_ids,
-            primary_entity_ids=await self.derive_primary_entity_ids(entity_ids=document.entity_ids),
+            primary_entity_ids=await self.derive_primary_entity_helper.derive_primary_entity_ids(
+                entity_ids=document.entity_ids
+            ),
         )
         return cast(Document, output)
 
     async def list(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: Literal["asc", "desc"] = "desc",
         **kwargs: Any,
     ) -> PaginatedDocument:
-        document_data = await self.service.list_documents(
+        document_data = await self.service.list_documents_as_dict(
             page=page,
             page_size=page_size,
             sort_by=sort_by,
             sort_dir=sort_dir,
             **kwargs,
         )
 
         # get all the default features & entities
         default_feature_ids = set(
             document["default_feature_id"] for document in document_data["data"]
         )
-        entity_id_to_entity = await self.get_entity_id_to_entity(doc_list=document_data["data"])
+        entity_id_to_entity = await self.derive_primary_entity_helper.get_entity_id_to_entity(
+            doc_list=document_data["data"]
+        )
 
         feature_id_to_primary_table_ids = {}
-        async for feature_dict in self.feature_service.list_documents_iterator(
+        async for feature in self.feature_service.list_documents_iterator(
             query_filter={"_id": {"$in": list(default_feature_ids)}}
         ):
-            feature = FeatureModel(**feature_dict)
             feature_id_to_primary_table_ids[feature.id] = feature.primary_table_ids
 
         # construct primary entity IDs and primary table IDs & add these attributes to feature namespace docs
         output = []
         for feature_namespace in document_data["data"]:
-            primary_entity_ids = await self.derive_primary_entity_ids(
+            primary_entity_ids = await self.derive_primary_entity_helper.derive_primary_entity_ids(
                 entity_ids=feature_namespace["entity_ids"], entity_id_to_entity=entity_id_to_entity
             )
             default_feature_id = feature_namespace["default_feature_id"]
             primary_table_ids = feature_id_to_primary_table_ids.get(default_feature_id, [])
             output.append(
                 FeatureNamespaceModelResponse(
                     **feature_namespace,
@@ -160,14 +168,34 @@
         if data.default_feature_id:
             feature_namespace = await self.service.get_document(document_id=feature_namespace_id)
             if feature_namespace.default_version_mode != DefaultVersionMode.MANUAL:
                 raise DocumentUpdateError(
                     "Cannot set default feature ID when default version mode is not MANUAL."
                 )
 
+            # check new default feature ID exists & make sure it is the highest readiness level among all versions
+            new_default_feature = await self.feature_service.get_document(
+                document_id=data.default_feature_id
+            )
+            max_readiness = FeatureReadiness(new_default_feature.readiness)
+            version: Optional[str] = None
+            async for feature_dict in self.feature_service.list_documents_as_dict_iterator(
+                query_filter={"_id": {"$in": feature_namespace.feature_ids}}
+            ):
+                max_readiness = max(max_readiness, FeatureReadiness(feature_dict["readiness"]))
+                if feature_dict["readiness"] == max_readiness:
+                    version = VersionIdentifier(**feature_dict["version"]).to_str()
+
+            if new_default_feature.readiness != max_readiness:
+                raise DocumentUpdateError(
+                    f"Cannot set default feature ID to {new_default_feature.id} "
+                    f"because its readiness level ({new_default_feature.readiness}) "
+                    f"is lower than the readiness level of version {version} ({max_readiness.value})."
+                )
+
             # update feature namespace default feature ID and update feature readiness
             await self.service.update_document(
                 document_id=feature_namespace_id,
                 data=FeatureNamespaceServiceUpdate(default_feature_id=data.default_feature_id),
             )
             await self.feature_readiness_service.update_feature_namespace(
                 feature_namespace_id=feature_namespace_id
@@ -190,11 +218,45 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         InfoDocument
         """
-        info_document = await self.info_service.get_feature_namespace_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        namespace = await self.service.get_document(document_id=document_id)
+        entities = await self.entity_service.list_documents_as_dict(
+            page=1, page_size=0, query_filter={"_id": {"$in": namespace.entity_ids}}
+        )
+        primary_entity = get_primary_entity_from_entities(entities=entities)
+
+        tables = await self.table_service.list_documents_as_dict(
+            page=1, page_size=0, query_filter={"_id": {"$in": namespace.table_ids}}
+        )
+
+        # Add catalog name to entities and tables
+        catalog_name, updated_docs = await self.catalog_name_injector.add_name(
+            namespace.catalog_id, [entities, tables]
+        )
+        entities, tables = updated_docs
+
+        # derive primary tables
+        table_id_to_doc = {table["_id"]: table for table in tables["data"]}
+        feature = await self.feature_service.get_document(document_id=namespace.default_feature_id)
+        primary_input_nodes = feature.graph.get_primary_input_nodes(node_name=feature.node_name)
+        primary_tables = [table_id_to_doc[node.parameters.id] for node in primary_input_nodes]
+
+        return FeatureNamespaceInfo(
+            name=namespace.name,
+            created_at=namespace.created_at,
+            updated_at=namespace.updated_at,
+            entities=EntityBriefInfoList.from_paginated_data(entities),
+            primary_entity=EntityBriefInfoList.from_paginated_data(primary_entity),
+            tables=TableBriefInfoList.from_paginated_data(tables),
+            primary_table=primary_tables,
+            default_version_mode=namespace.default_version_mode,
+            default_feature_id=namespace.default_feature_id,
+            dtype=namespace.dtype,
+            version_count=len(namespace.feature_ids),
+            catalog_name=catalog_name,
+            description=namespace.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_store/api.py` & `featurebyte-0.4.0/featurebyte/routes/feature_store/api.py`

 * *Files 5% similar despite different names*

```diff
@@ -19,14 +19,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.feature_store import (
     FeatureStoreCreate,
     FeatureStoreList,
     FeatureStorePreview,
     FeatureStoreSample,
     FeatureStoreShape,
 )
@@ -37,17 +38,15 @@
 
 @router.post("", response_model=FeatureStoreModel, status_code=HTTPStatus.CREATED)
 async def create_feature_store(request: Request, data: FeatureStoreCreate) -> FeatureStoreModel:
     """
     Create Feature Store
     """
     controller = request.state.app_container.feature_store_controller
-    feature_store: FeatureStoreModel = await controller.create_feature_store(
-        data=data, get_credential=request.state.get_credential
-    )
+    feature_store: FeatureStoreModel = await controller.create_feature_store(data=data)
     return feature_store
 
 
 @router.get("/{feature_store_id}", response_model=FeatureStoreModel)
 async def get_feature_store(
     request: Request, feature_store_id: PydanticObjectId
 ) -> FeatureStoreModel:
@@ -269,7 +268,24 @@
     controller = request.state.app_container.feature_store_controller
     return cast(
         Dict[str, Any],
         await controller.describe(
             sample=sample, size=size, seed=seed, get_credential=request.state.get_credential
         ),
     )
+
+
+@router.patch("/{feature_store_id}/description", response_model=FeatureStoreModel)
+async def update_feature_store_description(
+    request: Request,
+    feature_store_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> FeatureStoreModel:
+    """
+    Update feature_store description
+    """
+    controller = request.state.app_container.feature_store_controller
+    feature_store: FeatureStoreModel = await controller.update_description(
+        document_id=feature_store_id,
+        description=data.description,
+    )
+    return feature_store
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/feature_store/controller.py` & `featurebyte-0.4.0/featurebyte/routes/feature_store/controller.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     FeatureStoreSample,
     FeatureStoreShape,
 )
 from featurebyte.schema.info import FeatureStoreInfo
 from featurebyte.service.credential import CredentialService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
-from featurebyte.service.info import InfoService
 from featurebyte.service.preview import PreviewService
 from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.session_validator import SessionValidatorService
 from featurebyte.session.base import MetadataSchemaInitializer
 
 logger = get_logger(__name__)
 
@@ -40,50 +39,49 @@
     FeatureStore controller
     """
 
     paginated_document_class = FeatureStoreList
 
     def __init__(
         self,
-        service: FeatureStoreService,
+        feature_store_service: FeatureStoreService,
         preview_service: PreviewService,
-        info_service: InfoService,
         session_manager_service: SessionManagerService,
         session_validator_service: SessionValidatorService,
         feature_store_warehouse_service: FeatureStoreWarehouseService,
         credential_service: CredentialService,
     ):
-        super().__init__(service)
+        super().__init__(feature_store_service)
         self.preview_service = preview_service
-        self.info_service = info_service
         self.session_manager_service = session_manager_service
         self.session_validator_service = session_validator_service
         self.feature_store_warehouse_service = feature_store_warehouse_service
         self.credential_service = credential_service
 
     async def create_feature_store(
         self,
         data: FeatureStoreCreate,
-        get_credential: Any,
     ) -> FeatureStoreModel:
         """
         Create Feature Store at persistent
 
         Parameters
         ----------
         data: FeatureStoreCreate
             FeatureStore creation payload
-        get_credential: Any
-            credential handler function
 
         Returns
         -------
         FeatureStoreModel
             Newly created feature store document
 
+        Raises
+        ------
+        Exception
+            If feature store already exists or initialization fails
         """
         # Validate whether the feature store trying to be created, collides with another feature store
         #
         # OUTCOMES
         # exist in persistent | exist in DWH |           DWH                |     persistent
         #        Y            |      N       |          create              | error on write
         #        N            |      N       |          create              | write
@@ -92,74 +90,73 @@
         #        N            |      Y       | if matches in DWH, no error  | write
         #                                    | if not, error                |
         #
         # Validate that feature store ID isn't claimed by the working schema.
         # If the feature store ID is already in use, this will throw an error.
         # Create the new feature store. If one already exists, we'll throw an error here.
         logger.debug("Start create_feature_store")
-
         document = await self.service.create_document(data)
-        # Check credentials
-        credential = CredentialModel(
-            name=document.name,
-            feature_store_id=document.id,
-            database_credential=data.database_credential,
-            storage_credential=data.storage_credential,
-        )
+        credential_doc = None
+        try:
+            # Check credentials
+            credential = CredentialModel(
+                name=document.name,
+                feature_store_id=document.id,
+                database_credential=data.database_credential,
+                storage_credential=data.storage_credential,
+            )
 
-        async def _updated_get_credential(user_id: str, feature_store_name: str) -> Any:
-            """
-            Updated get_credential will try to look up the credentials from config.
-
-            If there are credentials in the config, we will ignore whatever is passed in here.
-            If not, we will use the params that are passed in.
-
-            Parameters
-            ----------
-            user_id: str
-                user id
-            feature_store_name: str
-                feature store name
-
-            Returns
-            -------
-            Any
-                credentials
-            """
-            cred = await get_credential(user_id, feature_store_name)
-            if cred is not None:
-                return cred
-            return credential
-
-        get_credential_to_use = _updated_get_credential
-        await self.session_validator_service.validate_feature_store_id_not_used_in_warehouse(
-            feature_store_name=data.name,
-            session_type=data.type,
-            details=data.details,
-            get_credential=get_credential_to_use,
-            users_feature_store_id=document.id,
-        )
-        logger.debug("End validate_feature_store_id_not_used_in_warehouse")
+            async def _temp_get_credential(user_id: str, feature_store_name: str) -> Any:
+                """
+                Use the temporary credential to try to initialize the session.
+
+                Parameters
+                ----------
+                user_id: str
+                    user id
+                feature_store_name: str
+                    feature store name
+
+                Returns
+                -------
+                Any
+                    credentials
+                """
+                _ = user_id, feature_store_name
+                return credential
+
+            await self.session_validator_service.validate_feature_store_id_not_used_in_warehouse(
+                feature_store_name=data.name,
+                session_type=data.type,
+                details=data.details,
+                users_feature_store_id=document.id,
+                get_credential=_temp_get_credential,
+            )
+            logger.debug("End validate_feature_store_id_not_used_in_warehouse")
 
-        async with self.service.persistent.start_transaction():
-            logger.debug("Start transaction")
-            # Retrieve a session for initializing
             session = await self.session_manager_service.get_feature_store_session(
                 feature_store=FeatureStoreModel(
                     name=data.name, type=data.type, details=data.details
                 ),
-                get_credential=get_credential_to_use,
+                get_credential=_temp_get_credential,
             )
             # Try to persist credential
-            await self.credential_service.create_document(
-                data=CredentialCreate(**credential.json_dict())
+            credential_doc = await self.credential_service.create_document(
+                data=CredentialCreate(**credential.dict(by_alias=True))
             )
+
             # If no error thrown from creating, try to create the metadata table with the feature store ID.
             metadata_schema_initializer = MetadataSchemaInitializer(session)
             await metadata_schema_initializer.update_feature_store_id(str(document.id))
+        except Exception:
+            # If there is an error, delete the feature store + credential and re-raise the error.
+            await self.service.delete_document(document.id)
+            if credential_doc:
+                await self.credential_service.delete_document(credential_doc.id)
+            raise
 
         return document
 
     async def list_databases(
         self,
         feature_store: FeatureStoreModel,
         get_credential: Any,
@@ -391,11 +388,11 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         InfoDocument
         """
-        info_document = await self.info_service.get_feature_store_info(
+        info_document = await self.service.get_feature_store_info(
             document_id=document_id, verbose=verbose
         )
         return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/historical_feature_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/historical_feature_table/api.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.historical_feature_table import (
     HistoricalFeatureTableCreate,
     HistoricalFeatureTableList,
 )
 from featurebyte.schema.info import HistoricalFeatureTableInfo
 from featurebyte.schema.task import Task
 
@@ -41,18 +42,17 @@
     observation_set: Optional[UploadFile] = None,
 ) -> Task:
     """
     Create HistoricalFeatureTable by submitting a materialization task
     """
     data = HistoricalFeatureTableCreate(**json.loads(payload))
     controller = request.state.app_container.historical_feature_table_controller
-    task_submit: Task = await controller.create_historical_feature_table(
+    task_submit: Task = await controller.create_table(
         data=data,
         observation_set=observation_set,
-        temp_storage=request.state.get_temp_storage(),
     )
     return task_submit
 
 
 @router.get("/{historical_feature_table_id}", response_model=HistoricalFeatureTableModel)
 async def get_historical_feature_table(
     request: Request, historical_feature_table_id: PydanticObjectId
@@ -152,7 +152,26 @@
     """
     controller = request.state.app_container.historical_feature_table_controller
     result: StreamingResponse = await controller.download_materialized_table(
         document_id=historical_feature_table_id,
         get_credential=request.state.get_credential,
     )
     return result
+
+
+@router.patch(
+    "/{historical_feature_table_id}/description", response_model=HistoricalFeatureTableModel
+)
+async def update_historical_feature_table_description(
+    request: Request,
+    historical_feature_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> HistoricalFeatureTableModel:
+    """
+    Update historical_feature_table description
+    """
+    controller = request.state.app_container.historical_feature_table_controller
+    historical_feature_table: HistoricalFeatureTableModel = await controller.update_description(
+        document_id=historical_feature_table_id,
+        description=data.description,
+    )
+    return historical_feature_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/historical_feature_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/batch_feature_table/controller.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,144 +1,127 @@
 """
-HistoricalTable API route controller
+BatchFeatureTable API route controller
 """
 from __future__ import annotations
 
-from typing import Optional
-
-from http import HTTPStatus
-
 from bson import ObjectId
-from fastapi import HTTPException, UploadFile
 
-from featurebyte.common.utils import dataframe_from_arrow_stream
-from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
+from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.routes.common.base_materialized_table import BaseMaterializedTableController
 from featurebyte.routes.task.controller import TaskController
-from featurebyte.schema.historical_feature_table import (
-    HistoricalFeatureTableCreate,
-    HistoricalFeatureTableList,
-)
-from featurebyte.schema.info import HistoricalFeatureTableInfo
+from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate, BatchFeatureTableList
+from featurebyte.schema.info import BatchFeatureTableInfo
 from featurebyte.schema.task import Task
+from featurebyte.service.batch_feature_table import BatchFeatureTableService
+from featurebyte.service.batch_request_table import BatchRequestTableService
+from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity_validation import EntityValidationService
+from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
-from featurebyte.service.info import InfoService
-from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.preview import PreviewService
-from featurebyte.storage import Storage
 
 
-class HistoricalFeatureTableController(
+class BatchFeatureTableController(
     BaseMaterializedTableController[
-        HistoricalFeatureTableModel, HistoricalFeatureTableService, HistoricalFeatureTableList
+        BatchFeatureTableModel, BatchFeatureTableService, BatchFeatureTableList
     ],
 ):
     """
-    HistoricalFeatureTable Controller
+    BatchFeatureTable Controller
     """
 
-    paginated_document_class = HistoricalFeatureTableList
+    paginated_document_class = BatchFeatureTableList
 
     def __init__(
         self,
-        service: HistoricalFeatureTableService,
+        batch_feature_table_service: BatchFeatureTableService,
         preview_service: PreviewService,
         feature_store_service: FeatureStoreService,
-        observation_table_service: ObservationTableService,
+        feature_list_service: FeatureListService,
+        batch_request_table_service: BatchRequestTableService,
+        deployment_service: DeploymentService,
         entity_validation_service: EntityValidationService,
-        info_service: InfoService,
         task_controller: TaskController,
     ):
-        super().__init__(service=service, preview_service=preview_service)
+        super().__init__(service=batch_feature_table_service, preview_service=preview_service)
         self.feature_store_service = feature_store_service
-        self.observation_table_service = observation_table_service
+        self.feature_list_service = feature_list_service
+        self.batch_request_table_service = batch_request_table_service
+        self.deployment_service = deployment_service
         self.entity_validation_service = entity_validation_service
-        self.info_service = info_service
         self.task_controller = task_controller
 
-    async def create_historical_feature_table(
+    async def create_batch_feature_table(
         self,
-        data: HistoricalFeatureTableCreate,
-        observation_set: Optional[UploadFile],
-        temp_storage: Storage,
+        data: BatchFeatureTableCreate,
     ) -> Task:
         """
-        Create HistoricalFeatureTable by submitting an async historical feature request task
+        Create BatchFeatureTable by submitting an async prediction request task
 
         Parameters
         ----------
-        data: HistoricalFeatureTableCreate
-            HistoricalFeatureTable creation payload
-        observation_set: Optional[UploadFile]
-            Observation set file
-        temp_storage: Storage
-            Storage instance
+        data: BatchFeatureTableCreate
+            BatchFeatureTable creation request parameters
 
         Returns
         -------
         Task
-
-        Raises
-        ------
-        HTTPException
-            If both observation_set and observation_table_id are set
         """
-        if data.observation_table_id is not None and observation_set is not None:
-            raise HTTPException(
-                status_code=HTTPStatus.UNPROCESSABLE_ENTITY,
-                detail="Only one of observation_set file and observation_table_id can be set",
-            )
-
-        # Validate the observation_table_id
-        if data.observation_table_id is not None:
-            observation_table = await self.observation_table_service.get_document(
-                document_id=data.observation_table_id
-            )
-            observation_set_dataframe = None
-            request_column_names = {col.name for col in observation_table.columns_info}
-        else:
-            assert observation_set is not None
-            observation_set_dataframe = dataframe_from_arrow_stream(observation_set.file)
-            request_column_names = set(observation_set_dataframe.columns)
-
-        # feature cluster group feature graph by feature store ID, only single feature store is
-        # supported
-        feature_cluster = data.featurelist_get_historical_features.feature_clusters[0]
+        # Validate the batch_request_table_id & deployment_id
+        batch_request_table = await self.batch_request_table_service.get_document(
+            document_id=data.batch_request_table_id
+        )
+        deployment = await self.deployment_service.get_document(document_id=data.deployment_id)
+        feature_list = await self.feature_list_service.get_document(
+            document_id=deployment.feature_list_id
+        )
+
+        # feature cluster group feature graph by feature store ID, only single feature store is supported
+        assert feature_list.feature_clusters is not None
+        feature_cluster = feature_list.feature_clusters[0]
         feature_store = await self.feature_store_service.get_document(
             document_id=feature_cluster.feature_store_id
         )
         await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
             graph=feature_cluster.graph,
             nodes=feature_cluster.nodes,
-            request_column_names=request_column_names,
+            request_column_names={col.name for col in batch_request_table.columns_info},
             feature_store=feature_store,
-            serving_names_mapping=data.featurelist_get_historical_features.serving_names_mapping,
         )
 
         # prepare task payload and submit task
-        payload = await self.service.get_historical_feature_table_task_payload(
-            data=data, storage=temp_storage, observation_set_dataframe=observation_set_dataframe
-        )
+        payload = await self.service.get_batch_feature_table_task_payload(data=data)
         task_id = await self.task_controller.task_manager.submit(payload=payload)
         return await self.task_controller.get_task(task_id=str(task_id))
 
-    async def get_info(self, document_id: ObjectId, verbose: bool) -> HistoricalFeatureTableInfo:
+    async def get_info(self, document_id: ObjectId, verbose: bool) -> BatchFeatureTableInfo:
         """
-        Get HistoricalFeatureTable info
+        Get BatchFeatureTable info
 
         Parameters
         ----------
         document_id: ObjectId
-            HistoricalFeatureTable ID
+            BatchFeatureTable ID
         verbose: bool
             Whether to return verbose info
 
         Returns
         -------
-        HistoricalFeatureTableInfo
+        BatchFeatureTableInfo
         """
-        info_document = await self.info_service.get_historical_feature_table_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        batch_feature_table = await self.service.get_document(document_id=document_id)
+        batch_request_table = await self.batch_request_table_service.get_document(
+            document_id=batch_feature_table.batch_request_table_id
+        )
+        deployment = await self.deployment_service.get_document(
+            document_id=batch_feature_table.deployment_id
+        )
+        return BatchFeatureTableInfo(
+            name=batch_feature_table.name,
+            deployment_name=deployment.name,
+            batch_request_table_name=batch_request_table.name,
+            table_details=batch_feature_table.location.table_details,
+            created_at=batch_feature_table.created_at,
+            updated_at=batch_feature_table.updated_at,
+            description=batch_feature_table.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/item_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/item_table/api.py`

 * *Files 9% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import ItemTableInfo
 from featurebyte.schema.item_table import ItemTableCreate, ItemTableList, ItemTableUpdate
 
 router = APIRouter(prefix="/item_table")
 
 
 @router.post("", response_model=ItemTableModel, status_code=HTTPStatus.CREATED)
@@ -128,7 +129,24 @@
     """
     controller = request.state.app_container.item_table_controller
     info = await controller.get_info(
         document_id=item_table_id,
         verbose=verbose,
     )
     return cast(ItemTableInfo, info)
+
+
+@router.patch("/{item_table_id}/description", response_model=ItemTableModel)
+async def update_item_table_description(
+    request: Request,
+    item_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> ItemTableModel:
+    """
+    Update item_table description
+    """
+    controller = request.state.app_container.item_table_controller
+    item_table: ItemTableModel = await controller.update_description(
+        document_id=item_table_id,
+        description=data.description,
+    )
+    return item_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/item_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/item_table/controller.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,27 +8,47 @@
 from bson import ObjectId
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.item_table import ItemTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
 from featurebyte.schema.info import ItemTableInfo
 from featurebyte.schema.item_table import ItemTableList, ItemTableServiceUpdate
+from featurebyte.service.event_table import EventTableService
 from featurebyte.service.item_table import ItemTableService
+from featurebyte.service.semantic import SemanticService
+from featurebyte.service.table_columns_info import TableColumnsInfoService, TableDocumentService
+from featurebyte.service.table_info import TableInfoService
+from featurebyte.service.table_status import TableStatusService
 
 
 class ItemTableController(
     BaseTableDocumentController[ItemTableModel, ItemTableService, ItemTableList]
 ):
     """
     ItemTable controller
     """
 
     paginated_document_class = ItemTableList
     document_update_schema_class = ItemTableServiceUpdate
 
+    def __init__(
+        self,
+        item_table_service: TableDocumentService,
+        table_columns_info_service: TableColumnsInfoService,
+        table_status_service: TableStatusService,
+        semantic_service: SemanticService,
+        table_info_service: TableInfoService,
+        event_table_service: EventTableService,
+    ):
+        super().__init__(
+            item_table_service, table_columns_info_service, table_status_service, semantic_service
+        )
+        self.table_info_service = table_info_service
+        self.event_table_service = event_table_service
+
     async def _get_column_semantic_map(self, document: ItemTableModel) -> dict[str, Any]:
         item_id = await self.semantic_service.get_or_create_document(name=SemanticType.ITEM_ID)
         return {document.item_id_column: item_id}
 
     async def get_info(self, document_id: ObjectId, verbose: bool) -> ItemTableInfo:
         """
         Get document info given document ID
@@ -40,11 +60,20 @@
         verbose: bool
             Flag to control verbose level
 
         Returns
         -------
         ItemTableInfo
         """
-        info_document = await self.info_service.get_item_table_info(
-            document_id=document_id, verbose=verbose
+        item_table = await self.service.get_document(document_id=document_id)
+        table_dict = await self.table_info_service.get_table_info(
+            data_document=item_table, verbose=verbose
+        )
+        event_table = await self.event_table_service.get_document(
+            document_id=item_table.event_table_id
+        )
+        return ItemTableInfo(
+            **table_dict,
+            event_id_column=item_table.event_id_column,
+            item_id_column=item_table.item_id_column,
+            event_table_name=event_table.name,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/observation_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/observation_table/api.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,14 +19,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import ObservationTableInfo
 from featurebyte.schema.observation_table import ObservationTableCreate, ObservationTableList
 from featurebyte.schema.task import Task
 
 router = APIRouter(prefix="/observation_table")
 
 
@@ -142,7 +143,24 @@
     """
     controller = request.state.app_container.observation_table_controller
     result: StreamingResponse = await controller.download_materialized_table(
         document_id=observation_table_id,
         get_credential=request.state.get_credential,
     )
     return result
+
+
+@router.patch("/{observation_table_id}/description", response_model=ObservationTableModel)
+async def update_observation_table_description(
+    request: Request,
+    observation_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> ObservationTableModel:
+    """
+    Update observation_table description
+    """
+    controller = request.state.app_container.observation_table_controller
+    observation_table: ObservationTableModel = await controller.update_description(
+        document_id=observation_table_id,
+        description=data.description,
+    )
+    return observation_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/observation_table/controller.py` & `featurebyte-0.4.0/featurebyte/routes/observation_table/controller.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,19 +7,18 @@
 
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.routes.common.base_materialized_table import BaseMaterializedTableController
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.info import ObservationTableInfo
 from featurebyte.schema.observation_table import ObservationTableCreate, ObservationTableList
 from featurebyte.schema.task import Task
-from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
-from featurebyte.service.info import InfoService
+from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.preview import PreviewService
-from featurebyte.service.validator.materialized_table_delete import check_delete_observation_table
+from featurebyte.service.validator.materialized_table_delete import ObservationTableDeleteValidator
 
 
 class ObservationTableController(
     BaseMaterializedTableController[
         ObservationTableModel, ObservationTableService, ObservationTableList
     ],
 ):
@@ -27,24 +26,24 @@
     ObservationTable Controller
     """
 
     paginated_document_class = ObservationTableList
 
     def __init__(
         self,
-        service: ObservationTableService,
+        observation_table_service: ObservationTableService,
         preview_service: PreviewService,
-        historical_feature_table_service: HistoricalFeatureTableService,
-        info_service: InfoService,
         task_controller: TaskController,
+        feature_store_service: FeatureStoreService,
+        observation_table_delete_validator: ObservationTableDeleteValidator,
     ):
-        super().__init__(service=service, preview_service=preview_service)
-        self.historical_feature_table_service = historical_feature_table_service
-        self.info_service = info_service
+        super().__init__(service=observation_table_service, preview_service=preview_service)
         self.task_controller = task_controller
+        self.feature_store_service = feature_store_service
+        self.observation_table_delete_validator = observation_table_delete_validator
 
     async def create_observation_table(
         self,
         data: ObservationTableCreate,
     ) -> Task:
         """
         Create ObservationTable by submitting a materialization task
@@ -59,18 +58,16 @@
         Task
         """
         payload = await self.service.get_observation_table_task_payload(data=data)
         task_id = await self.task_controller.task_manager.submit(payload=payload)
         return await self.task_controller.get_task(task_id=str(task_id))
 
     async def _verify_delete_operation(self, document_id: ObjectId) -> None:
-        await check_delete_observation_table(
-            observation_table_service=self.service,
-            historical_feature_table_service=self.historical_feature_table_service,
-            document_id=document_id,
+        await self.observation_table_delete_validator.check_delete_observation_table(
+            observation_table_id=document_id,
         )
 
     async def get_info(self, document_id: ObjectId, verbose: bool) -> ObservationTableInfo:
         """
         Get ObservationTable info given document_id
 
         Parameters
@@ -80,11 +77,21 @@
         verbose: bool
             Whether to return verbose info
 
         Returns
         -------
         ObservationTableInfo
         """
-        info_document = await self.info_service.get_observation_table_info(
-            document_id=document_id, verbose=verbose
+        _ = verbose
+        observation_table = await self.service.get_document(document_id=document_id)
+        feature_store = await self.feature_store_service.get_document(
+            document_id=observation_table.location.feature_store_id
+        )
+        return ObservationTableInfo(
+            name=observation_table.name,
+            type=observation_table.request_input.type,
+            feature_store_name=feature_store.name,
+            table_details=observation_table.location.table_details,
+            created_at=observation_table.created_at,
+            updated_at=observation_table.updated_at,
+            description=observation_table.description,
         )
-        return info_document
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/periodic_tasks/api.py` & `featurebyte-0.4.0/featurebyte/routes/periodic_tasks/api.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,14 +13,15 @@
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.periodic_task import PeriodicTaskList
 
 router = APIRouter(prefix="/periodic_task")
 
 
 @router.get("/{periodic_task_id}", response_model=PeriodicTask)
 async def get_periodic_task(request: Request, periodic_task_id: PydanticObjectId) -> PeriodicTask:
@@ -51,7 +52,24 @@
         page_size=page_size,
         sort_by=sort_by,
         sort_dir=sort_dir,
         search=search,
         name=name,
     )
     return periodic_task_list
+
+
+@router.patch("/{periodic_task_id}/description", response_model=PeriodicTask)
+async def update_periodic_task_description(
+    request: Request,
+    periodic_task_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> PeriodicTask:
+    """
+    Update periodic_task description
+    """
+    controller = request.state.app_container.periodic_task_controller
+    periodic_task: PeriodicTask = await controller.update_description(
+        document_id=periodic_task_id,
+        description=data.description,
+    )
+    return periodic_task
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/periodic_tasks/controller.py` & `featurebyte-0.4.0/featurebyte/routes/periodic_tasks/controller.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/relationship_info/api.py` & `featurebyte-0.4.0/featurebyte/routes/relationship_info/api.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,52 +1,38 @@
 """
 RelationshipInfo API routes
 """
 from __future__ import annotations
 
 from typing import Optional, cast
 
-from http import HTTPStatus
-
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.persistent import AuditDocumentList
 from featurebyte.models.relationship import RelationshipInfoModel
 from featurebyte.routes.common.schema import (
     AuditLogSortByQuery,
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.relationship_info import (
-    RelationshipInfoCreate,
     RelationshipInfoInfo,
     RelationshipInfoList,
     RelationshipInfoUpdate,
 )
 
 router = APIRouter(prefix="/relationship_info")
 
 
-@router.post("", response_model=RelationshipInfoModel, status_code=HTTPStatus.CREATED)
-async def create_relationship_info(
-    request: Request, data: RelationshipInfoCreate
-) -> RelationshipInfoModel:
-    """
-    Create relationship info
-    """
-    controller = request.state.app_container.relationship_info_controller
-    relationship_info: RelationshipInfoModel = await controller.create_relationship_info(data=data)
-    return relationship_info
-
-
 @router.get("", response_model=RelationshipInfoList)
 async def list_relationship_info(
     request: Request,
     page: int = PageQuery,
     page_size: int = PageSizeQuery,
     sort_by: Optional[str] = SortByQuery,
     sort_dir: Optional[str] = SortDirQuery,
@@ -132,7 +118,24 @@
         page=page,
         page_size=page_size,
         sort_by=sort_by,
         sort_dir=sort_dir,
         search=search,
     )
     return audit_doc_list
+
+
+@router.patch("/{relationship_info_id}/description", response_model=RelationshipInfoModel)
+async def update_relationship_info_description(
+    request: Request,
+    relationship_info_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> RelationshipInfoModel:
+    """
+    Update relationship_info description
+    """
+    controller = request.state.app_container.relationship_info_controller
+    relationship_info: RelationshipInfoModel = await controller.update_description(
+        document_id=relationship_info_id,
+        description=data.description,
+    )
+    return relationship_info
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/scd_table/api.py` & `featurebyte-0.4.0/featurebyte/routes/scd_table/api.py`

 * *Files 22% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
     VerboseQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.info import SCDTableInfo
 from featurebyte.schema.scd_table import SCDTableCreate, SCDTableList, SCDTableUpdate
 
 router = APIRouter(prefix="/scd_table")
 
 
 @router.post("", response_model=SCDTableModel, status_code=HTTPStatus.CREATED)
@@ -128,7 +129,24 @@
     """
     controller = request.state.app_container.scd_table_controller
     info = await controller.get_info(
         document_id=scd_table_id,
         verbose=verbose,
     )
     return cast(SCDTableInfo, info)
+
+
+@router.patch("/{scd_table_id}/description", response_model=SCDTableModel)
+async def update_scd_table_description(
+    request: Request,
+    scd_table_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> SCDTableModel:
+    """
+    Update scd_table description
+    """
+    controller = request.state.app_container.scd_table_controller
+    scd_table: SCDTableModel = await controller.update_description(
+        document_id=scd_table_id,
+        description=data.description,
+    )
+    return scd_table
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/semantic/api.py` & `featurebyte-0.4.0/featurebyte/routes/semantic/api.py`

 * *Files 19% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     NameQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortByQuery,
     SortDirQuery,
 )
+from featurebyte.schema.common.base import DescriptionUpdate
 from featurebyte.schema.semantic import SemanticCreate, SemanticList
 
 router = APIRouter(prefix="/semantic")
 
 
 @router.post("", response_model=SemanticModel, status_code=HTTPStatus.CREATED)
 async def create_semantic(request: Request, data: SemanticCreate) -> SemanticModel:
@@ -117,7 +118,24 @@
         page=page,
         page_size=page_size,
         sort_by=sort_by,
         sort_dir=sort_dir,
         search=search,
     )
     return audit_doc_list
+
+
+@router.patch("/{semantic_id}/description", response_model=SemanticModel)
+async def update_semantic_description(
+    request: Request,
+    semantic_id: PydanticObjectId,
+    data: DescriptionUpdate,
+) -> SemanticModel:
+    """
+    Update semantic description
+    """
+    controller = request.state.app_container.semantic_controller
+    semantic: SemanticModel = await controller.update_description(
+        document_id=semantic_id,
+        description=data.description,
+    )
+    return semantic
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/semantic/controller.py` & `featurebyte-0.4.0/featurebyte/routes/semantic/controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,18 +19,18 @@
     Semantic Controller
     """
 
     paginated_document_class = SemanticList
 
     def __init__(
         self,
-        service: SemanticService,
+        semantic_service: SemanticService,
         semantic_relationship_service: SemanticRelationshipService,
     ):
-        super().__init__(service)
+        super().__init__(semantic_service)
         self.relationship_service = semantic_relationship_service
 
     async def create_semantic(
         self,
         data: SemanticCreate,
     ) -> SemanticModel:
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/table/api.py` & `featurebyte-0.4.0/featurebyte/routes/table/api.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/task/api.py` & `featurebyte-0.4.0/featurebyte/routes/task/api.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/routes/task/controller.py` & `featurebyte-0.4.0/featurebyte/routes/task/controller.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,23 +6,24 @@
 from typing import Literal
 
 from http import HTTPStatus
 
 from fastapi import HTTPException
 
 from featurebyte.schema.task import Task, TaskList
-from featurebyte.service.task_manager import AbstractTaskManager
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
+from featurebyte.service.task_manager import TaskManager
 
 
 class TaskController:
     """
     TaskController
     """
 
-    def __init__(self, task_manager: AbstractTaskManager):
+    def __init__(self, task_manager: TaskManager):
         self.task_manager = task_manager
 
     async def get_task(self, task_id: str) -> Task:
         """
         Check task status
 
         Parameters
@@ -46,15 +47,15 @@
                 detail=f'Task (id: "{task_id}") not found.',
             )
         return task_status
 
     async def list_tasks(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_dir: Literal["asc", "desc"] = "desc",
     ) -> TaskList:
         """
         List task statuses of the given user
 
         Parameters
         ----------
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/temp_data/api.py` & `featurebyte-0.4.0/featurebyte/routes/temp_data/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,12 +13,12 @@
 
 
 @router.get("", status_code=HTTPStatus.OK)
 async def get_data(request: Request, path: Path = Query()) -> StreamingResponse:
     """
     Retrieve temp data
     """
-    controller = request.state.app_container.tempdata_controller
+    controller = request.state.app_container.temp_data_controller
     result: StreamingResponse = await controller.get_data(
         path=path,
     )
     return result
```

### Comparing `featurebyte-0.3.1/featurebyte/routes/temp_data/controller.py` & `featurebyte-0.4.0/featurebyte/routes/temp_data/controller.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/schema/batch_feature_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/schema/batch_request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/catalog.py` & `featurebyte-0.4.0/featurebyte/schema/catalog.py`

 * *Files 7% similar despite different names*

```diff
@@ -45,15 +45,15 @@
 
 
 class CatalogServiceUpdate(BaseDocumentServiceUpdateSchema):
     """
     Catalog service update schema
     """
 
-    name: Optional[str]
+    name: Optional[StrictStr]
 
     class Settings(BaseDocumentServiceUpdateSchema.Settings):
         """
         Unique contraints checking
         """
 
         unique_constraints: List[UniqueValuesConstraint] = [
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/common/base.py` & `featurebyte-0.4.0/featurebyte/schema/common/base.py`

 * *Files 11% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 class BaseInfo(BaseBriefInfo):
     """
     Base Info schema
     """
 
     created_at: datetime
     updated_at: Optional[datetime]
+    description: Optional[str]
 
 
 class BaseDocumentServiceUpdateSchema(FeatureByteBaseModel):
     """
     Base schema used for document service update
     """
 
@@ -53,7 +54,15 @@
     data: List[Any]
 
 
 class DeleteResponse(FeatureByteBaseModel):
     """
     Delete response
     """
+
+
+class DescriptionUpdate(FeatureByteBaseModel):
+    """
+    Description update schema
+    """
+
+    description: Optional[str]
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/common/operation.py` & `featurebyte-0.4.0/featurebyte/schema/common/operation.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/context.py` & `featurebyte-0.4.0/featurebyte/schema/context.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/credential.py` & `featurebyte-0.4.0/featurebyte/schema/credential.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/deployment.py` & `featurebyte-0.4.0/featurebyte/schema/deployment.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/dimension_table.py` & `featurebyte-0.4.0/featurebyte/schema/dimension_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/entity.py` & `featurebyte-0.4.0/featurebyte/schema/entity.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/event_table.py` & `featurebyte-0.4.0/featurebyte/schema/event_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature.py` & `featurebyte-0.4.0/featurebyte/schema/feature.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
 Feature API payload schema
 """
 from __future__ import annotations
 
-from typing import Any, Dict, Iterator, List, Optional
+from typing import Any, List, Optional
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
-from featurebyte.models.feature import FeatureModel, FeatureReadiness
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.feature_job_setting import TableFeatureJobSetting
+from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 from featurebyte.schema.common.operation import DictProject
 
 
 class FeatureCreate(FeatureByteBaseModel):
@@ -42,81 +44,40 @@
 
 
 class BatchFeatureItem(FeatureByteBaseModel):
     """
     Batch Feature Item schema
     """
 
-    id: Optional[PydanticObjectId]
+    id: PydanticObjectId
     name: StrictStr
     node_name: str
     tabular_source: TabularSource
 
 
-class BatchFeatureCreate(FeatureByteBaseModel):
+class BatchFeatureCreatePayload(FeatureByteBaseModel):
     """
-    Batch Feature Creation schema
+    Batch Feature Creation schema (used by the client to prepare the payload)
     """
 
-    graph: QueryGraph
+    # output of the quick-pruned operation is a QueryGraphModel type, not QueryGraph,
+    # since their serialization output is the same, QueryGraphModel is used here to avoid
+    # additional serialization/deserialization
+    graph: QueryGraphModel
     features: List[BatchFeatureItem]
 
-    @classmethod
-    def create(cls, features: List[FeatureCreate]) -> BatchFeatureCreate:
-        """
-        Create a batch feature create payload from a list of feature create payloads
-
-        Parameters
-        ----------
-        features: List[FeatureCreate]
-            List of feature create payloads
 
-        Returns
-        -------
-        BatchFeatureCreate
-        """
-        query_graph = QueryGraph()
-        feature_items = []
-        for feature in features:
-            query_graph, node_name_map = query_graph.load(feature.graph)
-            feature_items.append(
-                BatchFeatureItem(
-                    id=feature.id,
-                    name=feature.name,
-                    node_name=node_name_map[feature.node_name],
-                    tabular_source=feature.tabular_source,
-                )
-            )
-
-        return BatchFeatureCreate(
-            graph=query_graph,
-            features=feature_items,
-        )
-
-    def iterate_features(self) -> Iterator[FeatureCreate]:
-        """
-        Iterate over the batch feature create payload and yield feature create payloads
+class BatchFeatureCreate(BatchFeatureCreatePayload):
+    """
+    Batch Feature Creation schema (used by the featurebyte server side)
+    """
 
-        Yields
-        -------
-        Iterator[FeatureCreate]
-            List of feature create payloads
-        """
-        for feature in self.features:
-            target_node = self.graph.get_node_by_name(feature.node_name)
-            pruned_graph, node_name_map = self.graph.prune(
-                target_node=target_node, aggressive=False
-            )
-            yield FeatureCreate(
-                _id=feature.id,
-                name=feature.name,
-                node_name=node_name_map[feature.node_name],
-                graph=pruned_graph,
-                tabular_source=feature.tabular_source,
-            )
+    # while receiving the payload at the server side, the graph is converted to QueryGraph type
+    # so that it can be used for further processing without additional serialization/deserialization
+    graph: QueryGraph
 
 
 class FeatureNewVersionCreate(FeatureByteBaseModel):
     """
     New version creation schema based on existing feature
     """
 
@@ -134,14 +95,15 @@
 
 
 class FeatureModelResponse(FeatureModel):
     """
     Extended Feature model
     """
 
+    is_default: bool
     primary_entity_ids: List[PydanticObjectId]
 
 
 class FeaturePaginatedList(PaginationMixin):
     """
     Paginated list of features
     """
@@ -262,16 +224,7 @@
 class FeatureSQL(FeatureByteBaseModel):
     """
     Feature SQL schema
     """
 
     graph: QueryGraph
     node_name: str
-
-
-class FeaturePreview(FeatureSQL):
-    """
-    Feature Preview schema
-    """
-
-    feature_store_name: StrictStr
-    point_in_time_and_serving_name_list: List[Dict[str, Any]] = Field(min_items=1, max_items=50)
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/schema/feature_job_setting_analysis.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature_list.py` & `featurebyte-0.4.0/featurebyte/schema/feature_list.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,22 +6,25 @@
 from typing import Any, Dict, List, Optional
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import version_validator
+from featurebyte.enum import ConflictResolution
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
 from featurebyte.models.feature_list import (
     FeatureCluster,
     FeatureListModel,
     FeatureReadinessDistribution,
 )
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
+from featurebyte.schema.common.feature_or_target import ComputeRequest
+from featurebyte.schema.feature import BatchFeatureCreate, BatchFeatureCreatePayload
 
 
 class FeatureListCreate(FeatureByteBaseModel):
     """
     Feature List Creation schema
     """
 
@@ -34,14 +37,38 @@
     """
     Feature List Service Creation schema
     """
 
     feature_list_namespace_id: Optional[PydanticObjectId] = Field(default_factory=ObjectId)
 
 
+class FeatureListCreateWithBatchFeatureCreationMixin(FeatureByteBaseModel):
+    """Feature List Creation with Batch Feature Creation mixin"""
+
+    id: PydanticObjectId = Field(default_factory=ObjectId, alias="_id")
+    name: StrictStr
+    conflict_resolution: ConflictResolution
+
+
+class FeatureListCreateWithBatchFeatureCreationPayload(
+    BatchFeatureCreatePayload, FeatureListCreateWithBatchFeatureCreationMixin
+):
+    """
+    Feature List Creation with Batch Feature Creation schema (used by the client to prepare the payload)
+    """
+
+
+class FeatureListCreateWithBatchFeatureCreation(
+    BatchFeatureCreate, FeatureListCreateWithBatchFeatureCreationMixin
+):
+    """
+    Feature List Creation with Batch Feature Creation schema (used by the featurebyte server side)
+    """
+
+
 class FeatureVersionInfo(FeatureByteBaseModel):
     """
     Feature version info.
 
     Examples
     --------
     >>> new_feature_list = feature_list.create_new_version(  # doctest: +SKIP
@@ -68,20 +95,28 @@
 
     # pydantic validators
     _validate_unique_feat_name = validator("features", allow_reuse=True)(
         construct_unique_name_validator(field="name")
     )
 
 
+class FeatureListModelResponse(FeatureListModel):
+    """
+    Extended FeatureListModel with additional fields
+    """
+
+    is_default: bool
+
+
 class FeatureListPaginatedList(PaginationMixin):
     """
     Paginated list of Entity
     """
 
-    data: List[FeatureListModel]
+    data: List[FeatureListModelResponse]
 
 
 class FeatureListUpdate(FeatureByteBaseModel):
     """
     FeatureList update schema
     """
 
@@ -120,21 +155,20 @@
     """
     FeatureList preview schema
     """
 
     point_in_time_and_serving_name_list: List[Dict[str, Any]] = Field(min_items=1, max_items=50)
 
 
-class FeatureListGetHistoricalFeatures(FeatureByteBaseModel):
+class FeatureListGetHistoricalFeatures(ComputeRequest):
     """
     FeatureList get historical features schema
     """
 
     feature_clusters: List[FeatureCluster]
-    serving_names_mapping: Optional[Dict[str, str]]
     feature_list_id: Optional[PydanticObjectId]
 
 
 class OnlineFeaturesRequestPayload(FeatureByteBaseModel):
     """
     FeatureList get online features schema
     """
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature_list_namespace.py` & `featurebyte-0.4.0/featurebyte/schema/feature_list_namespace.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """
 FeatureListNamespace API payload scheme
 """
 from typing import List, Optional
 
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
-from featurebyte.models.feature import DefaultVersionMode
 from featurebyte.models.feature_list import (
     FeatureListNamespaceModel,
     FeatureListStatus,
     FeatureReadinessDistribution,
 )
+from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class FeatureListNamespaceModelResponse(FeatureListNamespaceModel):
     """
     Extended FeatureListNamespace model
     """
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature_namespace.py` & `featurebyte-0.4.0/featurebyte/schema/feature_namespace.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,15 +6,19 @@
 from typing import List, Optional
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr
 
 from featurebyte.enum import DBVarType
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
-from featurebyte.models.feature import DefaultVersionMode, FeatureNamespaceModel, FeatureReadiness
+from featurebyte.models.feature_namespace import (
+    DefaultVersionMode,
+    FeatureNamespaceModel,
+    FeatureReadiness,
+)
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class FeatureNamespaceCreate(FeatureByteBaseModel):
     """
     Feature Namespace Creation Schema
     """
@@ -45,23 +49,23 @@
     """
 
     data: List[FeatureNamespaceModelResponse]
 
 
 class FeatureNamespaceUpdate(BaseDocumentServiceUpdateSchema, FeatureByteBaseModel):
     """
-    FeatureNamespace update schema
+    FeatureNamespace update schema - exposed to client
     """
 
     default_version_mode: Optional[DefaultVersionMode]
     default_feature_id: Optional[PydanticObjectId]
 
 
 class FeatureNamespaceServiceUpdate(FeatureNamespaceUpdate):
     """
-    FeatureNamespace service update schema
+    FeatureNamespace service update schema - used by server side only, not exposed to client
     """
 
     feature_ids: Optional[List[PydanticObjectId]]
     online_enabled_feature_ids: Optional[List[PydanticObjectId]]
     readiness: Optional[FeatureReadiness]
     default_feature_id: Optional[PydanticObjectId]
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/feature_store.py` & `featurebyte-0.4.0/featurebyte/schema/feature_store.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/historical_feature_table.py` & `featurebyte-0.4.0/featurebyte/schema/target_table.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,47 +1,58 @@
 """
-HistoricalFeatureTable API payload schema
+TargetTable API payload schema
 """
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
-from bson import ObjectId
-from pydantic import Field, StrictStr, root_validator
+from pydantic import StrictStr, root_validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
-from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
+from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.target_table import TargetTableModel
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.node import Node
 from featurebyte.schema.common.base import PaginationMixin
-from featurebyte.schema.feature_list import FeatureListGetHistoricalFeatures
+from featurebyte.schema.common.feature_or_target import FeatureOrTargetTableCreate
 from featurebyte.schema.materialized_table import BaseMaterializedTableListRecord
 
 
-class HistoricalFeatureTableCreate(FeatureByteBaseModel):
+class TargetTableCreate(FeatureOrTargetTableCreate):
     """
-    HistoricalFeatureTable creation payload
+    TargetTable creation payload
     """
 
-    id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
-    feature_store_id: PydanticObjectId
-    observation_table_id: Optional[PydanticObjectId]
-    featurelist_get_historical_features: FeatureListGetHistoricalFeatures
+    serving_names_mapping: Optional[Dict[str, str]]
+    target_id: Optional[PydanticObjectId]
+    graph: QueryGraph
+    node_names: List[StrictStr]
+
+    @property
+    def nodes(self) -> List[Node]:
+        """
+        Get feature nodes
+
+        Returns
+        -------
+        List[Node]
+        """
+        return [self.graph.get_node_by_name(name) for name in self.node_names]
 
 
-class HistoricalFeatureTableList(PaginationMixin):
+class TargetTableList(PaginationMixin):
     """
-    Schema for listing historical feature tables
+    Schema for listing targe tables
     """
 
-    data: List[HistoricalFeatureTableModel]
+    data: List[TargetTableModel]
 
 
-class HistoricalFeatureTableListRecord(BaseMaterializedTableListRecord):
+class TargetTableListRecord(BaseMaterializedTableListRecord):
     """
-    Schema for listing historical feature tables as a DataFrame
+    Schema for listing target tables as a DataFrame
     """
 
     feature_store_id: PydanticObjectId
     observation_table_id: PydanticObjectId
 
     @root_validator(pre=True)
     @classmethod
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/info.py` & `featurebyte-0.4.0/featurebyte/schema/info.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,22 +8,23 @@
 from datetime import datetime
 
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.enum import DBVarType, SourceType
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
 from featurebyte.models.credential import DatabaseCredentialType, StorageCredentialType
-from featurebyte.models.feature import DefaultVersionMode
 from featurebyte.models.feature_list import (
     FeatureListStatus,
     FeatureReadinessDistribution,
     FeatureTypeFeatureCount,
 )
+from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.models.feature_store import TableStatus
 from featurebyte.models.request_input import RequestInputType
+from featurebyte.models.user_defined_function import FunctionParameter
 from featurebyte.query_graph.model.critical_data_info import CriticalDataInfo
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.node.schema import DatabaseDetails, TableDetails
 from featurebyte.schema.common.base import BaseBriefInfo, BaseInfo
 from featurebyte.schema.common.operation import DictProject
 from featurebyte.schema.feature import (
     FeatureBriefInfoList,
@@ -249,14 +250,15 @@
     dtype: DBVarType
     version: VersionComparison
     readiness: ReadinessComparison
     table_feature_job_setting: TableFeatureJobSettingComparison
     table_cleaning_operation: TableCleaningOperationComparison
     versions_info: Optional[FeatureBriefInfoList]
     metadata: Any
+    namespace_description: Optional[str]
 
 
 class FeatureListBriefInfo(FeatureByteBaseModel):
     """
     FeatureList brief info schema
     """
 
@@ -337,14 +339,15 @@
     """
 
     version: VersionComparison
     production_ready_fraction: ProductionReadyFractionComparison
     default_feature_fraction: DefaultFeatureFractionComparison
     versions_info: Optional[FeatureListBriefInfoList]
     deployed: bool
+    namespace_description: Optional[str]
 
 
 class FeatureJobSettingAnalysisInfo(FeatureByteBaseModel):
     """
     FeatureJobSettingAnalysis info schema
     """
 
@@ -390,23 +393,38 @@
     """
 
     type: RequestInputType
     feature_store_name: str
     table_details: TableDetails
 
 
-class HistoricalFeatureTableInfo(BaseInfo):
+class BaseFeatureOrTargetTableInfo(BaseInfo):
     """
-    Schema for historical feature table info
+    BaseFeatureOrTargetTable info schema
     """
 
     observation_table_name: Optional[str]
+    table_details: TableDetails
+
+
+class HistoricalFeatureTableInfo(BaseFeatureOrTargetTableInfo):
+    """
+    Schema for historical feature table info
+    """
+
     feature_list_name: str
     feature_list_version: str
-    table_details: TableDetails
+
+
+class TargetTableInfo(BaseFeatureOrTargetTableInfo):
+    """
+    Schema for target table info
+    """
+
+    target_name: str
 
 
 class DeploymentInfo(BaseInfo):
     """
     Schema for deployment info
     """
 
@@ -431,7 +449,39 @@
     """
     Schema for batch feature table info
     """
 
     batch_request_table_name: str
     deployment_name: str
     table_details: TableDetails
+
+
+class StaticSourceTableInfo(BaseInfo):
+    """
+    StaticSourceTable info schema
+    """
+
+    type: RequestInputType
+    feature_store_name: str
+    table_details: TableDetails
+
+
+class UserDefinedFunctionFeatureInfo(FeatureByteBaseModel):
+    """
+    UserDefinedFunction's feature info schema
+    """
+
+    id: PydanticObjectId
+    name: str
+
+
+class UserDefinedFunctionInfo(BaseInfo):
+    """
+    UserDefinedFunction info schema
+    """
+
+    sql_function_name: str
+    function_parameters: List[FunctionParameter]
+    signature: str
+    output_dtype: DBVarType
+    feature_store_name: str
+    used_by_features: List[UserDefinedFunctionFeatureInfo]
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/item_table.py` & `featurebyte-0.4.0/featurebyte/schema/item_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/materialized_table.py` & `featurebyte-0.4.0/featurebyte/schema/materialized_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/observation_table.py` & `featurebyte-0.4.0/featurebyte/schema/observation_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/relationship_info.py` & `featurebyte-0.4.0/featurebyte/schema/relationship_info.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/request_table.py` & `featurebyte-0.4.0/featurebyte/schema/request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/scd_table.py` & `featurebyte-0.4.0/featurebyte/schema/scd_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/semantic.py` & `featurebyte-0.4.0/featurebyte/schema/semantic.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/table.py` & `featurebyte-0.4.0/featurebyte/schema/table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/task.py` & `featurebyte-0.4.0/featurebyte/schema/task.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/base.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/base.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/batch_feature_create.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/batch_feature_table.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 """
-Batch feature create task payload
+Online prediction table task payload schema
 """
 from __future__ import annotations
 
-from pydantic import Field
-
 from featurebyte.enum import WorkerCommand
-from featurebyte.schema.feature import BatchFeatureCreate
-from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
+from featurebyte.models.batch_feature_table import BatchFeatureTableModel
+from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
+from featurebyte.schema.worker.task.base import BaseTaskPayload
 
 
-class BatchFeatureCreateTaskPayload(BaseTaskPayload, BatchFeatureCreate):
+class BatchFeatureTableTaskPayload(BaseTaskPayload, BatchFeatureTableCreate):
     """
-    Batch Feature create task payload
+    BatchFeatureTable creation task payload
     """
 
-    command = WorkerCommand.BATCH_FEATURE_CREATE
-    task_type: TaskType = Field(default=TaskType.CPU_TASK)
+    output_collection_name = BatchFeatureTableModel.collection_name()
+    command = WorkerCommand.BATCH_FEATURE_TABLE_CREATE
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/historical_feature_table.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 """
-Online prediction table task payload schema
+HistoricalFeaturesTaskPayload schema
 """
 from __future__ import annotations
 
+from typing import Optional
+
 from featurebyte.enum import WorkerCommand
-from featurebyte.models.batch_feature_table import BatchFeatureTableModel
-from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
+from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
+from featurebyte.schema.historical_feature_table import HistoricalFeatureTableCreate
 from featurebyte.schema.worker.task.base import BaseTaskPayload
 
 
-class BatchFeatureTableTaskPayload(BaseTaskPayload, BatchFeatureTableCreate):
+class HistoricalFeatureTableTaskPayload(BaseTaskPayload, HistoricalFeatureTableCreate):
     """
-    BatchFeatureTable creation task payload
+    HistoricalFeatureTable creation task payload
     """
 
-    output_collection_name = BatchFeatureTableModel.collection_name()
-    command = WorkerCommand.BATCH_FEATURE_TABLE_CREATE
+    output_collection_name = HistoricalFeatureTableModel.collection_name()
+    command = WorkerCommand.HISTORICAL_FEATURE_TABLE_CREATE
+    observation_set_storage_path: Optional[str]
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/batch_request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/deployment_create_update.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/deployment_create_update.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/feature_job_setting_analysis.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/materialized_table_delete.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/materialized_table_delete.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,26 +5,30 @@
 
 from featurebyte.enum import StrEnum, WorkerCommand
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.models.observation_table import ObservationTableModel
+from featurebyte.models.static_source_table import StaticSourceTableModel
+from featurebyte.models.target_table import TargetTableModel
 from featurebyte.schema.worker.task.base import BaseTaskPayload
 
 
 class MaterializedTableCollectionName(StrEnum):
     """
     Materialized table collection name
     """
 
     OBSERVATION = ObservationTableModel.collection_name()
     HISTORICAL_FEATURE = HistoricalFeatureTableModel.collection_name()
     BATCH_REQUEST = BatchRequestTableModel.collection_name()
     BATCH_FEATURE = BatchFeatureTableModel.collection_name()
+    STATIC_SOURCE = StaticSourceTableModel.collection_name()
+    TARGET = TargetTableModel.collection_name()
 
 
 class MaterializedTableDeleteTaskPayload(BaseTaskPayload):
     """
     Materialized Table Delete Task Payload
     """
```

### Comparing `featurebyte-0.3.1/featurebyte/schema/worker/task/observation_table.py` & `featurebyte-0.4.0/featurebyte/schema/worker/task/observation_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/base_document.py` & `featurebyte-0.4.0/featurebyte/service/base_document.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,66 +1,104 @@
 """
 BaseService class
 """
+# pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import Any, AsyncIterator, Dict, Generic, Iterator, List, Optional, Type, TypeVar, Union
 
 import copy
 from contextlib import contextmanager
 
 import numpy as np
 import pandas as pd
 from bson.objectid import ObjectId
 
 from featurebyte.common.dict_util import get_field_path_value
 from featurebyte.exception import (
+    CatalogNotSpecifiedError,
     DocumentConflictError,
+    DocumentModificationBlockedError,
     DocumentNotFoundError,
     QueryNotSupportedError,
 )
 from featurebyte.logging import get_logger
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
     FeatureByteCatalogBaseDocumentModel,
+    ReferenceInfo,
     UniqueConstraintResolutionSignature,
     VersionIdentifier,
 )
-from featurebyte.models.persistent import AuditActionType, FieldValueHistory, QueryFilter
+from featurebyte.models.persistent import (
+    AuditActionType,
+    DocumentUpdate,
+    FieldValueHistory,
+    QueryFilter,
+)
 from featurebyte.persistent.base import Persistent
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, BaseInfo
-from featurebyte.service.mixin import Document, DocumentCreateSchema, OpsServiceMixin, SortDir
+from featurebyte.service.mixin import (
+    DEFAULT_PAGE_SIZE,
+    Document,
+    DocumentCreateSchema,
+    OpsServiceMixin,
+    SortDir,
+)
 
 DocumentUpdateSchema = TypeVar("DocumentUpdateSchema", bound=BaseDocumentServiceUpdateSchema)
 InfoDocument = TypeVar("InfoDocument", bound=BaseInfo)
 RAW_QUERY_FILTER_WARNING = (
     "Using raw query filter breaks application logic. "
     "It should only be used when absolutely necessary."
 )
 
 
 logger = get_logger(__name__)
 
 
+def as_object_id(object_id: Any) -> ObjectId:
+    """
+    Convert object_id to ObjectId type if necessary
+
+    Parameters
+    ----------
+    object_id: Any
+        The ObjectId like object to convert. Must not be None.
+
+    Returns
+    -------
+    ObjectId
+    """
+    if isinstance(object_id, ObjectId):
+        return object_id
+    assert object_id is not None
+    return ObjectId(object_id)
+
+
 class BaseDocumentService(
     Generic[Document, DocumentCreateSchema, DocumentUpdateSchema], OpsServiceMixin
 ):
     """
     BaseDocumentService class is responsible to perform CRUD of the underlying persistent
     collection. It will perform model level validation before writing to persistent and after
     reading from the persistent.
     """
 
     document_class: Type[Document]
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
+    def __init__(self, user: Any, persistent: Persistent, catalog_id: Optional[ObjectId]):
         self.user = user
         self.persistent = persistent
         self.catalog_id = catalog_id
         self._allow_to_use_raw_query_filter = False
+        if self.is_catalog_specific and not catalog_id:
+            raise CatalogNotSpecifiedError(
+                f"No active catalog specified for service: {self.__class__.__name__}"
+            )
 
     @contextmanager
     def allow_use_raw_query_filter(self) -> Iterator[None]:
         """
         Activate use of raw query filter.
         This should be used ONLY when there is need to access all documents regardless of catalog membership.
         Valid use cases are table migration or table restoration.
@@ -106,14 +144,27 @@
 
         Returns
         -------
         bool
         """
         return issubclass(self.document_class, FeatureByteCatalogBaseDocumentModel)
 
+    @property
+    def should_disable_audit(self) -> bool:
+        """
+        Whether service operates on a collection that should not be audited on write operations.
+        This is typically the case for internally used collections that are not directly user
+        facing.
+
+        Returns
+        -------
+        bool
+        """
+        return not self.document_class.Settings.auditable
+
     @staticmethod
     def _extract_additional_creation_kwargs(data: DocumentCreateSchema) -> dict[str, Any]:
         """
         Extract additional document creation from document creation schema
 
         Parameters
         ----------
@@ -142,29 +193,30 @@
         """
         kwargs = self._extract_additional_creation_kwargs(data)
         if self.is_catalog_specific:
             kwargs = {**kwargs, "catalog_id": self.catalog_id}
 
         document = self.document_class(
             **{
-                **data.json_dict(),
+                **data.dict(by_alias=True),
                 **kwargs,
                 "user_id": self.user.id,
             },
         )
 
         # check any conflict with existing documents
         await self._check_document_unique_constraints(
             document=document,
             document_class=self.document_class,
         )
         insert_id = await self.persistent.insert_one(
             collection_name=self.collection_name,
             document=document.dict(by_alias=True),
             user_id=self.user.id,
+            disable_audit=self.should_disable_audit,
         )
         assert insert_id == document.id
         return await self.get_document(document_id=insert_id)
 
     def _construct_get_query_filter(
         self, document_id: ObjectId, use_raw_query_filter: bool = False, **kwargs: Any
     ) -> QueryFilter:
@@ -186,15 +238,15 @@
 
         Raises
         ------
         NotImplementedError
             Using raw query filter without activating override
         """
         _ = self, kwargs
-        kwargs = {"_id": ObjectId(document_id)}
+        kwargs = {"_id": ObjectId(as_object_id(document_id))}
         if use_raw_query_filter:
             if not self._allow_to_use_raw_query_filter:
                 raise NotImplementedError(RAW_QUERY_FILTER_WARNING)
             return kwargs
         # inject catalog_id into filter if document is catalog specific
         if self.is_catalog_specific:
             kwargs = {**kwargs, "catalog_id": self.catalog_id}
@@ -266,47 +318,49 @@
         kwargs: Any
             Additional keyword arguments
 
         Returns
         -------
         int
             number of records deleted
-
-        Raises
-        ------
-        DocumentNotFoundError
-            If the requested document not found
         """
+        document = await self.get_document(
+            document_id=document_id,
+            exception_detail=exception_detail,
+            use_raw_query_filter=use_raw_query_filter,
+            disable_audit=self.should_disable_audit,
+            **kwargs,
+        )
+
+        # check if document is modifiable
+        self._check_document_modifiable(document=document)
+
         query_filter = self._construct_get_query_filter(
             document_id=document_id, use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         num_of_records_deleted = await self.persistent.delete_one(
             collection_name=self.collection_name,
             query_filter=query_filter,
             user_id=self.user.id,
+            disable_audit=self.should_disable_audit,
         )
-        if not num_of_records_deleted:
-            exception_detail = exception_detail or (
-                f'{self.class_name} (id: "{document_id}") not found. Please save the {self.class_name} object first.'
-            )
-            raise DocumentNotFoundError(exception_detail)
         return int(num_of_records_deleted)
 
-    def _construct_list_query_filter(
+    def construct_list_query_filter(
         self,
-        query_filter: Optional[Dict[str, Any]] = None,
+        query_filter: Optional[QueryFilter] = None,
         use_raw_query_filter: bool = False,
         **kwargs: Any,
     ) -> QueryFilter:
         """
         Construct query filter used in list route
 
         Parameters
         ----------
-        query_filter: Optional[Dict[str, Any]]
+        query_filter: Optional[QueryFilter]
             Query filter to use as starting point
         use_raw_query_filter: bool
             Use only provided query filter
         kwargs: Any
             Keyword arguments passed to the list controller
 
         Returns
@@ -315,14 +369,15 @@
 
         Raises
         ------
         NotImplementedError
             Using raw query filter without activating override
         """
         _ = self
+        output: QueryFilter
         if not query_filter:
             output = {}
         else:
             output = copy.deepcopy(query_filter)
 
         if use_raw_query_filter:
             if not self._allow_to_use_raw_query_filter:
@@ -336,18 +391,18 @@
             output["$text"] = {"$search": kwargs["search"]}
         # inject catalog_id into filter if document is catalog specific
         if self.is_catalog_specific:
             output["catalog_id"] = self.catalog_id
 
         return output
 
-    async def list_documents(
+    async def list_documents_as_dict(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: SortDir = "desc",
         use_raw_query_filter: bool = False,
         **kwargs: Any,
     ) -> dict[str, Any]:
         """
         List documents stored at persistent (GitDB or MongoDB)
@@ -373,15 +428,15 @@
             List of documents fulfilled the filtering condition
 
         Raises
         ------
         QueryNotSupportedError
             If the persistent query is not supported
         """
-        query_filter = self._construct_list_query_filter(
+        query_filter = self.construct_list_query_filter(
             use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         try:
             docs, total = await self.persistent.find(
                 collection_name=self.collection_name,
                 query_filter=query_filter,
                 sort_by=sort_by,
@@ -390,52 +445,82 @@
                 page_size=page_size,
                 user_id=self.user.id,
             )
         except NotImplementedError as exc:
             raise QueryNotSupportedError from exc
         return {"page": page, "page_size": page_size, "total": total, "data": list(docs)}
 
-    async def list_documents_iterator(
+    async def list_documents_as_dict_iterator(
         self,
-        query_filter: Dict[str, Any],
-        page_size: int = 1000,
+        query_filter: QueryFilter,
+        page_size: int = DEFAULT_PAGE_SIZE,
         use_raw_query_filter: bool = False,
     ) -> AsyncIterator[Dict[str, Any]]:
         """
         List documents iterator to retrieve all the results based on given document service & query filter
 
         Parameters
         ----------
-        query_filter: Dict[str, Any]
+        query_filter: QueryFilter
             Query filter
         page_size: int
             Page size
         use_raw_query_filter: bool
             Use only provided query filter
 
         Yields
         ------
         AsyncIterator[Dict[str, Any]]
             List query output
         """
         to_iterate, page = True, 1
 
         while to_iterate:
-            list_results = await self.list_documents(
+            list_results = await self.list_documents_as_dict(
                 page=page,
                 page_size=page_size,
                 query_filter=query_filter,
                 use_raw_query_filter=use_raw_query_filter,
             )
             for doc in list_results["data"]:
                 yield doc
 
             to_iterate = bool(list_results["total"] > (page * page_size))
             page += 1
 
+    async def list_documents_iterator(
+        self,
+        query_filter: QueryFilter,
+        page_size: int = DEFAULT_PAGE_SIZE,
+        use_raw_query_filter: bool = False,
+    ) -> AsyncIterator[Document]:
+        """
+        List documents iterator to retrieve all the results based on given document service & query filter
+
+        Parameters
+        ----------
+        query_filter: QueryFilter
+            Query filter
+        page_size: int
+            Page size per query
+        use_raw_query_filter: bool
+            Use only provided query filter (without any further processing)
+
+        Yields
+        -------
+        AsyncIterator[Document]
+            List query output
+        """
+        async for doc in self.list_documents_as_dict_iterator(
+            query_filter=query_filter,
+            page_size=page_size,
+            use_raw_query_filter=use_raw_query_filter,
+        ):
+            yield self.document_class(**doc)
+
     def _construct_list_audit_query_filter(
         self, query_filter: Optional[QueryFilter], **kwargs: Any
     ) -> QueryFilter:
         """
         Construct query filter used in list audit route
 
         Parameters
@@ -456,15 +541,15 @@
         return query_filter
 
     async def list_document_audits(
         self,
         document_id: ObjectId,
         query_filter: Optional[QueryFilter] = None,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: SortDir = "desc",
         **kwargs: Any,
     ) -> dict[str, Any]:
         """
         List audit records stored at persistent (GitDB or MongoDB)
 
@@ -623,24 +708,24 @@
                 message += (
                     f' Please rename object (name: "{conflict_doc["name"]}") to something else.'
                 )
         return message
 
     async def _check_document_unique_constraint(
         self,
-        query_filter: dict[str, Any],
+        query_filter: QueryFilter,
         conflict_signature: dict[str, Any],
         resolution_signature: UniqueConstraintResolutionSignature | None,
     ) -> None:
         """
         Check document creation conflict
 
         Parameters
         ----------
-        query_filter: dict[str, Any]
+        query_filter: QueryFilter
             Query filter that will be passed to persistent
         conflict_signature: dict[str, Any]
             Document representation that shows user the conflict fields
         resolution_signature: UniqueConstraintResolutionSignature | None
             Object retrieval option shows in error message.
 
         Raises
@@ -738,14 +823,64 @@
         ):
             await self._check_document_unique_constraint(
                 query_filter=query_filter,
                 conflict_signature=conflict_signature,
                 resolution_signature=resolution_signature,
             )
 
+    @staticmethod
+    def _check_document_modifiable(document: Document) -> None:
+        if document.block_modification_by:
+            block_modification_by = [
+                f"{item.asset_name}(id: {item.document_id})"
+                for item in document.block_modification_by
+            ]
+            raise DocumentModificationBlockedError(
+                f"Document {document.id} is blocked from modification by {block_modification_by}"
+            )
+
+    async def _update_document(
+        self,
+        document: Document,
+        update_dict: Dict[str, Any],
+        update_document_class: Optional[Type[DocumentUpdateSchema]],
+    ) -> None:
+        """
+        Update document to persistent
+
+        Parameters
+        ----------
+        document: Document
+            Document
+        update_dict: Dict[str, Any]
+            Update dictionary
+        update_document_class: Optional[DocumentUpdateSchema]
+            Document update schema class
+        """
+        # check if document is modifiable
+        self._check_document_modifiable(document=document)
+
+        # check any conflict with existing documents
+        updated_document = self.document_class(**{**document.dict(by_alias=True), **update_dict})
+        await self._check_document_unique_constraints(
+            document=updated_document,
+            document_class=update_document_class,
+            original_document=document,
+        )
+
+        if document != updated_document:
+            # only update if change is detected
+            await self.persistent.update_one(
+                collection_name=self.collection_name,
+                query_filter=self._construct_get_query_filter(document_id=document.id),
+                update={"$set": update_dict},
+                user_id=self.user.id,
+                disable_audit=self.should_disable_audit,
+            )
+
     async def update_document(
         self,
         document_id: ObjectId,
         data: DocumentUpdateSchema,
         exclude_none: bool = True,
         document: Optional[Document] = None,
         return_document: bool = True,
@@ -771,34 +906,53 @@
         Optional[Document]
         """
         if document is None:
             document = await self.get_document(document_id=document_id)
 
         # perform validation first before actual update
         update_dict = data.dict(exclude_none=exclude_none)
-        updated_document = self.document_class(**{**document.json_dict(), **update_dict})
 
-        # check any conflict with existing documents
-        await self._check_document_unique_constraints(
-            document=updated_document, document_class=type(data), original_document=document
+        # update document to persistent
+        await self._update_document(
+            document=document, update_dict=update_dict, update_document_class=type(data)
         )
 
-        if document != updated_document:
-            # only update if change is detected
-            await self.persistent.update_one(
-                collection_name=self.collection_name,
-                query_filter=self._construct_get_query_filter(document_id=document_id),
-                update={"$set": update_dict},
-                user_id=self.user.id,
-            )
-
         if return_document:
             return await self.get_document(document_id=document_id)
         return None
 
+    async def update_documents(
+        self,
+        query_filter: QueryFilter,
+        update: DocumentUpdate,
+    ) -> int:
+        """
+        Update documents at persistent
+
+        Parameters
+        ----------
+        query_filter: QueryFilter
+            Query filter
+        update: DocumentUpdate
+            Document update payload object
+
+        Returns
+        -------
+        int
+            Number of documents updated
+        """
+        updated_count = await self.persistent.update_many(
+            collection_name=self.collection_name,
+            query_filter=query_filter,
+            update=update,
+            user_id=self.user.id,
+            disable_audit=self.should_disable_audit,
+        )
+        return int(updated_count)
+
     async def historical_document_generator(
         self, document_id: ObjectId
     ) -> AsyncIterator[Optional[Document]]:
         """
         Reconstruct documents of older history
 
         Parameters
@@ -814,7 +968,73 @@
         async for _, audit_doc in self.persistent.historical_document_generator(
             collection_name=self.collection_name, document_id=document_id
         ):
             if audit_doc:
                 yield self.document_class(**audit_doc)
             else:
                 yield None
+
+    async def add_block_modification_by(
+        self, query_filter: QueryFilter, reference_info: ReferenceInfo
+    ) -> None:
+        """
+        Add block modification by to records matching query filter
+
+        Parameters
+        ----------
+        query_filter: QueryFilter
+            Query filter to find records to be blocked
+        reference_info: ReferenceInfo
+            Reference info of the blocking document
+        """
+        await self.persistent.update_many(
+            collection_name=self.collection_name,
+            query_filter=query_filter,
+            update={
+                "$addToSet": {"block_modification_by": reference_info.dict(by_alias=True)},
+            },
+            user_id=self.user.id,
+            disable_audit=self.should_disable_audit,
+        )
+
+    async def remove_block_modification_by(
+        self, query_filter: QueryFilter, reference_info: ReferenceInfo
+    ) -> None:
+        """
+        Remove block modification by from records matching query filter
+
+        Parameters
+        ----------
+        query_filter: QueryFilter
+            Query filter to find records to be unblocked
+        reference_info: ReferenceInfo
+            Reference info of the blocking document
+        """
+        await self.persistent.update_many(
+            collection_name=self.collection_name,
+            query_filter=query_filter,
+            update={
+                "$pull": {"block_modification_by": reference_info.dict(by_alias=True)},
+            },
+            user_id=self.user.id,
+            disable_audit=self.should_disable_audit,
+        )
+
+    async def update_document_description(
+        self,
+        document_id: ObjectId,
+        description: Optional[str],
+    ) -> None:
+        """
+        Update document at persistent
+
+        Parameters
+        ----------
+        document_id: ObjectId
+            Document ID
+        description: Optional[str]
+            Document description
+        """
+        document = await self.get_document(document_id=document_id)
+        await self._update_document(
+            document=document, update_dict={"description": description}, update_document_class=None
+        )
```

### Comparing `featurebyte-0.3.1/featurebyte/service/base_table_document.py` & `featurebyte-0.4.0/featurebyte/service/base_table_document.py`

 * *Files 6% similar despite different names*

```diff
@@ -69,21 +69,21 @@
     ) -> QueryFilter:
         query_filter = super()._construct_get_query_filter(
             document_id=document_id, use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         query_filter["type"] = self.table_type
         return query_filter
 
-    def _construct_list_query_filter(
+    def construct_list_query_filter(
         self,
-        query_filter: Optional[dict[str, Any]] = None,
+        query_filter: Optional[QueryFilter] = None,
         use_raw_query_filter: bool = False,
         **kwargs: Any,
     ) -> QueryFilter:
-        output = super()._construct_list_query_filter(
+        output = super().construct_list_query_filter(
             query_filter=query_filter, use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         output["type"] = self.table_type
         return output
 
     def _get_conflict_message(
         self,
@@ -118,17 +118,18 @@
         # retrieve feature store to check the feature_store_id is valid
         _ = await FeatureStoreService(
             user=self.user, persistent=self.persistent, catalog_id=self.catalog_id
         ).get_document(document_id=data.tabular_source.feature_store_id)
 
         # create document ID if it is None
         data_doc_id = data.id or ObjectId()
-        payload_dict = {**data.json_dict(), "_id": data_doc_id}
+        payload_dict = {**data.dict(by_alias=True), "_id": data_doc_id}
         if self.is_catalog_specific:
-            payload_dict = {**payload_dict, "catalog_id": self.catalog_id}
+            assert self.catalog_id
+            payload_dict["catalog_id"] = self.catalog_id
 
         # create document for insertion
         document = self.document_class(
             user_id=self.user.id, status=TableStatus.PUBLIC_DRAFT, **payload_dict
         )
 
         # check any conflict with existing documents
```

### Comparing `featurebyte-0.3.1/featurebyte/service/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/service/batch_feature_table.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """
 BatchFeatureTableService class
 """
 from __future__ import annotations
 
 from bson import ObjectId
 
+from featurebyte.enum import MaterializedTableNamePrefix
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
 from featurebyte.schema.worker.task.batch_feature_table import BatchFeatureTableTaskPayload
 from featurebyte.service.materialized_table import BaseMaterializedTableService
 
 
@@ -16,15 +17,15 @@
     BaseMaterializedTableService[BatchFeatureTableModel, BatchFeatureTableModel]
 ):
     """
     BatchFeatureTableService class
     """
 
     document_class = BatchFeatureTableModel
-    materialized_table_name_prefix = "BATCH_FEATURE_TABLE"
+    materialized_table_name_prefix = MaterializedTableNamePrefix.BATCH_FEATURE_TABLE
 
     @property
     def class_name(self) -> str:
         return "BatchFeatureTable"
 
     async def get_batch_feature_table_task_payload(
         self, data: BatchFeatureTableCreate
```

### Comparing `featurebyte-0.3.1/featurebyte/service/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/service/batch_request_table.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 """
 BatchRequestTableService class
 """
 from __future__ import annotations
 
-from typing import Any
+from typing import Any, Optional
 
 from bson import ObjectId
 
+from featurebyte.enum import MaterializedTableNamePrefix
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.persistent import Persistent
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate
 from featurebyte.schema.worker.task.batch_request_table import BatchRequestTableTaskPayload
 from featurebyte.service.context import ContextService
 from featurebyte.service.feature_store import FeatureStoreService
@@ -21,21 +22,21 @@
     BaseMaterializedTableService[BatchRequestTableModel, BatchRequestTableModel]
 ):
     """
     BatchRequestTableService class
     """
 
     document_class = BatchRequestTableModel
-    materialized_table_name_prefix = "BATCH_REQUEST_TABLE"
+    materialized_table_name_prefix = MaterializedTableNamePrefix.BATCH_REQUEST_TABLE
 
     def __init__(
         self,
         user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
+        catalog_id: Optional[ObjectId],
         feature_store_service: FeatureStoreService,
         context_service: ContextService,
     ):
         super().__init__(user, persistent, catalog_id, feature_store_service)
         self.context_service = context_service
 
     @property
@@ -68,12 +69,12 @@
         if data.context_id is not None:
             # Check if the context document exists when provided. This should perform additional
             # validation once additional information such as request schema are available in the
             # context.
             await self.context_service.get_document(document_id=data.context_id)
 
         return BatchRequestTableTaskPayload(
-            **data.dict(),
+            **data.dict(by_alias=True),
             user_id=self.user.id,
             catalog_id=self.catalog_id,
             output_document_id=output_document_id,
         )
```

### Comparing `featurebyte-0.3.1/featurebyte/service/context.py` & `featurebyte-0.4.0/featurebyte/service/context.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 ContextService class
 """
 from __future__ import annotations
 
-from typing import Optional
+from typing import Any, Optional
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
 from featurebyte.models.context import ContextModel
+from featurebyte.persistent import Persistent
 from featurebyte.query_graph.enum import NodeOutputType
 from featurebyte.query_graph.node.metadata.operation import NodeOutputCategory, OperationStructure
 from featurebyte.schema.context import ContextCreate, ContextUpdate
 from featurebyte.service.base_document import BaseDocumentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.table import TableService
 
@@ -20,27 +21,26 @@
 class ContextService(BaseDocumentService[ContextModel, ContextCreate, ContextUpdate]):
     """
     ContextService class
     """
 
     document_class = ContextModel
 
-    @property
-    def entity_service(self) -> EntityService:
-        """
-        Entity service instance
-
-        Returns
-        -------
-        EntityService
-        """
-        return EntityService(user=self.user, persistent=self.persistent, catalog_id=self.catalog_id)
+    def __init__(
+        self,
+        user: Any,
+        persistent: Persistent,
+        catalog_id: Optional[ObjectId],
+        entity_service: EntityService,
+    ):
+        super().__init__(user, persistent, catalog_id)
+        self.entity_service = entity_service
 
     async def create_document(self, data: ContextCreate) -> ContextModel:
-        entities = await self.entity_service.list_documents(
+        entities = await self.entity_service.list_documents_as_dict(
             page=1, page_size=0, query_filter={"_id": {"$in": data.entity_ids}}
         )
         found_entity_ids = set(doc["_id"] for doc in entities["data"])
         not_found_entity_ids = set(data.entity_ids).difference(found_entity_ids)
         if not_found_entity_ids:
             # trigger entity not found error
             await self.entity_service.get_document(document_id=list(not_found_entity_ids)[0])
@@ -99,15 +99,15 @@
                     f'Column "{source_col.name}" not found in table "{table.name}".'
                 )
             if column_info.entity_id:
                 found_entity_ids.add(column_info.entity_id)
 
         missing_entity_ids = list(set(context.entity_ids).difference(found_entity_ids))
         if missing_entity_ids:
-            missing_entities = await self.entity_service.list_documents(
+            missing_entities = await self.entity_service.list_documents_as_dict(
                 query_filter={"_id": {"$in": missing_entity_ids}}
             )
             missing_entity_names = [entity["name"] for entity in missing_entities["data"]]
             raise DocumentUpdateError(
                 f"Entities {missing_entity_names} not found in the context view."
             )
 
@@ -118,15 +118,17 @@
         exclude_none: bool = True,
         document: Optional[ContextModel] = None,
         return_document: bool = True,
     ) -> Optional[ContextModel]:
         document = await self.get_document(document_id=document_id)
         if data.graph and data.node_name:
             node = data.graph.get_node_by_name(data.node_name)
-            operation_structure = data.graph.extract_operation_structure(node=node)
+            operation_structure = data.graph.extract_operation_structure(
+                node=node, keep_all_source_columns=True
+            )
             await self._validate_view(operation_structure=operation_structure, context=document)
 
         document = await super().update_document(
             document_id=document_id,
             document=document,
             data=data,
             exclude_none=exclude_none,
```

### Comparing `featurebyte-0.3.1/featurebyte/service/credential.py` & `featurebyte-0.4.0/featurebyte/service/credential.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 """
 CredentialService
 """
 from __future__ import annotations
 
-from typing import Any, Dict, Optional
+from typing import Any, Optional
 
 from bson.objectid import ObjectId
 from cryptography.fernet import InvalidToken
 
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import CredentialModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.persistent.base import Persistent
 from featurebyte.schema.credential import CredentialCreate, CredentialServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
+from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
 
 logger = get_logger(__name__)
 
 
 class CredentialService(
     BaseDocumentService[CredentialModel, CredentialCreate, CredentialServiceUpdate]
@@ -28,62 +29,61 @@
 
     document_class = CredentialModel
 
     def __init__(
         self,
         user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
-        feature_store_warehouse_service: Optional[FeatureStoreWarehouseService] = None,
+        catalog_id: Optional[ObjectId],
+        feature_store_warehouse_service: FeatureStoreWarehouseService,
+        feature_store_service: FeatureStoreService,
     ):
         super().__init__(user=user, persistent=persistent, catalog_id=catalog_id)
         self.feature_store_warehouse_service = feature_store_warehouse_service
+        self.feature_store_service = feature_store_service
 
     def _construct_get_query_filter(
         self, document_id: ObjectId, use_raw_query_filter: bool = False, **kwargs: Any
     ) -> QueryFilter:
         query_filter = super()._construct_get_query_filter(
             document_id=document_id, use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         # credentials are personal to the user
-        query_filter["user_id"] = self.user.id
+        if not use_raw_query_filter:
+            query_filter["user_id"] = self.user.id
         return query_filter
 
-    def _construct_list_query_filter(
+    def construct_list_query_filter(
         self,
-        query_filter: Optional[Dict[str, Any]] = None,
+        query_filter: Optional[QueryFilter] = None,
         use_raw_query_filter: bool = False,
         **kwargs: Any,
     ) -> QueryFilter:
-        output = super()._construct_list_query_filter(
+        output = super().construct_list_query_filter(
             query_filter=query_filter,
             use_raw_query_filter=use_raw_query_filter,
             **kwargs,
         )
         # credentials are personal to the user
-        output["user_id"] = self.user.id
+        if not use_raw_query_filter:
+            output["user_id"] = self.user.id
         return output
 
     async def _validate_credential(self, credential: CredentialModel) -> None:
         """
         Validate credential is valid
 
         Parameters
         ----------
         credential: CredentialModel
             CredentialModel to validate
         """
-        # feature_store_warehouse_service needs to be provided
-        assert self.feature_store_warehouse_service
-
         # test credential works
-        feature_store = (
-            await self.feature_store_warehouse_service.feature_store_service.get_document(
-                document_id=credential.feature_store_id
-            )
+        feature_store = await self.feature_store_service.get_document(
+            document_id=credential.feature_store_id
         )
 
         async def get_credential(**kwargs: Any) -> CredentialModel:
             """
             Get credential to test with feature store
 
             Parameters
@@ -112,18 +112,20 @@
         data: CredentialCreate
             Credential creation payload object
 
         Returns
         -------
         CredentialModel
         """
-        credential = self.document_class(**data.json_dict())
+        credential = self.document_class(**data.dict(by_alias=True))
         await self._validate_credential(credential=credential)
         credential.encrypt()
-        return await super().create_document(data=CredentialCreate(**credential.json_dict()))
+        return await super().create_document(
+            data=CredentialCreate(**credential.dict(by_alias=True))
+        )
 
     async def update_document(
         self,
         document_id: ObjectId,
         data: CredentialServiceUpdate,
         exclude_none: bool = True,
         document: Optional[CredentialModel] = None,
@@ -156,15 +158,15 @@
         try:
             document.decrypt()
         except InvalidToken:
             logger.warning("Credential is already decrypted")
 
         # verify credential is valid
         update_dict = data.dict(exclude_none=exclude_none)
-        updated_document = self.document_class(**{**document.json_dict(), **update_dict})
+        updated_document = self.document_class(**{**document.dict(by_alias=True), **update_dict})
         await self._validate_credential(credential=updated_document)
         data.encrypt()
 
         return await super().update_document(
             document_id=document_id,
             data=data,
             exclude_none=exclude_none,
```

### Comparing `featurebyte-0.3.1/featurebyte/service/default_version_mode.py` & `featurebyte-0.4.0/featurebyte/service/default_version_mode.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,44 +1,39 @@
 """
 DefaultVersionModeService class
 """
 from __future__ import annotations
 
-from typing import Any, Optional
+from typing import Optional
 
 from bson.objectid import ObjectId
 
-from featurebyte.models.feature import DefaultVersionMode, FeatureNamespaceModel
 from featurebyte.models.feature_list import FeatureListNamespaceModel
-from featurebyte.persistent import Persistent
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureNamespaceModel
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
 from featurebyte.schema.feature_namespace import FeatureNamespaceServiceUpdate
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
 from featurebyte.service.feature_readiness import FeatureReadinessService
+from featurebyte.service.mixin import OpsServiceMixin
 
 
-class DefaultVersionModeService(BaseService):
+class DefaultVersionModeService(OpsServiceMixin):
     """
     DefaultVersionModeService class is responsible for handling feature & feature list version mode.
     When there is a change in default version mode, this class will orchestrate feature readiness update
     through FeatureReadinessService.
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         feature_namespace_service: FeatureNamespaceService,
         feature_readiness_service: FeatureReadinessService,
         feature_list_namespace_service: FeatureListNamespaceService,
     ):
-        super().__init__(user, persistent, catalog_id)
         self.feature_namespace_service = feature_namespace_service
         self.feature_readiness_service = feature_readiness_service
         self.feature_list_namespace_service = feature_list_namespace_service
 
     async def update_feature_namespace(
         self,
         feature_namespace_id: ObjectId,
```

### Comparing `featurebyte-0.3.1/featurebyte/service/deploy.py` & `featurebyte-0.4.0/featurebyte/service/deploy.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,61 +6,57 @@
 from typing import Any, Callable, Optional
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentCreationError, DocumentError, DocumentUpdateError
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.deployment import DeploymentModel
-from featurebyte.models.feature import FeatureModel, FeatureReadiness
+from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import (
     FeatureListModel,
     FeatureListNamespaceModel,
     FeatureListStatus,
 )
+from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.persistent import Persistent
 from featurebyte.schema.deployment import DeploymentUpdate
 from featurebyte.schema.feature import FeatureServiceUpdate
 from featurebyte.schema.feature_list import FeatureListServiceUpdate
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_list_status import FeatureListStatusService
+from featurebyte.service.mixin import OpsServiceMixin
 from featurebyte.service.online_enable import OnlineEnableService
 
 
-class DeployService(BaseService):
+class DeployService(OpsServiceMixin):
     """
     DeployService class is responsible for maintaining the feature & feature list structure
     of feature list deployment.
     """
 
     def __init__(
         self,
-        user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
+        feature_service: FeatureService,
         online_enable_service: OnlineEnableService,
         feature_list_status_service: FeatureListStatusService,
         deployment_service: DeploymentService,
+        feature_list_namespace_service: FeatureListNamespaceService,
+        feature_list_service: FeatureListService,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_service = FeatureService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+        self.persistent = persistent
+        self.feature_service = feature_service
         self.online_enable_service = online_enable_service
-        self.feature_list_service = FeatureListService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+        self.feature_list_service = feature_list_service
         self.feature_list_status_service = feature_list_status_service
-        self.feature_list_namespace_service = FeatureListNamespaceService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+        self.feature_list_namespace_service = feature_list_namespace_service
         self.deployment_service = deployment_service
 
     @classmethod
     def _extract_deployed_feature_list_ids(
         cls, feature_list: FeatureListModel, document: FeatureListNamespaceModel | FeatureModel
     ) -> list[ObjectId]:
         if feature_list.deployed:
@@ -180,15 +176,15 @@
                 document_id=feature_list.feature_list_namespace_id
             )
             if feature_list_namespace.status == FeatureListStatus.DEPRECATED:
                 raise DocumentUpdateError("Deprecated feature list cannot be deployed.")
 
             # if enabling deployment, check is there any feature with readiness not equal to production ready
             query_filter = {"_id": {"$in": feature_list.feature_ids}}
-            async for feature in self.feature_service.list_documents_iterator(
+            async for feature in self.feature_service.list_documents_as_dict_iterator(
                 query_filter=query_filter
             ):
                 if FeatureReadiness(feature["readiness"]) != FeatureReadiness.PRODUCTION_READY:
                     raise DocumentUpdateError(
                         "Only FeatureList object of all production ready features can be deployed."
                     )
 
@@ -257,15 +253,15 @@
         ------
         Exception
             When there is an unexpected error during feature online_enabled status update
         """
         if update_progress:
             update_progress(0, "Start updating feature list")
 
-        list_deployment_results = await self.deployment_service.list_documents(
+        list_deployment_results = await self.deployment_service.list_documents_as_dict(
             query_filter={"feature_list_id": feature_list_id, "enabled": True}
         )
         target_deployed = list_deployment_results["total"] > 0
         document = await self.feature_list_service.get_document(document_id=feature_list_id)
 
         if document.deployed != target_deployed:
             await self._validate_deployed_operation(document, target_deployed)
```

### Comparing `featurebyte-0.3.1/featurebyte/service/dimension_table.py` & `featurebyte-0.4.0/featurebyte/service/dimension_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/entity.py` & `featurebyte-0.4.0/featurebyte/sql/base.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,77 +1,103 @@
 """
-EntityService class
+Base Class for SQL related operations
 """
-from __future__ import annotations
-
 from typing import Any
 
-from bson import ObjectId
+from pydantic.fields import PrivateAttr
+from pydantic.main import BaseModel
 
-from featurebyte.models.entity import EntityModel
-from featurebyte.schema.entity import EntityCreate, EntityServiceUpdate
-from featurebyte.service.base_document import BaseDocumentService
+from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
+from featurebyte.session.base import BaseSession
+from featurebyte.session.snowflake import SnowflakeSession
 
 
-class EntityService(BaseDocumentService[EntityModel, EntityCreate, EntityServiceUpdate]):
+class BaseSqlModel(BaseModel):
     """
-    EntityService class
+    Base class for Tile Operation Classes
     """
 
-    document_class = EntityModel
+    _session: BaseSession = PrivateAttr()
+
+    def __init__(self, session: BaseSession, **kwargs: Any):
+        """
+        Initialize Tile Operation Instance
 
-    @staticmethod
-    def _extract_additional_creation_kwargs(data: EntityCreate) -> dict[str, Any]:
-        return {"serving_names": [data.serving_name]}
+        Parameters
+        ----------
+        session: BaseSession
+            input SparkSession
+        kwargs: Any
+            constructor arguments
+        """
+        super().__init__(**kwargs)
+        self._session = session
 
-    async def get_entities_with_serving_names(
-        self, serving_names_set: set[str]
-    ) -> list[EntityModel]:
+    @property
+    def adapter(self) -> BaseAdapter:
         """
-        Retrieve all entities matching the set of provided serving names
+        Get SQL adapter based on session type
+
+        Returns
+        -------
+        BaseAdapter
+        """
+        return get_sql_adapter(self._session.source_type)
+
+    def quote_column(self, col_val: str) -> str:
+        """
+        Quote column name based on session type
 
         Parameters
         ----------
-        serving_names_set: set[str]
-            Set of serving names to match
+        col_val: str
+            input column name
 
         Returns
         -------
-        list[EntityModel]
+            quoted column name
         """
-        docs = self.list_documents_iterator(
-            query_filter={"serving_names": {"$in": list(serving_names_set)}}
-        )
-        return [EntityModel(**doc) async for doc in docs]
+        if isinstance(self._session, SnowflakeSession):
+            return f'"{col_val}"'
 
-    async def get_children_entities(self, entity_id: ObjectId) -> list[EntityModel]:
+        return f"`{col_val}`"
+
+    def quote_column_null_aware_equal(self, left_expr: str, right_expr: str) -> str:
         """
-        Retrieve the children of an entity
+        Compares whether two expressions are null-safe equal
 
         Parameters
         ----------
-        entity_id: ObjectId
-            Entity identifier
+        left_expr: str
+            left expression
+        right_expr: str
+            right expression
 
         Returns
         -------
-        list[EntityModel]
+            null aware equal expression
         """
-        query_filter = {"parents": {"$elemMatch": {"id": ObjectId(entity_id)}}}
-        docs = self.list_documents_iterator(query_filter=query_filter)
-        return [EntityModel(**doc) async for doc in docs]
+        if isinstance(self._session, SnowflakeSession):
+            return f"EQUAL_NULL({left_expr}, {right_expr})"
+
+        return f"{left_expr} <=> {right_expr}"
 
-    async def get_entities(self, entity_ids: set[ObjectId]) -> list[EntityModel]:
+    async def table_exists(self, table_name: str) -> bool:
         """
-        Retrieve entities given a list of entity ids
+        Check if table exists
 
         Parameters
         ----------
-        entity_ids: list[ObjectId]
-            Entity identifiers
+        table_name: str
+            input table name
 
         Returns
         -------
-        list[EntityModel]
+            True if table exists, False otherwise
         """
-        docs = self.list_documents_iterator(query_filter={"_id": {"$in": list(entity_ids)}})
-        return [EntityModel(**doc) async for doc in docs]
+        table_exist_flag = True
+        try:
+            await self._session.execute_query(f"select * from {table_name} limit 1")
+        except self._session._no_schema_error:  # pylint: disable=protected-access
+            table_exist_flag = False
+
+        return table_exist_flag
```

### Comparing `featurebyte-0.3.1/featurebyte/service/entity_validation.py` & `featurebyte-0.4.0/featurebyte/service/entity_validation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,51 +1,43 @@
 """
 Module to support serving using parent-child relationship
 """
 from __future__ import annotations
 
-from typing import Any, Optional
-
-from bson import ObjectId
+from typing import Optional
 
 from featurebyte.exception import (
     AmbiguousEntityRelationshipError,
     EntityJoinPathNotFoundError,
     RequiredEntityNotProvidedError,
     UnexpectedServingNamesMappingError,
 )
 from featurebyte.models.entity import EntityModel
 from featurebyte.models.entity_validation import EntityInfo
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.parent_serving import ParentServingPreparation
-from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.schema import FeatureStoreDetails
 from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.parent_serving import ParentEntityLookupService
 
 
-class EntityValidationService(BaseService):
+class EntityValidationService:
     """
     EntityValidationService class is responsible for validating that required entities are
     provided in feature requests during preview, historical features, and online serving.
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         entity_service: EntityService,
         parent_entity_lookup_service: ParentEntityLookupService,
     ):
-        super().__init__(user, persistent, catalog_id)
         self.entity_service = entity_service
         self.parent_entity_lookup_service = parent_entity_lookup_service
 
     async def get_entity_info_from_request(
         self,
         graph: QueryGraphModel,
         nodes: list[Node],
```

### Comparing `featurebyte-0.3.1/featurebyte/service/event_table.py` & `featurebyte-0.4.0/featurebyte/service/event_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/feature.py` & `featurebyte-0.4.0/featurebyte/service/online_enable.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,297 +1,244 @@
 """
-FeatureService class
+OnlineEnableService class
 """
 from __future__ import annotations
 
-from typing import Any, Dict
+from typing import Any, Optional
 
-from bson import ObjectId
+from bson.objectid import ObjectId
 
-from featurebyte.common.model_util import get_version
-from featurebyte.exception import DocumentInconsistencyError, DocumentNotFoundError
-from featurebyte.models.base import VersionIdentifier
-from featurebyte.models.feature import (
-    DefaultVersionMode,
-    FeatureModel,
-    FeatureNamespaceModel,
-    FeatureReadiness,
-)
+from featurebyte.feature_manager.model import ExtendedFeatureModel
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_list import FeatureListModel
+from featurebyte.models.feature_namespace import FeatureNamespaceModel
+from featurebyte.models.online_store import OnlineFeatureSpec
 from featurebyte.persistent import Persistent
-from featurebyte.query_graph.enum import NodeType
-from featurebyte.query_graph.graph import QueryGraph
-from featurebyte.query_graph.model.graph import QueryGraphModel
-from featurebyte.query_graph.transform.sdk_code import SDKCodeExtractor
-from featurebyte.schema.feature import FeatureServiceCreate, FeatureServiceUpdate
-from featurebyte.schema.feature_namespace import (
-    FeatureNamespaceCreate,
-    FeatureNamespaceServiceUpdate,
-)
-from featurebyte.service.base_document import BaseDocumentService
+from featurebyte.schema.feature import FeatureServiceUpdate
+from featurebyte.schema.feature_list import FeatureListServiceUpdate
+from featurebyte.schema.feature_namespace import FeatureNamespaceServiceUpdate
+from featurebyte.service.feature import FeatureService
+from featurebyte.service.feature_list import FeatureListService
+from featurebyte.service.feature_manager import FeatureManagerService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
-from featurebyte.service.table import TableService
-from featurebyte.service.view_construction import ViewConstructionService
-
-
-async def validate_feature_version_and_namespace_consistency(
-    feature: FeatureModel, feature_namespace: FeatureNamespaceModel
-) -> None:
-    """
-    Validate whether the feature list & feature list namespace are consistent
-
-    Parameters
-    ----------
-    feature: FeatureModel
-        Feature object
-    feature_namespace: FeatureNamespaceModel
-        FeatureNamespace object
-
-    Raises
-    ------
-    DocumentInconsistencyError
-        If the inconsistency between version & namespace found
-    """
-    attrs = ["name", "dtype", "entity_ids", "table_ids"]
-    for attr in attrs:
-        version_attr = getattr(feature, attr)
-        namespace_attr = getattr(feature_namespace, attr)
-        version_attr_str: str | list[str] = f'"{version_attr}"'
-        namespace_attr_str: str | list[str] = f'"{namespace_attr}"'
-        if isinstance(version_attr, list):
-            version_attr = sorted(version_attr)
-            version_attr_str = [str(val) for val in version_attr]
-
-        if isinstance(namespace_attr, list):
-            namespace_attr = sorted(namespace_attr)
-            namespace_attr_str = [str(val) for val in namespace_attr]
-
-        if version_attr != namespace_attr:
-            raise DocumentInconsistencyError(
-                f'Feature (name: "{feature.name}") object(s) within the same namespace '
-                f'must have the same "{attr}" value (namespace: {namespace_attr_str}, '
-                f"feature: {version_attr_str})."
-            )
-
-
-def sanitize_query_graph_for_feature_definition(graph: QueryGraphModel) -> QueryGraphModel:
-    """
-    Sanitize the query graph for feature creation
-
-    Parameters
-    ----------
-    graph: QueryGraphModel
-        The query graph
-
-    Returns
-    -------
-    QueryGraphModel
-    """
-    # Since the generated feature definition contains all the settings in manual mode,
-    # we need to sanitize the graph to make sure that the graph is in manual mode.
-    # Otherwise, the generated feature definition & graph hash before and after
-    # feature creation could be different.
-    output = graph.dict()
-    for node in output["nodes"]:
-        if node["type"] == NodeType.GRAPH:
-            if "view_mode" in node["parameters"]["metadata"]:
-                node["parameters"]["metadata"]["view_mode"] = "manual"
-    return QueryGraphModel(**output)
-
-
-class FeatureService(BaseDocumentService[FeatureModel, FeatureServiceCreate, FeatureServiceUpdate]):
-    """
-    FeatureService class
-    """
-
-    document_class = FeatureModel
-
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user=user, persistent=persistent, catalog_id=catalog_id)
-        self.view_construction_service = ViewConstructionService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-
-    async def _get_feature_version(self, name: str) -> VersionIdentifier:
-        version_name = get_version()
-        query_result = await self.list_documents(
-            query_filter={"name": name, "version.name": version_name}
-        )
-        count = query_result["total"]
-        return VersionIdentifier(name=version_name, suffix=count or None)
-
-    async def _prepare_graph_to_store(
-        self, feature: FeatureModel, sanitize_for_definition: bool = False
-    ) -> tuple[QueryGraphModel, str]:
-        # reconstruct view graph node to remove unused column cleaning operations
-        graph, node_name_map = await self.view_construction_service.construct_graph(
-            query_graph=feature.graph,
-            target_node=feature.node,
-            table_cleaning_operations=[],
-        )
-        node = graph.get_node_by_name(node_name_map[feature.node_name])
-
-        # prune the graph to remove unused nodes
-        pruned_graph, pruned_node_name_map = QueryGraph(**graph.dict(by_alias=True)).prune(
-            target_node=node, aggressive=True
-        )
-        if sanitize_for_definition:
-            pruned_graph = sanitize_query_graph_for_feature_definition(graph=pruned_graph)
-        return pruned_graph, pruned_node_name_map[node.name]
-
-    async def prepare_feature_model(
-        self, data: FeatureServiceCreate, sanitize_for_definition: bool
-    ) -> FeatureModel:
+from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.mixin import OpsServiceMixin
+from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.session.base import BaseSession
+
+
+class OnlineEnableService(OpsServiceMixin):
+    """
+    OnlineEnableService class is responsible for maintaining the feature & feature list structure
+    of feature online enablement.
+    """
+
+    def __init__(
+        self,
+        persistent: Persistent,
+        session_manager_service: SessionManagerService,
+        feature_service: FeatureService,
+        feature_store_service: FeatureStoreService,
+        feature_namespace_service: FeatureNamespaceService,
+        feature_list_service: FeatureListService,
+        feature_manager_service: FeatureManagerService,
+    ):
+        # pylint: disable=too-many-arguments
+        self.persistent = persistent
+        self.feature_service = feature_service
+        self.session_manager_service = session_manager_service
+        self.feature_store_service = feature_store_service
+        self.feature_namespace_service = feature_namespace_service
+        self.feature_list_service = feature_list_service
+        self.feature_manager_service = feature_manager_service
+
+    @classmethod
+    def _extract_online_enabled_feature_ids(
+        cls, feature: FeatureModel, document: FeatureListModel | FeatureNamespaceModel
+    ) -> list[ObjectId]:
+        if feature.online_enabled:
+            return cls.include_object_id(document.online_enabled_feature_ids, feature.id)
+        return cls.exclude_object_id(document.online_enabled_feature_ids, feature.id)
+
+    async def _update_feature_list(
+        self,
+        feature_list_id: ObjectId,
+        feature: FeatureModel,
+        return_document: bool = True,
+    ) -> Optional[FeatureListModel]:
         """
-        Prepare the feature model by pruning the query graph
+        Update online_enabled_feature_ids in feature list
 
         Parameters
         ----------
-        data: FeatureServiceCreate
-            Feature creation data
-        sanitize_for_definition: bool
-            Whether to sanitize the query graph for generating feature definition
+        feature_list_id: ObjectId
+            Target feature list ID
+        feature: FeatureModel
+            Updated Feature object
+        return_document: bool
+            Whether to return updated document
 
         Returns
         -------
-        FeatureModel
+        Optional[FeatureListModel]
         """
-        document = FeatureModel(
-            **{
-                **data.json_dict(),
-                "readiness": FeatureReadiness.DRAFT,
-                "version": await self._get_feature_version(data.name),
-                "user_id": self.user.id,
-                "catalog_id": self.catalog_id,
-            }
-        )
-
-        # prepare the graph to store
-        graph, node_name = await self._prepare_graph_to_store(
-            feature=document, sanitize_for_definition=sanitize_for_definition
-        )
-
-        # create a new feature document (so that the derived attributes like table_ids is generated properly)
-        return FeatureModel(**{**document.json_dict(), "graph": graph, "node_name": node_name})
-
-    async def prepare_feature_definition(self, document: FeatureModel) -> str:
+        document = await self.feature_list_service.get_document(document_id=feature_list_id)
+        return await self.feature_list_service.update_document(
+            document_id=feature_list_id,
+            data=FeatureListServiceUpdate(
+                online_enabled_feature_ids=self._extract_online_enabled_feature_ids(
+                    feature=feature, document=document
+                ),
+            ),
+            document=document,
+            return_document=return_document,
+        )
+
+    async def _update_feature_namespace(
+        self,
+        feature_namespace_id: ObjectId,
+        feature: FeatureModel,
+        return_document: bool = True,
+    ) -> Optional[FeatureNamespaceModel]:
         """
-        Prepare the feature definition for the given feature document
+        Update online_enabled_feature_ids in feature namespace
 
         Parameters
         ----------
-        document: FeatureModel
-            Feature document
+        feature_namespace_id: ObjectId
+            FeatureNamespace ID
+        feature: FeatureModel
+            Updated Feature object
+        return_document: bool
+            Whether to return updated document
 
         Returns
         -------
-        str
+        Optional[FeatureNamespaceModel]
         """
-        # check whether table has been saved at persistent storage
-        table_service = TableService(
-            user=self.user, persistent=self.persistent, catalog_id=self.catalog_id
-        )
-        table_id_to_info: Dict[ObjectId, Dict[str, Any]] = {}
-        for table_id in document.table_ids:
-            table = await table_service.get_document(document_id=table_id)
-            table_id_to_info[table_id] = table.dict()
-
-        # create feature definition
-        graph, node_name = document.graph, document.node_name
-        sdk_code_gen_state = SDKCodeExtractor(graph=graph).extract(
-            node=graph.get_node_by_name(node_name),
-            to_use_saved_data=True,
-            table_id_to_info=table_id_to_info,
-            output_id=document.id,
+        document = await self.feature_namespace_service.get_document(
+            document_id=feature_namespace_id
         )
-        definition = sdk_code_gen_state.code_generator.generate(to_format=True)
-        return definition
+        return await self.feature_namespace_service.update_document(
+            document_id=feature_namespace_id,
+            data=FeatureNamespaceServiceUpdate(
+                online_enabled_feature_ids=self._extract_online_enabled_feature_ids(
+                    feature=feature, document=document
+                ),
+            ),
+            document=document,
+            return_document=return_document,
+        )
+
+    @staticmethod
+    async def update_data_warehouse_with_session(
+        session: BaseSession,
+        feature_manager_service: FeatureManagerService,
+        feature: FeatureModel,
+        is_recreating_schema: bool = False,
+    ) -> None:
+        """
+        Update data warehouse registry upon changes to online enable status, such as enabling or
+        disabling scheduled tile and feature jobs
 
-    async def create_document(self, data: FeatureServiceCreate) -> FeatureModel:
-        document = await self.prepare_feature_model(data=data, sanitize_for_definition=False)
-        async with self.persistent.start_transaction() as session:
-            # check any conflict with existing documents
-            await self._check_document_unique_constraints(document=document)
-
-            # prepare feature definition
-            definition = await self.prepare_feature_definition(document=document)
-
-            # insert the document
-            insert_id = await session.insert_one(
-                collection_name=self.collection_name,
-                document={
-                    **document.dict(by_alias=True),
-                    "definition": definition,
-                    "raw_graph": data.graph.dict(),
-                },
-                user_id=self.user.id,
+        Parameters
+        ----------
+        session: BaseSession
+            Session object
+        feature_manager_service: FeatureManagerService
+            An instance of FeatureManagerService to handle materialization of features and tiles
+        feature: FeatureModel
+            Updated Feature object
+        is_recreating_schema: bool
+            Whether we are recreating the working schema from scratch. Only set as True when called
+            by WorkingSchemaService.
+        """
+        extended_feature_model = ExtendedFeatureModel(**feature.dict(by_alias=True))
+        online_feature_spec = OnlineFeatureSpec(feature=extended_feature_model)
+
+        if not online_feature_spec.is_online_store_eligible:
+            return
+
+        if feature.online_enabled:
+            await feature_manager_service.online_enable(
+                session, online_feature_spec, is_recreating_schema=is_recreating_schema
             )
-            assert insert_id == document.id
+        else:
+            await feature_manager_service.online_disable(online_feature_spec)
 
-            feature_namespace_service = FeatureNamespaceService(
-                user=self.user, persistent=self.persistent, catalog_id=self.catalog_id
-            )
-            try:
-                feature_namespace = await feature_namespace_service.get_document(
-                    document_id=document.feature_namespace_id,
-                )
-                await validate_feature_version_and_namespace_consistency(
-                    feature=document, feature_namespace=feature_namespace
-                )
-                await feature_namespace_service.update_document(
-                    document_id=document.feature_namespace_id,
-                    data=FeatureNamespaceServiceUpdate(
-                        feature_ids=self.include_object_id(
-                            feature_namespace.feature_ids, document.id
-                        )
-                    ),
-                    return_document=True,
-                )
-            except DocumentNotFoundError:
-                await feature_namespace_service.create_document(
-                    data=FeatureNamespaceCreate(
-                        _id=document.feature_namespace_id,
-                        name=document.name,
-                        dtype=document.dtype,
-                        feature_ids=[insert_id],
-                        readiness=FeatureReadiness.DRAFT,
-                        default_feature_id=insert_id,
-                        default_version_mode=DefaultVersionMode.AUTO,
-                        entity_ids=sorted(document.entity_ids),
-                        table_ids=sorted(document.table_ids),
-                    ),
-                )
-        return await self.get_document(document_id=insert_id)
+    async def update_data_warehouse(
+        self, updated_feature: FeatureModel, online_enabled_before_update: bool, get_credential: Any
+    ) -> None:
+        """
+        Update data warehouse registry upon changes to online enable status, such as enabling or
+        disabling scheduled tile and feature jobs
 
-    async def get_document_by_name_and_version(
-        self, name: str, version: VersionIdentifier
+        Parameters
+        ----------
+        updated_feature: FeatureModel
+            Updated Feature
+        online_enabled_before_update: bool
+            Online enabled status
+        get_credential: Any
+            Get credential handler function
+        """
+        if updated_feature.online_enabled == online_enabled_before_update:
+            # updated_feature has the same online_enabled status as the original one
+            # no need to update
+            return
+
+        feature_store_model = await self.feature_store_service.get_document(
+            document_id=updated_feature.tabular_source.feature_store_id
+        )
+        session = await self.session_manager_service.get_feature_store_session(
+            feature_store_model, get_credential
+        )
+        await self.update_data_warehouse_with_session(
+            session=session,
+            feature_manager_service=self.feature_manager_service,
+            feature=updated_feature,
+        )
+
+    async def update_feature(
+        self,
+        feature_id: ObjectId,
+        online_enabled: bool,
     ) -> FeatureModel:
         """
-        Retrieve feature given name & version
+        Update feature online enabled & trigger list of cascading updates
 
         Parameters
         ----------
-        name: str
-            Feature name
-        version: VersionIdentifier
-            Feature version
+        feature_id: ObjectId
+            Target feature ID
+        online_enabled: bool
+            Value to update the feature online_enabled status
 
         Returns
         -------
         FeatureModel
-
-        Raises
-        ------
-        DocumentNotFoundError
-            If the specified feature name & version cannot be found
         """
-        document_dict = None
-        query_filter = {"name": name, "version": version.dict()}
-        async for doc_dict in self.list_documents_iterator(query_filter=query_filter, page_size=1):
-            document_dict = doc_dict
-
-        if document_dict is None:
-            exception_detail = (
-                f'{self.class_name} (name: "{name}", version: "{version.to_str()}") not found. '
-                f"Please save the {self.class_name} object first."
-            )
-            raise DocumentNotFoundError(exception_detail)
-        return FeatureModel(**document_dict)
+        document = await self.feature_service.get_document(document_id=feature_id)
+        if document.online_enabled != online_enabled:
+            async with self.persistent.start_transaction():
+                feature = await self.feature_service.update_document(
+                    document_id=feature_id,
+                    data=FeatureServiceUpdate(online_enabled=online_enabled),
+                    document=document,
+                    return_document=True,
+                )
+                assert isinstance(feature, FeatureModel)
+                await self._update_feature_namespace(
+                    feature_namespace_id=feature.feature_namespace_id,
+                    feature=feature,
+                    return_document=False,
+                )
+                for feature_list_id in feature.feature_list_ids:
+                    await self._update_feature_list(
+                        feature_list_id=feature_list_id,
+                        feature=feature,
+                        return_document=False,
+                    )
+
+                return await self.feature_service.get_document(document_id=feature_id)
+
+        return document
```

### Comparing `featurebyte-0.3.1/featurebyte/service/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/service/feature_job_setting_analysis.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 """
 FeatureJobSettingAnalysisService class
 """
 from __future__ import annotations
 
+from typing import Any, Optional
+
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentError
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.feature_job_setting_analysis import FeatureJobSettingAnalysisModel
+from featurebyte.persistent import Persistent
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
 from featurebyte.schema.feature_job_setting_analysis import (
     FeatureJobSettingAnalysisBacktest,
     FeatureJobSettingAnalysisCreate,
 )
 from featurebyte.schema.worker.task.feature_job_setting_analysis import (
     FeatureJobSettingAnalysisBackTestTaskPayload,
@@ -30,14 +33,24 @@
 ):
     """
     FeatureJobSettingAnalysisService class
     """
 
     document_class = FeatureJobSettingAnalysisModel
 
+    def __init__(
+        self,
+        user: Any,
+        persistent: Persistent,
+        catalog_id: Optional[ObjectId],
+        event_table_service: EventTableService,
+    ):
+        super().__init__(user, persistent, catalog_id)
+        self.event_table_service = event_table_service
+
     async def create_document_creation_task(
         self, data: FeatureJobSettingAnalysisCreate
     ) -> FeatureJobSettingAnalysisTaskPayload:
         """
         Create document creation task payload
 
         Parameters
@@ -57,20 +70,15 @@
         # check any conflict with existing documents
         output_document_id = data.id or ObjectId()
         await self._check_document_unique_constraints(
             document=FeatureByteBaseDocumentModel(_id=output_document_id),
         )
 
         # check that event table exists
-        event_table_service = EventTableService(
-            user=self.user,
-            persistent=self.persistent,
-            catalog_id=self.catalog_id,
-        )
-        event_table = await event_table_service.get_document(document_id=data.event_table_id)
+        event_table = await self.event_table_service.get_document(document_id=data.event_table_id)
         if not event_table.record_creation_timestamp_column:
             raise DocumentError("Creation date column is not available for the event table.")
 
         return FeatureJobSettingAnalysisTaskPayload(
             **data.dict(),
             user_id=self.user.id,
             catalog_id=self.catalog_id,
```

### Comparing `featurebyte-0.3.1/featurebyte/service/feature_list_namespace.py` & `featurebyte-0.4.0/featurebyte/service/feature_namespace.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 """
-FeatureListNamespaceService class
+FeatureNamespaceService class
 """
 from __future__ import annotations
 
-from featurebyte.models.feature_list import FeatureListNamespaceModel
-from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
+from featurebyte.models.feature_namespace import FeatureNamespaceModel
+from featurebyte.schema.feature_namespace import (
+    FeatureNamespaceCreate,
+    FeatureNamespaceServiceUpdate,
+)
 from featurebyte.service.base_document import BaseDocumentService
 
 
-class FeatureListNamespaceService(
+class FeatureNamespaceService(
     BaseDocumentService[
-        FeatureListNamespaceModel, FeatureListNamespaceModel, FeatureListNamespaceServiceUpdate
+        FeatureNamespaceModel, FeatureNamespaceCreate, FeatureNamespaceServiceUpdate
     ],
 ):
     """
-    FeatureListNamespaceService class
+    FeatureNamespaceService class
     """
 
-    document_class = FeatureListNamespaceModel
+    document_class = FeatureNamespaceModel
```

### Comparing `featurebyte-0.3.1/featurebyte/service/feature_list_status.py` & `featurebyte-0.4.0/featurebyte/service/feature_list_status.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,33 @@
 """
 FeatureListStatusService class
 """
-from typing import Any
 
 import json
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
 from featurebyte.models.feature_list import FeatureListStatus
-from featurebyte.persistent import Persistent
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 
 
-class FeatureListStatusService(BaseService):
+class FeatureListStatusService:
     """
     FeatureListStatusService class is responsible for handling feature list status update and
     orchestrate feature list status update through FeatureListService and FeatureListNamespaceService.
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         feature_list_namespace_service: FeatureListNamespaceService,
         feature_list_service: FeatureListService,
     ):
-        super().__init__(user, persistent, catalog_id)
         self.feature_list_namespace_service = feature_list_namespace_service
         self.feature_list_service = feature_list_service
 
     async def update_feature_list_namespace_status(
         self, feature_list_namespace_id: ObjectId, target_feature_list_status: FeatureListStatus
     ) -> None:
         """
@@ -59,26 +52,36 @@
             str(feature_list_id)
             for feature_list_id in feature_list_namespace.deployed_feature_list_ids
         ]
         if feature_list_namespace.status == target_feature_list_status:
             # if no change in status, do nothing
             return
 
+        if (
+            feature_list_namespace.status == FeatureListStatus.DRAFT
+            and target_feature_list_status == FeatureListStatus.DEPRECATED
+        ):
+            raise DocumentUpdateError(
+                "Not allowed to update status of feature list from DRAFT to DEPRECATED. "
+                "Valid status transition: DRAFT -> PUBLIC_DRAFT or DRAFT -> TEMPLATE. "
+                "Please delete feature list instead if it is no longer needed."
+            )
+
         if target_feature_list_status == FeatureListStatus.DRAFT:
             # feature list is not allowed to be updated to draft status
             raise DocumentUpdateError(
                 f'Not allowed to update status of FeatureList (name: "{feature_list_namespace.name}") '
-                f"to draft status."
+                "to draft status."
             )
 
         if target_feature_list_status == FeatureListStatus.DEPLOYED:
             if not deployed_feature_list_ids:
                 raise DocumentUpdateError(
                     f'Not allowed to update status of FeatureList (name: "{feature_list_namespace.name}") '
-                    f"to deployed status without deployed feature list."
+                    "to deployed status without deployed feature list."
                 )
 
         if feature_list_namespace.status == FeatureListStatus.DEPLOYED:
             # if feature list is deployed, it can only be updated to public draft status if there is no
             # deployed feature list
             if deployed_feature_list_ids:
                 raise DocumentUpdateError(
```

### Comparing `featurebyte-0.3.1/featurebyte/service/feature_readiness.py` & `featurebyte-0.4.0/featurebyte/service/feature_readiness.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,69 +4,59 @@
 from __future__ import annotations
 
 from typing import Any, Optional, Sequence
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
-from featurebyte.models.feature import (
-    DefaultVersionMode,
-    FeatureModel,
-    FeatureNamespaceModel,
-    FeatureReadiness,
-)
+from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import (
     FeatureListModel,
     FeatureListNamespaceModel,
     FeatureReadinessTransition,
 )
+from featurebyte.models.feature_namespace import (
+    DefaultVersionMode,
+    FeatureNamespaceModel,
+    FeatureReadiness,
+)
 from featurebyte.persistent import Persistent
 from featurebyte.schema.feature import FeatureServiceUpdate
 from featurebyte.schema.feature_list import FeatureListServiceUpdate
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
 from featurebyte.schema.feature_namespace import FeatureNamespaceServiceUpdate
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
-from featurebyte.service.table import TableService
+from featurebyte.service.mixin import OpsServiceMixin
 from featurebyte.service.validator.production_ready_validator import ProductionReadyValidator
-from featurebyte.service.version import VersionService
 
 
-class FeatureReadinessService(BaseService):
+class FeatureReadinessService(OpsServiceMixin):
     """
     FeatureReadinessService class is responsible for maintaining the feature readiness structure
     consistencies between feature & feature list (version & namespace).
     """
 
     def __init__(
         self,
-        user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
-        table_service: TableService,
         feature_service: FeatureService,
         feature_namespace_service: FeatureNamespaceService,
         feature_list_service: FeatureListService,
         feature_list_namespace_service: FeatureListNamespaceService,
-        version_service: VersionService,
+        production_ready_validator: ProductionReadyValidator,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.table_service = table_service
+        self.persistent = persistent
         self.feature_service = feature_service
         self.feature_namespace_service = feature_namespace_service
         self.feature_list_service = feature_list_service
         self.feature_list_namespace_service = feature_list_namespace_service
-        self.production_ready_validator = ProductionReadyValidator(
-            table_service=table_service,
-            feature_service=feature_service,
-            version_service=version_service,
-        )
+        self.production_ready_validator = production_ready_validator
 
     async def _get_default_feature_list(
         self, feature_list_ids: Sequence[ObjectId]
     ) -> FeatureListModel:
         """
         Get default feature from list of feature IDs
 
@@ -77,18 +67,17 @@
 
         Returns
         -------
         FeatureListModel
         """
         assert len(feature_list_ids) > 0, "feature_list_ids should not be empty"
         default_feature_list: Optional[FeatureListModel] = None
-        async for feature_list_doc in self.feature_list_service.list_documents_iterator(
+        async for feature_list in self.feature_list_service.list_documents_iterator(
             query_filter={"_id": {"$in": feature_list_ids}}
         ):
-            feature_list = FeatureListModel(**feature_list_doc)
             if default_feature_list is None:
                 default_feature_list = feature_list
             elif feature_list.readiness_distribution > default_feature_list.readiness_distribution:
                 default_feature_list = feature_list
             elif (
                 feature_list.readiness_distribution == default_feature_list.readiness_distribution
                 and feature_list.created_at > default_feature_list.created_at  # type: ignore[operator]
@@ -208,21 +197,22 @@
 
         Returns
         -------
         FeatureModel
         """
         assert len(feature_ids) > 0, "feature_ids should not be empty"
         default_feature: Optional[FeatureModel] = None
-        async for feature_doc in self.feature_service.list_documents_iterator(
+        async for feature in self.feature_service.list_documents_iterator(
             query_filter={"_id": {"$in": feature_ids}}
         ):
-            feature = FeatureModel(**feature_doc)
             if default_feature is None:
                 default_feature = feature
-            elif feature.readiness > default_feature.readiness:
+            elif FeatureReadiness(feature.readiness) > FeatureReadiness(default_feature.readiness):
+                # when doing non-equality comparison, must cast it explicitly to FeatureReadiness
+                # otherwise, it will become normal string comparison
                 default_feature = feature
             elif (
                 feature.readiness == default_feature.readiness
                 and feature.created_at > default_feature.created_at  # type: ignore[operator]
             ):
                 default_feature = feature
         assert default_feature is not None, "default_feature should not be None"
@@ -303,14 +293,24 @@
 
         if (
             document.readiness != FeatureReadiness.DRAFT
             and target_readiness == FeatureReadiness.DRAFT
         ):
             raise DocumentUpdateError("Cannot update feature readiness to DRAFT.")
 
+        if (
+            document.readiness == FeatureReadiness.DRAFT
+            and target_readiness == FeatureReadiness.DEPRECATED
+        ):
+            raise DocumentUpdateError(
+                "Not allowed to update feature readiness from DRAFT to DEPRECATED. "
+                "Valid transitions are DRAFT -> PUBLIC_DRAFT or DRAFT -> PRODUCTION_READY. "
+                "Please delete the feature instead if it is no longer needed."
+            )
+
     async def update_feature(
         self,
         feature_id: ObjectId,
         readiness: FeatureReadiness,
         ignore_guardrails: bool = False,
         return_document: bool = True,
     ) -> Optional[FeatureModel]:
```

### Comparing `featurebyte-0.3.1/featurebyte/service/feature_store_warehouse.py` & `featurebyte-0.4.0/featurebyte/service/tile_manager.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,457 +1,418 @@
 """
-Service for interacting with the data warehouse for queries around the feature store.
-
-We split this into a separate service, as these typically require a session object that is created.
+TileManagerService class
 """
 from __future__ import annotations
 
-from typing import Any, List
+from typing import Callable, List, Optional, Tuple
 
-import datetime
+import time
+from datetime import datetime
 
+import numpy as np
 import pandas as pd
-from bson.objectid import ObjectId
-from sqlglot import expressions
 
-from featurebyte.common.date_util import get_next_job_datetime
-from featurebyte.common.utils import dataframe_to_json
-from featurebyte.exception import DatabaseNotFoundError, SchemaNotFoundError, TableNotFoundError
-from featurebyte.feature_manager.model import ExtendedFeatureModel
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.persistent import Persistent
-from featurebyte.query_graph.node.schema import ColumnSpec
-from featurebyte.query_graph.sql.ast.literal import make_literal_value
-from featurebyte.query_graph.sql.common import quoted_identifier, sql_to_string
-from featurebyte.service.base_service import BaseService
-from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.enum import InternalName
+from featurebyte.logging import get_logger
+from featurebyte.models.tile import TileScheduledJobParameters, TileSpec, TileType
+from featurebyte.service.feature import FeatureService
+from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
+from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
+from featurebyte.service.tile_registry_service import TileRegistryService
+from featurebyte.service.tile_scheduler import TileSchedulerService
 from featurebyte.session.base import BaseSession
+from featurebyte.sql.tile_generate import TileGenerate
+from featurebyte.sql.tile_generate_entity_tracking import TileGenerateEntityTracking
+from featurebyte.sql.tile_schedule_online_store import TileScheduleOnlineStore
+from featurebyte.tile.sql_template import tm_retrieve_tile_job_audit_logs
+
+logger = get_logger(__name__)
 
 
-class FeatureStoreWarehouseService(BaseService):
+class TileManagerService:
     """
-    FeatureStoreWarehouseService is responsible for interacting with the data warehouse.
+    TileManagerService is responsible for materialization of tiles in the data warehouse and
+    scheduling of periodic tile jobs
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
-        session_manager_service: SessionManagerService,
-        feature_store_service: FeatureStoreService,
+        online_store_table_version_service: OnlineStoreTableVersionService,
+        online_store_compute_query_service: OnlineStoreComputeQueryService,
+        tile_scheduler_service: TileSchedulerService,
+        tile_registry_service: TileRegistryService,
+        feature_service: FeatureService,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.session_manager_service = session_manager_service
-        self.feature_store_service = feature_store_service
-
-    @classmethod
-    async def check_database_exists(
-        cls,
-        db_session: BaseSession,
-        database_name: str,
+        self.online_store_table_version_service = online_store_table_version_service
+        self.online_store_compute_query_service = online_store_compute_query_service
+        self.tile_scheduler_service = tile_scheduler_service
+        self.tile_registry_service = tile_registry_service
+        self.feature_service = feature_service
+
+    async def generate_tiles_on_demand(
+        self,
+        session: BaseSession,
+        tile_inputs: List[Tuple[TileSpec, str]],
+        progress_callback: Optional[Callable[[int, str], None]] = None,
     ) -> None:
         """
-        Check tables in feature store
+        Generate Tiles and update tile entity checking table
 
         Parameters
         ----------
-        db_session: BaseSession
-            BaseSession object
-        database_name: str
-            Name of database to use
-
-        Raises
-        ------
-        DatabaseNotFoundError
-            If database does not exist
-        """
-        databases = await db_session.list_databases()
-        databases = [database.lower() for database in databases]
-        if database_name.lower() not in databases:
-            raise DatabaseNotFoundError(f"Database {database_name} not found.")
-
-    @classmethod
-    async def check_schema_exists(
-        cls,
-        db_session: BaseSession,
-        database_name: str,
-        schema_name: str,
-    ) -> None:
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        tile_inputs: List[Tuple[TileSpec, str]]
+            list of TileSpec, temp_entity_table to update the feature store
+        progress_callback: Optional[Callable[[int, str], None]]
+            Optional progress callback function
+        """
+        num_jobs = len(tile_inputs)
+        if progress_callback:
+            progress_callback(0, f"0/{num_jobs} completed")
+
+        for index, (tile_spec, entity_table) in enumerate(tile_inputs):
+            tic = time.time()
+            await self.generate_tiles(
+                session=session,
+                tile_spec=tile_spec,
+                tile_type=TileType.OFFLINE,
+                start_ts_str=None,
+                end_ts_str=None,
+            )
+            logger.debug(
+                "Done generating tiles",
+                extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
+            )
+
+            tic = time.time()
+            await self.update_tile_entity_tracker(
+                session=session, tile_spec=tile_spec, temp_entity_table=entity_table
+            )
+            logger.debug(
+                "Done update_tile_entity_tracker",
+                extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
+            )
+
+            if progress_callback:
+                progress_callback(
+                    int(100 * np.floor((index + 1) / num_jobs)),
+                    f"{index+1}/{num_jobs} completed",
+                )
+
+    async def tile_job_exists(self, tile_spec: TileSpec) -> bool:
         """
-        Check tables in feature store
+        Get existing tile jobs for the given tile_spec
 
         Parameters
         ----------
-        db_session: BaseSession
-            BaseSession object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-
-        Raises
-        ------
-        SchemaNotFoundError
-            If schema does not exist
-        """
-        await cls.check_database_exists(db_session=db_session, database_name=database_name)
-        schemas = await db_session.list_schemas(database_name=database_name)
-        schemas = [schema.lower() for schema in schemas]
-        if schema_name.lower() not in schemas:
-            raise SchemaNotFoundError(f"Schema {schema_name} not found.")
-
-    @classmethod
-    async def check_table_exists(
-        cls,
-        db_session: BaseSession,
-        database_name: str,
-        schema_name: str,
-        table_name: str,
+        tile_spec: TileSpec
+            the input TileSpec
+
+        Returns
+        -------
+            whether the tile jobs already exist
+        """
+        job_id = f"{TileType.ONLINE}_{tile_spec.aggregation_id}"
+        return await self.tile_scheduler_service.get_job_details(job_id=job_id) is not None
+
+    async def populate_feature_store(
+        self,
+        session: BaseSession,
+        tile_spec: TileSpec,
+        job_schedule_ts_str: str,
+        aggregation_result_name: str,
     ) -> None:
         """
-        Check tables in feature store
+        Populate feature store with the given tile_spec and timestamp string
 
         Parameters
         ----------
-        db_session: BaseSession
-            BaseSession object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-        table_name: str
-            Name of table to use
-
-        Raises
-        ------
-        TableNotFoundError
-            If table does not exist
-        """
-        await cls.check_schema_exists(
-            db_session=db_session, database_name=database_name, schema_name=schema_name
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        tile_spec: TileSpec
+            the input TileSpec
+        job_schedule_ts_str: str
+            timestamp string of the job schedule
+        aggregation_result_name: str
+            aggregation result name to populate
+        """
+        executor = TileScheduleOnlineStore(
+            session=session,
+            aggregation_id=tile_spec.aggregation_id,
+            job_schedule_ts_str=job_schedule_ts_str,
+            online_store_table_version_service=self.online_store_table_version_service,
+            online_store_compute_query_service=self.online_store_compute_query_service,
+            aggregation_result_name=aggregation_result_name,
         )
-        tables = await db_session.list_tables(database_name=database_name, schema_name=schema_name)
-        tables = [table.lower() for table in tables]
-        if table_name.lower() not in tables:
-            raise TableNotFoundError(f"Table {table_name} not found.")
-
-    async def list_databases(
-        self, feature_store: FeatureStoreModel, get_credential: Any
-    ) -> List[str]:
+        await executor.execute()
+
+    async def generate_tiles(
+        self,
+        session: BaseSession,
+        tile_spec: TileSpec,
+        tile_type: TileType,
+        start_ts_str: Optional[str],
+        end_ts_str: Optional[str],
+        last_tile_start_ts_str: Optional[str] = None,
+    ) -> None:
         """
-        List databases in feature store
+        Manually trigger tile generation
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        get_credential: Any
-            Get credential handler function
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        tile_spec: TileSpec
+            the input TileSpec
+        tile_type: TileType
+            tile type. ONLINE or OFFLINE
+        start_ts_str: str
+            start_timestamp of tile. ie. 2022-06-20 15:00:00
+        end_ts_str: str
+            end_timestamp of tile. ie. 2022-06-21 15:00:00
+        last_tile_start_ts_str: str
+            start date string of last tile used to update the tile_registry table
+        """
+
+        if start_ts_str and end_ts_str:
+            tile_sql = tile_spec.tile_sql.replace(
+                InternalName.TILE_START_DATE_SQL_PLACEHOLDER, f"'{start_ts_str}'"
+            ).replace(InternalName.TILE_END_DATE_SQL_PLACEHOLDER, f"'{end_ts_str}'")
+        else:
+            tile_sql = tile_spec.tile_sql
+
+        tile_generate_ins = TileGenerate(
+            session=session,
+            feature_store_id=tile_spec.feature_store_id,
+            tile_id=tile_spec.tile_id,
+            time_modulo_frequency_second=tile_spec.time_modulo_frequency_second,
+            blind_spot_second=tile_spec.blind_spot_second,
+            frequency_minute=tile_spec.frequency_minute,
+            sql=tile_sql,
+            entity_column_names=tile_spec.entity_column_names,
+            value_column_names=tile_spec.value_column_names,
+            value_column_types=tile_spec.value_column_types,
+            tile_type=tile_type,
+            last_tile_start_str=last_tile_start_ts_str,
+            aggregation_id=tile_spec.aggregation_id,
+            tile_registry_service=self.tile_registry_service,
+        )
+        await tile_generate_ins.execute()
+
+    async def update_tile_entity_tracker(
+        self, session: BaseSession, tile_spec: TileSpec, temp_entity_table: str
+    ) -> str:
+        """
+        Update <tile_id>_entity_tracker table for last_tile_start_date
+
+        Parameters
+        ----------
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        tile_spec: TileSpec
+            the input TileSpec
+        temp_entity_table: str
+            temporary entity table to be merged into <tile_id>_entity_tracker
 
         Returns
         -------
-        List[str]
-            List of database names
+            spark job run detail
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
+        if tile_spec.category_column_name is None:
+            entity_column_names = tile_spec.entity_column_names
+        else:
+            entity_column_names = [
+                c for c in tile_spec.entity_column_names if c != tile_spec.category_column_name
+            ]
+
+        tile_entity_tracking_ins = TileGenerateEntityTracking(
+            session=session,
+            tile_id=tile_spec.aggregation_id,
+            entity_column_names=entity_column_names,
+            entity_table=temp_entity_table,
         )
-        return await db_session.list_databases()
 
-    async def list_schemas(
+        await tile_entity_tracking_ins.execute()
+
+        return tile_entity_tracking_ins.json()
+
+    async def schedule_online_tiles(
         self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        get_credential: Any,
-    ) -> List[str]:
+        tile_spec: TileSpec,
+        monitor_periods: int = 10,
+    ) -> Optional[str]:
         """
-        List schemas in feature store
+        Schedule online tiles
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        get_credential: Any
-            Get credential handler function
+        tile_spec: TileSpec
+            the input TileSpec
+        monitor_periods: int
+            number of tile periods to monitor and re-generate. Default is 10
 
         Returns
         -------
-        List[str]
-            List of schema names
+            generated sql to be executed or None if the tile job already exists
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
+        sql = await self._schedule_tiles_custom(
+            tile_spec=tile_spec,
+            tile_type=TileType.ONLINE,
+            monitor_periods=monitor_periods,
         )
-        # check database exists
-        await self.check_database_exists(db_session=db_session, database_name=database_name)
 
-        return await db_session.list_schemas(database_name=database_name)
+        return sql
 
-    async def list_tables(
+    async def schedule_offline_tiles(
         self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        schema_name: str,
-        get_credential: Any,
-    ) -> List[str]:
+        tile_spec: TileSpec,
+        offline_minutes: int = 1440,
+    ) -> Optional[str]:
         """
-        List tables in feature store
+        Schedule offline tiles
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-        get_credential: Any
-            Get credential handler function
+        tile_spec: TileSpec
+            the input TileSpec
+        offline_minutes: int
+            offline tile lookback minutes to monitor and re-generate. Default is 1440
 
         Returns
         -------
-        List[str]
-            List of table names
+            generated sql to be executed or None if the tile job already exists
         """
 
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
-        )
-
-        # check schema exists
-        await self.check_schema_exists(
-            db_session=db_session, database_name=database_name, schema_name=schema_name
+        sql = await self._schedule_tiles_custom(
+            tile_spec=tile_spec,
+            tile_type=TileType.OFFLINE,
+            offline_minutes=offline_minutes,
         )
 
-        tables = await db_session.list_tables(database_name=database_name, schema_name=schema_name)
-        # exclude tables with names that has a "__" prefix
-        return [table_name for table_name in tables if not table_name.startswith("__")]
+        return sql
 
-    async def list_columns(
+    async def _schedule_tiles_custom(
         self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        schema_name: str,
-        table_name: str,
-        get_credential: Any,
-    ) -> List[ColumnSpec]:
+        tile_spec: TileSpec,
+        tile_type: TileType,
+        offline_minutes: int = 1440,
+        monitor_periods: int = 10,
+    ) -> Optional[str]:
         """
-        List columns in database table
+        Common tile schedule method
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-        table_name: str
-            Name of table to use
-        get_credential: Any
-            Get credential handler function
+        tile_spec: TileSpec
+            the input TileSpec
+        tile_type: TileType
+            ONLINE or OFFLINE
+        offline_minutes: int
+            offline tile lookback minutes
+        monitor_periods: int
+            online tile lookback period
 
         Returns
         -------
-        List[ColumnSpec]
-            List of ColumnSpec object
+            generated sql to be executed or None if the tile job already exists
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
-        )
 
-        # check table exists
-        await self.check_table_exists(
-            db_session=db_session,
-            database_name=database_name,
-            schema_name=schema_name,
-            table_name=table_name,
-        )
+        logger.info(f"Scheduling {tile_type} tile job for {tile_spec.aggregation_id}")
+        job_id = f"{tile_type}_{tile_spec.aggregation_id}"
 
-        table_schema = await db_session.list_table_schema(
-            database_name=database_name, schema_name=schema_name, table_name=table_name
-        )
-        return [ColumnSpec(name=name, dtype=dtype) for name, dtype in table_schema.items()]
+        assert tile_spec.feature_store_id is not None
+        exist_job = await self.tile_scheduler_service.get_job_details(job_id=job_id)
+        if not exist_job:
+            logger.info(f"Creating new job {job_id}")
+            parameters = TileScheduledJobParameters(
+                feature_store_id=tile_spec.feature_store_id,
+                tile_id=tile_spec.tile_id,
+                time_modulo_frequency_second=tile_spec.time_modulo_frequency_second,
+                blind_spot_second=tile_spec.blind_spot_second,
+                frequency_minute=tile_spec.frequency_minute,
+                sql=tile_spec.tile_sql,
+                entity_column_names=tile_spec.entity_column_names,
+                value_column_names=tile_spec.value_column_names,
+                value_column_types=tile_spec.value_column_types,
+                tile_type=tile_type,
+                offline_period_minute=offline_minutes,
+                monitor_periods=monitor_periods,
+                aggregation_id=tile_spec.aggregation_id,
+            )
+            interval_seconds = (
+                tile_spec.frequency_minute * 60
+                if tile_type == TileType.ONLINE
+                else offline_minutes * 60
+            )
+            await self.tile_scheduler_service.start_job_with_interval(
+                job_id=job_id,
+                interval_seconds=interval_seconds,
+                time_modulo_frequency_second=tile_spec.time_modulo_frequency_second,
+                feature_store_id=tile_spec.feature_store_id,
+                parameters=parameters,
+            )
 
-    @staticmethod
-    def _summarize_logs(logs: pd.DataFrame, features: List[ExtendedFeatureModel]) -> pd.DataFrame:
+            return parameters.json()
+
+        return None
+
+    async def remove_tile_jobs(self, tile_spec: TileSpec) -> None:
         """
-        Summarize logs by session
+        Remove tiles
 
         Parameters
         ----------
-        logs: pd.DataFrame
-            Logs records
-        features: List[ExtendedFeatureModel]
-            List of features
-
-        Returns
-        -------
-        pd.DataFrame
+        tile_spec: TileSpec
+            the input TileSpec
         """
-
-        def _summarize_session(session_logs: pd.DataFrame) -> pd.DataFrame:
-            """
-            Compute session durations
-
-            Parameters
-            ----------
-            session_logs: pd.DataFrame
-                Log records in a single session
-
-            Returns
-            -------
-            pd.DataFrame
-            """
-            session_logs.index = session_logs["STATUS"]
-            timestamps = session_logs["CREATED_AT"].to_dict()
-
-            # extract timestamps for key steps
-            standard_statuses = ["STARTED", "MONITORED", "GENERATED", "COMPLETED"]
-            summarized_logs = pd.DataFrame(
-                {status: [timestamps.get(status, pd.NaT)] for status in standard_statuses}
-            )
-
-            # extract error message if any
-            summarized_logs["ERROR"] = None
-            error_logs = session_logs[~session_logs["STATUS"].isin(standard_statuses)]
-            if error_logs.shape[0] > 0:
-                summarized_logs["ERROR"] = error_logs["MESSAGE"].iloc[0]
-            return summarized_logs
-
-        tile_specs = []
-        tile_specs_cols = ["aggregation_id", "frequency_minute", "time_modulo_frequency_second"]
-        for feature in features:
-            if feature.tile_specs:
-                _tile_specs = pd.DataFrame.from_dict(
-                    [tile_spec.dict() for tile_spec in feature.tile_specs]
+        async for _ in self.feature_service.list_documents_as_dict_iterator(
+            query_filter={
+                "aggregation_ids": tile_spec.aggregation_id,
+                "online_enabled": True,
+            }
+        ):
+            break
+        else:
+            # Only disable the tile job if the aggregation_id is not referenced by any currently
+            # online enabled features
+            logger.info("Stopping job with custom scheduler")
+            for t_type in [TileType.ONLINE, TileType.OFFLINE]:
+                await self.tile_scheduler_service.stop_job(
+                    job_id=f"{t_type}_{tile_spec.aggregation_id}"
                 )
-                tile_specs.append(_tile_specs[tile_specs_cols])
-        feature_tile_specs = (
-            pd.concat(tile_specs).drop_duplicates()
-            if tile_specs
-            else pd.DataFrame(columns=tile_specs_cols)
-        )
-
-        # summarize logs by session
-        sessions = logs.groupby(["SESSION_ID", "AGGREGATION_ID"], group_keys=True).apply(
-            _summarize_session
-        )
-        output_columns = [
-            "SESSION_ID",
-            "AGGREGATION_ID",
-            "SCHEDULED",
-            "STARTED",
-            "COMPLETED",
-            "QUEUE_DURATION",
-            "COMPUTE_DURATION",
-            "TOTAL_DURATION",
-            "ERROR",
-        ]
-        if sessions.shape[0] > 0:
-            # exclude sessions that started before the range
-            sessions = sessions[~sessions["STARTED"].isnull()].reset_index()
-            sessions = sessions.merge(
-                feature_tile_specs, left_on="AGGREGATION_ID", right_on="aggregation_id"
-            )
-            if sessions.shape[0] > 0:
-                sessions["SCHEDULED"] = sessions.apply(
-                    lambda row: get_next_job_datetime(
-                        row.STARTED, row.frequency_minute, row.time_modulo_frequency_second
-                    ),
-                    axis=1,
-                ) - pd.to_timedelta(sessions.frequency_minute, unit="minute")
-                sessions["COMPUTE_DURATION"] = (
-                    sessions["COMPLETED"] - sessions["STARTED"]
-                ).dt.total_seconds()
-                sessions["QUEUE_DURATION"] = (
-                    sessions["STARTED"] - sessions["SCHEDULED"]
-                ).dt.total_seconds()
-                sessions["TOTAL_DURATION"] = (
-                    sessions["COMPLETED"] - sessions["SCHEDULED"]
-                ).dt.total_seconds()
-                return sessions[output_columns]
-        return pd.DataFrame(columns=output_columns)
 
-    async def get_feature_job_logs(
-        self,
-        feature_store_id: ObjectId,
-        features: List[ExtendedFeatureModel],
-        hour_limit: int,
-        get_credential: Any,
-    ) -> dict[str, Any]:
+    @staticmethod
+    async def retrieve_tile_job_audit_logs(
+        session: BaseSession,
+        start_date: datetime,
+        end_date: Optional[datetime] = None,
+        tile_id: Optional[str] = None,
+    ) -> pd.DataFrame:
         """
-        Retrieve data preview for query graph node
+        Retrieve audit logs of tile job executions
 
         Parameters
         ----------
-        feature_store_id: ObjectId
-            Feature List
-        features: List[ExtendedFeatureModel]
-            List of features
-        hour_limit: int
-            Limit in hours on the job history to fetch
-        get_credential: Any
-            Get credential handler function
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        start_date: datetime
+            start date of retrieval
+        end_date: Optional[datetime]
+            end date of retrieval, optional
+        tile_id: Optional[str]
+            targeted tile id of retrieval, optional
 
         Returns
         -------
-        dict[str, Any]
-            Dataframe converted to json string
+            dataframe of tile job execution information
         """
-        feature_store = await self.feature_store_service.get_document(feature_store_id)
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
-        )
-        utcnow = datetime.datetime.utcnow()
+        date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
+        start_date_str = start_date.strftime(date_format)
+        end_date_str = end_date.strftime(date_format) if end_date else ""
+        tile_id = tile_id if tile_id else ""
 
-        # compile list of aggregation_ids to filter logs
-        aggregation_ids = []
-        for feature in features:
-            for tile_spec in feature.tile_specs:
-                aggregation_ids.append(tile_spec.aggregation_id)
-
-        sql_expr = (
-            expressions.select(
-                quoted_identifier("SESSION_ID"),
-                quoted_identifier("CREATED_AT"),
-                quoted_identifier("AGGREGATION_ID"),
-                quoted_identifier("STATUS"),
-                quoted_identifier("MESSAGE"),
-            )
-            .from_("TILE_JOB_MONITOR")
-            .where(
-                expressions.And(
-                    expressions=[
-                        expressions.GTE(
-                            this=quoted_identifier("CREATED_AT"),
-                            expression=make_literal_value(
-                                utcnow - datetime.timedelta(hours=hour_limit),
-                                cast_as_timestamp=True,
-                            ),
-                        ),
-                        expressions.LT(
-                            this=quoted_identifier("CREATED_AT"),
-                            expression=make_literal_value(utcnow, cast_as_timestamp=True),
-                        ),
-                        expressions.In(
-                            this=quoted_identifier("AGGREGATION_ID"),
-                            expressions=[
-                                make_literal_value(aggregation_id)
-                                for aggregation_id in set(aggregation_ids)
-                            ],
-                        ),
-                        expressions.EQ(
-                            this=quoted_identifier("TILE_TYPE"),
-                            expression=make_literal_value("ONLINE"),
-                        ),
-                    ]
-                )
-            )
+        sql = tm_retrieve_tile_job_audit_logs.render(
+            start_date_str=start_date_str, end_date_str=end_date_str, tile_id=tile_id
         )
-        sql = sql_to_string(sql_expr, source_type=feature_store.type)
-        result = await db_session.execute_query(sql)
-        return dataframe_to_json(self._summarize_logs(result, features))
+        result = await session.execute_query(sql)
+        return result
```

### Comparing `featurebyte-0.3.1/featurebyte/service/historical_feature_table.py` & `featurebyte-0.4.0/featurebyte/service/historical_feature_table.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,59 +1,70 @@
 """
 HistoricalFeatureTableService class
 """
 from __future__ import annotations
 
-from typing import Optional
+from typing import Any, Optional
 
 from pathlib import Path
 
 import pandas as pd
 from bson import ObjectId
 
+from featurebyte.enum import MaterializedTableNamePrefix
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
+from featurebyte.persistent import Persistent
 from featurebyte.schema.historical_feature_table import HistoricalFeatureTableCreate
 from featurebyte.schema.worker.task.historical_feature_table import (
     HistoricalFeatureTableTaskPayload,
 )
+from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.materialized_table import BaseMaterializedTableService
 from featurebyte.storage import Storage
 
 
 class HistoricalFeatureTableService(
     BaseMaterializedTableService[HistoricalFeatureTableModel, HistoricalFeatureTableModel]
 ):
     """
     HistoricalFeatureTableService class
     """
 
+    def __init__(
+        self,
+        user: Any,
+        persistent: Persistent,
+        catalog_id: ObjectId,
+        feature_store_service: FeatureStoreService,
+        temp_storage: Storage,
+    ):
+        super().__init__(user, persistent, catalog_id, feature_store_service)
+        self.temp_storage = temp_storage
+
     document_class = HistoricalFeatureTableModel
-    materialized_table_name_prefix = "HISTORICAL_FEATURE_TABLE"
+    materialized_table_name_prefix = MaterializedTableNamePrefix.HISTORICAL_FEATURE_TABLE
 
     @property
     def class_name(self) -> str:
         return "HistoricalFeatureTable"
 
     async def get_historical_feature_table_task_payload(
         self,
         data: HistoricalFeatureTableCreate,
-        storage: Storage,
         observation_set_dataframe: Optional[pd.DataFrame],
     ) -> HistoricalFeatureTableTaskPayload:
         """
         Validate and convert a HistoricalFeatureTableCreate schema to a HistoricalFeatureTableTaskPayload schema
         which will be used to initiate the HistoricalFeatureTable creation task.
 
         Parameters
         ----------
         data: HistoricalFeatureTableCreate
             HistoricalFeatureTable creation payload
-        storage: Storage
-            Storage instance
         observation_set_dataframe: Optional[pd.DataFrame]
             Optional observation set DataFrame. If provided, the DataFrame will be stored in the
             temp storage to be used by the HistoricalFeatureTable creation task.
 
         Returns
         -------
         HistoricalFeatureTableTaskPayload
@@ -65,15 +76,15 @@
             document=FeatureByteBaseDocumentModel(_id=output_document_id, name=data.name),
         )
 
         if observation_set_dataframe is not None:
             observation_set_storage_path = (
                 f"historical_feature_table/observation_set/{output_document_id}.parquet"
             )
-            await storage.put_dataframe(
+            await self.temp_storage.put_dataframe(
                 observation_set_dataframe, Path(observation_set_storage_path)
             )
         else:
             observation_set_storage_path = None
 
         return HistoricalFeatureTableTaskPayload(
             **data.dict(),
```

### Comparing `featurebyte-0.3.1/featurebyte/service/item_table.py` & `featurebyte-0.4.0/featurebyte/service/item_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/materialized_table.py` & `featurebyte-0.4.0/featurebyte/service/materialized_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 BaseMaterializedTableService contains common functionality for materialized tables
 """
 from __future__ import annotations
 
-from typing import Any, List, Tuple
+from typing import Any, List, Optional, Tuple
 
 from bson import ObjectId
 
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.node.schema import ColumnSpec, TableDetails
 from featurebyte.query_graph.sql.common import sql_to_string
@@ -32,15 +32,15 @@
 
     materialized_table_name_prefix = ""
 
     def __init__(
         self,
         user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
+        catalog_id: Optional[ObjectId],
         feature_store_service: FeatureStoreService,
     ):
         super().__init__(user, persistent, catalog_id)
         self.feature_store_service = feature_store_service
 
     async def get_materialized_table_delete_task_payload(
         self, document_id: ObjectId
```

### Comparing `featurebyte-0.3.1/featurebyte/service/mixin.py` & `featurebyte-0.4.0/featurebyte/service/mixin.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,23 +15,22 @@
     PydanticObjectId,
 )
 
 GeneralT = TypeVar("GeneralT")
 Document = TypeVar("Document", bound=FeatureByteBaseDocumentModel)
 DocumentCreateSchema = TypeVar("DocumentCreateSchema", bound=FeatureByteBaseModel)
 SortDir = Literal["asc", "desc"]
+DEFAULT_PAGE_SIZE = 100
 
 
 class OpsServiceMixin:
     """
     OpsServiceMixin class contains common operation methods used across different type of services
     """
 
-    user: Any
-
     @staticmethod
     def include_object_id(
         document_ids: list[ObjectId] | list[PydanticObjectId], document_id: ObjectId
     ) -> list[ObjectId]:
         """
         Include document_id to the document_ids list
 
@@ -42,15 +41,15 @@
         document_id: ObjectId
             Document ID to be included
 
         Returns
         -------
         List of sorted document_ids
         """
-        return sorted(document_ids + [document_id])  # type: ignore
+        return sorted(set(document_ids + [document_id]))  # type: ignore
 
     @staticmethod
     def exclude_object_id(
         document_ids: list[ObjectId] | list[PydanticObjectId], document_id: ObjectId
     ) -> list[ObjectId]:
         """
         Exclude document_id from the document_ids list
@@ -62,15 +61,15 @@
         document_id: ObjectId
             Document ID to be excluded
 
         Returns
         -------
         List of sorted document_ids
         """
-        return sorted(ObjectId(doc_id) for doc_id in document_ids if doc_id != document_id)
+        return sorted(set(ObjectId(doc_id) for doc_id in document_ids if doc_id != document_id))
 
     @staticmethod
     def conditional_return(document: GeneralT, condition: bool) -> Optional[GeneralT]:
         """
         Return output only if condition is True
 
         Parameters
@@ -109,18 +108,18 @@
 
         Returns
         -------
         Document
         """
 
     @abstractmethod
-    async def list_documents(
+    async def list_documents_as_dict(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: str | None = "created_at",
         sort_dir: SortDir = "desc",
         **kwargs: Any,
     ) -> dict[str, Any]:
         """
         List documents stored at persistent (GitDB or MongoDB)
 
@@ -152,11 +151,11 @@
         name: str
             Name used to retrieve the document
 
         Returns
         -------
         SemanticModel
         """
-        documents = await self.list_documents(query_filter={"name": name})
+        documents = await self.list_documents_as_dict(query_filter={"name": name})
         if documents["data"]:
             return self.document_class(**documents["data"][0])
         return await self.create_document(data=self.document_create_class(name=name))
```

### Comparing `featurebyte-0.3.1/featurebyte/service/observation_table.py` & `featurebyte-0.4.0/featurebyte/service/observation_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 """
 ObservationTableService class
 """
 from __future__ import annotations
 
-from typing import Any, Dict, cast
+from typing import Any, Dict, Optional, cast
 
 import pandas as pd
 from bson import ObjectId
 
-from featurebyte.enum import DBVarType, SpecialColumnName
+from featurebyte.enum import DBVarType, MaterializedTableNamePrefix, SpecialColumnName
 from featurebyte.exception import (
     MissingPointInTimeColumnError,
     UnsupportedPointInTimeColumnTypeError,
 )
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.persistent import Persistent
@@ -30,21 +30,21 @@
     BaseMaterializedTableService[ObservationTableModel, ObservationTableModel]
 ):
     """
     ObservationTableService class
     """
 
     document_class = ObservationTableModel
-    materialized_table_name_prefix = "OBSERVATION_TABLE"
+    materialized_table_name_prefix = MaterializedTableNamePrefix.OBSERVATION_TABLE
 
     def __init__(
         self,
         user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
+        catalog_id: Optional[ObjectId],
         feature_store_service: FeatureStoreService,
         context_service: ContextService,
     ):
         super().__init__(user, persistent, catalog_id, feature_store_service)
         self.context_service = context_service
 
     @property
@@ -77,15 +77,15 @@
         if data.context_id is not None:
             # Check if the context document exists when provided. This should perform additional
             # validation once additional information such as request schema are available in the
             # context.
             await self.context_service.get_document(document_id=data.context_id)
 
         return ObservationTableTaskPayload(
-            **data.dict(),
+            **data.dict(by_alias=True),
             user_id=self.user.id,
             catalog_id=self.catalog_id,
             output_document_id=output_document_id,
         )
 
     @staticmethod
     async def get_most_recent_point_in_time(
```

### Comparing `featurebyte-0.3.1/featurebyte/service/online_enable.py` & `featurebyte-0.4.0/featurebyte/service/feature_store_warehouse.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,248 +1,284 @@
 """
-OnlineEnableService class
+Service for interacting with the data warehouse for queries around the feature store.
+
+We split this into a separate service, as these typically require a session object that is created.
 """
 from __future__ import annotations
 
-from typing import Any, Optional
-
-from bson.objectid import ObjectId
-from pydantic import PrivateAttr
+from typing import Any, List
 
-from featurebyte.feature_manager.manager import FeatureManager
-from featurebyte.feature_manager.model import ExtendedFeatureModel
-from featurebyte.models.feature import FeatureModel, FeatureNamespaceModel
-from featurebyte.models.feature_list import FeatureListModel
-from featurebyte.models.online_store import OnlineFeatureSpec
-from featurebyte.persistent import Persistent
-from featurebyte.schema.feature import FeatureServiceUpdate
-from featurebyte.schema.feature_list import FeatureListServiceUpdate
-from featurebyte.schema.feature_namespace import FeatureNamespaceServiceUpdate
-from featurebyte.service.base_service import BaseService
-from featurebyte.service.feature import FeatureService
-from featurebyte.service.feature_list import FeatureListService
-from featurebyte.service.feature_namespace import FeatureNamespaceService
+from featurebyte.exception import DatabaseNotFoundError, SchemaNotFoundError, TableNotFoundError
+from featurebyte.models.feature_store import FeatureStoreModel
+from featurebyte.models.user_defined_function import UserDefinedFunctionModel
+from featurebyte.query_graph.node.schema import ColumnSpec
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.session_manager import SessionManagerService
-from featurebyte.service.task_manager import TaskManager
 from featurebyte.session.base import BaseSession
 
 
-class OnlineEnableService(BaseService):
+class FeatureStoreWarehouseService:
     """
-    OnlineEnableService class is responsible for maintaining the feature & feature list structure
-    of feature online enablement.
+    FeatureStoreWarehouseService is responsible for interacting with the data warehouse.
     """
 
-    _task_manager: TaskManager = PrivateAttr()
-
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         session_manager_service: SessionManagerService,
+        feature_store_service: FeatureStoreService,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_service = FeatureService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
         self.session_manager_service = session_manager_service
-        self.feature_store_service = FeatureStoreService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.feature_namespace_service = FeatureNamespaceService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.feature_list_service = FeatureListService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self._task_manager = TaskManager(user=user, persistent=persistent, catalog_id=catalog_id)
+        self.feature_store_service = feature_store_service
 
     @classmethod
-    def _extract_online_enabled_feature_ids(
-        cls, feature: FeatureModel, document: FeatureListModel | FeatureNamespaceModel
-    ) -> list[ObjectId]:
-        if feature.online_enabled:
-            return cls.include_object_id(document.online_enabled_feature_ids, feature.id)
-        return cls.exclude_object_id(document.online_enabled_feature_ids, feature.id)
+    async def check_database_exists(
+        cls,
+        db_session: BaseSession,
+        database_name: str,
+    ) -> None:
+        """
+        Check tables in feature store
 
-    async def _update_feature_list(
-        self,
-        feature_list_id: ObjectId,
-        feature: FeatureModel,
-        return_document: bool = True,
-    ) -> Optional[FeatureListModel]:
+        Parameters
+        ----------
+        db_session: BaseSession
+            BaseSession object
+        database_name: str
+            Name of database to use
+
+        Raises
+        ------
+        DatabaseNotFoundError
+            If database does not exist
+        """
+        databases = await db_session.list_databases()
+        databases = [database.lower() for database in databases]
+        if database_name.lower() not in databases:
+            raise DatabaseNotFoundError(f"Database {database_name} not found.")
+
+    @classmethod
+    async def check_schema_exists(
+        cls,
+        db_session: BaseSession,
+        database_name: str,
+        schema_name: str,
+    ) -> None:
         """
-        Update online_enabled_feature_ids in feature list
+        Check tables in feature store
 
         Parameters
         ----------
-        feature_list_id: ObjectId
-            Target feature list ID
-        feature: FeatureModel
-            Updated Feature object
-        return_document: bool
-            Whether to return updated document
+        db_session: BaseSession
+            BaseSession object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
+
+        Raises
+        ------
+        SchemaNotFoundError
+            If schema does not exist
+        """
+        await cls.check_database_exists(db_session=db_session, database_name=database_name)
+        schemas = await db_session.list_schemas(database_name=database_name)
+        schemas = [schema.lower() for schema in schemas]
+        if schema_name.lower() not in schemas:
+            raise SchemaNotFoundError(f"Schema {schema_name} not found.")
 
-        Returns
-        -------
-        Optional[FeatureListModel]
+    @classmethod
+    async def check_table_exists(
+        cls,
+        db_session: BaseSession,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+    ) -> None:
         """
-        document = await self.feature_list_service.get_document(document_id=feature_list_id)
-        return await self.feature_list_service.update_document(
-            document_id=feature_list_id,
-            data=FeatureListServiceUpdate(
-                online_enabled_feature_ids=self._extract_online_enabled_feature_ids(
-                    feature=feature, document=document
-                ),
-            ),
-            document=document,
-            return_document=return_document,
+        Check tables in feature store
+
+        Parameters
+        ----------
+        db_session: BaseSession
+            BaseSession object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
+        table_name: str
+            Name of table to use
+
+        Raises
+        ------
+        TableNotFoundError
+            If table does not exist
+        """
+        await cls.check_schema_exists(
+            db_session=db_session, database_name=database_name, schema_name=schema_name
         )
+        tables = await db_session.list_tables(database_name=database_name, schema_name=schema_name)
+        tables = [table.lower() for table in tables]
+        if table_name.lower() not in tables:
+            raise TableNotFoundError(f"Table {table_name} not found.")
 
-    async def _update_feature_namespace(
+    async def check_user_defined_function_exists(
         self,
-        feature_namespace_id: ObjectId,
-        feature: FeatureModel,
-        document: Optional[FeatureNamespaceModel] = None,
-        return_document: bool = True,
-    ) -> Optional[FeatureNamespaceModel]:
-        """
-        Update online_enabled_feature_ids in feature namespace
+        user_defined_function: UserDefinedFunctionModel,
+        feature_store: FeatureStoreModel,
+        get_credential: Any,
+    ) -> None:
+        """
+        Check whether user defined function in feature store
+
+        Parameters
+        ----------
+        user_defined_function: UserDefinedFunctionModel
+            User defined function model
+        feature_store: FeatureStoreModel
+            Feature store model
+        get_credential: Any
+            Get credential handler function
+        """
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
+        )
+        await db_session.check_user_defined_function(user_defined_function=user_defined_function)
+
+    async def list_databases(
+        self, feature_store: FeatureStoreModel, get_credential: Any
+    ) -> List[str]:
+        """
+        List databases in feature store
 
         Parameters
         ----------
-        feature_namespace_id: ObjectId
-            FeatureNamespace ID
-        feature: FeatureModel
-            Updated Feature object
-        document: Optional[FeatureNamespaceModel]
-            Document to be updated (when provided, this method won't query persistent for retrieval)
-        return_document: bool
-            Whether to return updated document
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        get_credential: Any
+            Get credential handler function
 
         Returns
         -------
-        Optional[FeatureNamespaceModel]
+        List[str]
+            List of database names
         """
-        document = await self.feature_namespace_service.get_document(
-            document_id=feature_namespace_id
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
         )
-        return await self.feature_namespace_service.update_document(
-            document_id=feature_namespace_id,
-            data=FeatureNamespaceServiceUpdate(
-                online_enabled_feature_ids=self._extract_online_enabled_feature_ids(
-                    feature=feature, document=document
-                ),
-            ),
-            document=document,
-            return_document=return_document,
-        )
-
-    @staticmethod
-    async def update_data_warehouse_with_session(
-        session: BaseSession,
-        feature: FeatureModel,
-        task_manager: Optional[TaskManager] = None,
-    ) -> None:
+        return await db_session.list_databases()
+
+    async def list_schemas(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        get_credential: Any,
+    ) -> List[str]:
         """
-        Update data warehouse registry upon changes to online enable status, such as enabling or
-        disabling scheduled tile and feature jobs
+        List schemas in feature store
 
         Parameters
         ----------
-        session: BaseSession
-            Session object
-        feature: FeatureModel
-            Updated Feature object
-        task_manager: Optional[TaskManager]
-            TaskManager object
-        """
-        extended_feature_model = ExtendedFeatureModel(**feature.dict())
-        online_feature_spec = OnlineFeatureSpec(feature=extended_feature_model)
-
-        if not online_feature_spec.is_online_store_eligible:
-            return
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        get_credential: Any
+            Get credential handler function
 
-        feature_manager = FeatureManager(session=session, task_manager=task_manager)
+        Returns
+        -------
+        List[str]
+            List of schema names
+        """
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
+        )
+        # check database exists
+        await self.check_database_exists(db_session=db_session, database_name=database_name)
 
-        if feature.online_enabled:
-            await feature_manager.online_enable(online_feature_spec)
-        else:
-            await feature_manager.online_disable(online_feature_spec)
+        return await db_session.list_schemas(database_name=database_name)
 
-    async def update_data_warehouse(
-        self, updated_feature: FeatureModel, online_enabled_before_update: bool, get_credential: Any
-    ) -> None:
+    async def list_tables(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        schema_name: str,
+        get_credential: Any,
+    ) -> List[str]:
         """
-        Update data warehouse registry upon changes to online enable status, such as enabling or
-        disabling scheduled tile and feature jobs
+        List tables in feature store
 
         Parameters
         ----------
-        updated_feature: FeatureModel
-            Updated Feature
-        online_enabled_before_update: bool
-            Online enabled status
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
         get_credential: Any
             Get credential handler function
+
+        Returns
+        -------
+        List[str]
+            List of table names
         """
-        if updated_feature.online_enabled == online_enabled_before_update:
-            # updated_feature has the same online_enabled status as the original one
-            # no need to update
-            return
 
-        feature_store_model = await self.feature_store_service.get_document(
-            document_id=updated_feature.tabular_source.feature_store_id
-        )
-        session = await self.session_manager_service.get_feature_store_session(
-            feature_store_model, get_credential
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
         )
-        await self.update_data_warehouse_with_session(
-            session=session, feature=updated_feature, task_manager=self._task_manager
+
+        # check schema exists
+        await self.check_schema_exists(
+            db_session=db_session, database_name=database_name, schema_name=schema_name
         )
 
-    async def update_feature(
+        tables = await db_session.list_tables(database_name=database_name, schema_name=schema_name)
+        # exclude tables with names that has a "__" prefix
+        return [table_name for table_name in tables if not table_name.startswith("__")]
+
+    async def list_columns(
         self,
-        feature_id: ObjectId,
-        online_enabled: bool,
-    ) -> FeatureModel:
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+        get_credential: Any,
+    ) -> List[ColumnSpec]:
         """
-        Update feature online enabled & trigger list of cascading updates
+        List columns in database table
 
         Parameters
         ----------
-        feature_id: ObjectId
-            Target feature ID
-        online_enabled: bool
-            Value to update the feature online_enabled status
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
+        table_name: str
+            Name of table to use
+        get_credential: Any
+            Get credential handler function
 
         Returns
         -------
-        FeatureModel
+        List[ColumnSpec]
+            List of ColumnSpec object
         """
-        document = await self.feature_service.get_document(document_id=feature_id)
-        if document.online_enabled != online_enabled:
-            async with self.persistent.start_transaction():
-                feature = await self.feature_service.update_document(
-                    document_id=feature_id,
-                    data=FeatureServiceUpdate(online_enabled=online_enabled),
-                    document=document,
-                    return_document=True,
-                )
-                assert isinstance(feature, FeatureModel)
-                await self._update_feature_namespace(
-                    feature_namespace_id=feature.feature_namespace_id,
-                    feature=feature,
-                    return_document=False,
-                )
-                for feature_list_id in feature.feature_list_ids:
-                    await self._update_feature_list(
-                        feature_list_id=feature_list_id,
-                        feature=feature,
-                        return_document=False,
-                    )
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
+        )
 
-                return await self.feature_service.get_document(document_id=feature_id)
+        # check table exists
+        await self.check_table_exists(
+            db_session=db_session,
+            database_name=database_name,
+            schema_name=schema_name,
+            table_name=table_name,
+        )
 
-        return document
+        table_schema = await db_session.list_table_schema(
+            database_name=database_name, schema_name=schema_name, table_name=table_name
+        )
+        return [ColumnSpec(name=name, dtype=dtype) for name, dtype in table_schema.items()]
```

### Comparing `featurebyte-0.3.1/featurebyte/service/online_serving.py` & `featurebyte-0.4.0/featurebyte/service/online_serving.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,48 +2,43 @@
 OnlineServingService class
 """
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Union
 
 import pandas as pd
-from bson import ObjectId
 
 from featurebyte.exception import FeatureListNotOnlineEnabledError
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.feature_list import FeatureListModel
-from featurebyte.persistent import Persistent
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.online_serving import get_online_features
 from featurebyte.schema.deployment import OnlineFeaturesResponseModel
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.entity_validation import EntityValidationService
 from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
 from featurebyte.service.session_manager import SessionManagerService
 
 
-class OnlineServingService(BaseService):
+class OnlineServingService:
     """
     OnlineServingService is responsible for retrieving features from online store
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         session_manager_service: SessionManagerService,
         entity_validation_service: EntityValidationService,
+        online_store_table_version_service: OnlineStoreTableVersionService,
+        feature_store_service: FeatureStoreService,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_store_service = FeatureStoreService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+        self.feature_store_service = feature_store_service
         self.session_manager_service = session_manager_service
         self.entity_validation_service = entity_validation_service
+        self.online_store_table_version_service = online_store_table_version_service
 
     async def get_online_features_from_feature_list(
         self,
         feature_list: FeatureListModel,
         request_data: Union[List[Dict[str, Any]], BatchRequestTableModel],
         get_credential: Any,
         output_table_details: Optional[TableDetails] = None,
@@ -109,11 +104,12 @@
             session=db_session,
             graph=feature_cluster.graph,
             nodes=feature_cluster.nodes,
             request_data=request_input,
             source_type=feature_store.type,
             parent_serving_preparation=parent_serving_preparation,
             output_table_details=output_table_details,
+            online_store_table_version_service=self.online_store_table_version_service,
         )
         if features is None:
             return None
         return OnlineFeaturesResponseModel(features=features)
```

### Comparing `featurebyte-0.3.1/featurebyte/service/parent_serving.py` & `featurebyte-0.4.0/featurebyte/service/parent_serving.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,43 +1,37 @@
 """
 Module to support serving parent features using child entities
 """
 from __future__ import annotations
 
-from typing import Any, List, Tuple
+from typing import List, Tuple
 
 from collections import OrderedDict
 
 from bson import ObjectId
 
 from featurebyte.exception import AmbiguousEntityRelationshipError, EntityJoinPathNotFoundError
 from featurebyte.models.entity import EntityModel
 from featurebyte.models.entity_validation import EntityInfo
 from featurebyte.models.parent_serving import JoinStep
-from featurebyte.persistent import Persistent
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.table import TableService
 
 
-class ParentEntityLookupService(BaseService):
+class ParentEntityLookupService:
     """
     ParentEntityLookupService is responsible for identifying the joins required to lookup parent
     entities in order to serve parent features given child entities
     """
 
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
         entity_service: EntityService,
         table_service: TableService,
     ):
-        super().__init__(user, persistent, catalog_id)
         self.entity_service = entity_service
         self.table_service = table_service
 
     async def get_required_join_steps(self, entity_info: EntityInfo) -> list[JoinStep]:
         """
         Get the list of required JoinStep to lookup the missing entities in the request
```

### Comparing `featurebyte-0.3.1/featurebyte/service/preview.py` & `featurebyte-0.4.0/featurebyte/routes/feature/controller.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,682 +1,591 @@
 """
-PreviewService class
+Feature API route controller
 """
 from __future__ import annotations
 
-from typing import Any, AsyncGenerator, Callable, Dict, Optional, Tuple, Union
+from typing import Any, Dict, Literal, Optional, Union
 
-import os
+from http import HTTPStatus
+from pprint import pformat
 
-import pandas as pd
-from bson import ObjectId
+from bson.objectid import ObjectId
+from fastapi.exceptions import HTTPException
 
-from featurebyte.common.utils import dataframe_to_json
-from featurebyte.enum import SpecialColumnName
 from featurebyte.exception import (
-    DocumentNotFoundError,
-    LimitExceededError,
+    DocumentDeletionError,
     MissingPointInTimeColumnError,
+    RequiredEntityNotProvidedError,
 )
-from featurebyte.logging import get_logger
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.observation_table import ObservationTableModel
-from featurebyte.persistent import Persistent
-from featurebyte.query_graph.graph import QueryGraph
-from featurebyte.query_graph.model.common_table import TabularSource
-from featurebyte.query_graph.node.schema import TableDetails
-from featurebyte.query_graph.sql.common import REQUEST_TABLE_NAME, sql_to_string
-from featurebyte.query_graph.sql.feature_historical import (
-    get_historical_features,
-    get_historical_features_expr,
+from featurebyte.feature_manager.model import ExtendedFeatureModel
+from featurebyte.models.base import VersionIdentifier
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
+from featurebyte.query_graph.enum import GraphNodeType
+from featurebyte.query_graph.model.feature_job_setting import (
+    FeatureJobSetting,
+    TableFeatureJobSetting,
 )
-from featurebyte.query_graph.sql.feature_preview import get_feature_preview_sql
-from featurebyte.query_graph.sql.interpreter import GraphInterpreter
-from featurebyte.query_graph.sql.materialisation import get_source_count_expr, get_source_expr
-from featurebyte.schema.feature import FeaturePreview, FeatureSQL
-from featurebyte.schema.feature_list import (
-    FeatureListGetHistoricalFeatures,
-    FeatureListPreview,
-    FeatureListSQL,
+from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
+from featurebyte.routes.common.base import BaseDocumentController, DerivePrimaryEntityHelper
+from featurebyte.routes.common.feature_metadata_extractor import FeatureOrTargetMetadataExtractor
+from featurebyte.routes.feature_namespace.controller import FeatureNamespaceController
+from featurebyte.routes.task.controller import TaskController
+from featurebyte.schema.feature import (
+    BatchFeatureCreate,
+    FeatureBriefInfoList,
+    FeatureCreate,
+    FeatureModelResponse,
+    FeatureNewVersionCreate,
+    FeaturePaginatedList,
+    FeatureServiceCreate,
+    FeatureSQL,
+    FeatureUpdate,
 )
-from featurebyte.schema.feature_store import (
-    FeatureStorePreview,
-    FeatureStoreSample,
-    FeatureStoreShape,
-)
-from featurebyte.service.base_service import BaseService
-from featurebyte.service.entity_validation import EntityValidationService
+from featurebyte.schema.info import FeatureInfo
+from featurebyte.schema.preview import FeatureOrTargetPreview
+from featurebyte.schema.task import Task
+from featurebyte.schema.worker.task.batch_feature_create import BatchFeatureCreateTaskPayload
+from featurebyte.service.catalog import CatalogService
+from featurebyte.service.entity import EntityService
+from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
-from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.session_manager import SessionManagerService
-from featurebyte.session.base import BaseSession
+from featurebyte.service.feature_namespace import FeatureNamespaceService
+from featurebyte.service.feature_preview import FeaturePreviewService
+from featurebyte.service.feature_readiness import FeatureReadinessService
+from featurebyte.service.table import TableService
+from featurebyte.service.tile_job_log import TileJobLogService
+from featurebyte.service.version import VersionService
+
+
+def _extract_feature_table_cleaning_operations(
+    feature: FeatureModel, table_id_to_name: dict[ObjectId, str]
+) -> list[TableCleaningOperation]:
+    """
+    Helper method to extract table cleaning operations from a feature model.
+
+    Parameters
+    ----------
+    feature: FeatureModel
+        Feature model
+    table_id_to_name: dict[ObjectId, str]
+        Table ID to table name mapping
+
+    Returns
+    -------
+    list[TableCleaningOperation]
+    """
+    table_cleaning_operations: list[TableCleaningOperation] = []
+    for view_graph_node in feature.graph.iterate_sorted_graph_nodes(
+        graph_node_types=GraphNodeType.view_graph_node_types()
+    ):
+        view_metadata = view_graph_node.parameters.metadata  # type: ignore
+        if view_metadata.column_cleaning_operations:
+            table_cleaning_operations.append(
+                TableCleaningOperation(
+                    table_name=table_id_to_name[view_metadata.table_id],
+                    column_cleaning_operations=view_metadata.column_cleaning_operations,
+                )
+            )
+    return table_cleaning_operations
 
-# This time is used as an arbitrary value to use in scenarios where we don't have any time provided in previews.
-ARBITRARY_TIME = pd.Timestamp(1970, 1, 1, 12)
-MAX_TABLE_CELLS = int(
-    os.environ.get("MAX_TABLE_CELLS", 10000000 * 300)
-)  # 10 million rows, 300 columns
 
+def _extract_table_feature_job_settings(
+    feature: FeatureModel, table_id_to_name: dict[ObjectId, str]
+) -> list[TableFeatureJobSetting]:
+    """
+    Helper method to extract table feature job settings from a feature model.
 
-logger = get_logger(__name__)
+    Parameters
+    ----------
+    feature: FeatureModel
+        Feature model
+    table_id_to_name: dict[ObjectId, str]
+        Table ID to table name mapping
+
+    Returns
+    -------
+    list[TableFeatureJobSetting]
+    """
+    table_feature_job_settings = []
+    for group_by_node, data_id in feature.graph.iterate_group_by_node_and_table_id_pairs(
+        target_node=feature.node
+    ):
+        assert data_id is not None, "Event table ID not found"
+        table_name = table_id_to_name[data_id]
+        group_by_node_params = group_by_node.parameters
+        table_feature_job_settings.append(
+            TableFeatureJobSetting(
+                table_name=table_name,
+                feature_job_setting=FeatureJobSetting(
+                    blind_spot=f"{group_by_node_params.blind_spot}s",
+                    frequency=f"{group_by_node_params.frequency}s",
+                    time_modulo_frequency=f"{group_by_node_params.time_modulo_frequency}s",
+                ),
+            )
+        )
+    return table_feature_job_settings
 
 
-class PreviewService(BaseService):
+# pylint: disable=too-many-instance-attributes
+class FeatureController(
+    BaseDocumentController[FeatureModelResponse, FeatureService, FeaturePaginatedList]
+):
     """
-    PreviewService class
+    Feature controller
     """
 
+    paginated_document_class = FeaturePaginatedList
+
     def __init__(
         self,
-        user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
-        session_manager_service: SessionManagerService,
+        feature_service: FeatureService,
+        feature_namespace_service: FeatureNamespaceService,
+        entity_service: EntityService,
         feature_list_service: FeatureListService,
-        entity_validation_service: EntityValidationService,
+        feature_readiness_service: FeatureReadinessService,
+        feature_preview_service: FeaturePreviewService,
+        version_service: VersionService,
+        task_controller: TaskController,
+        catalog_service: CatalogService,
+        table_service: TableService,
+        feature_namespace_controller: FeatureNamespaceController,
+        derive_primary_entity_helper: DerivePrimaryEntityHelper,
+        feature_or_target_metadata_extractor: FeatureOrTargetMetadataExtractor,
+        tile_job_log_service: TileJobLogService,
     ):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_store_service = FeatureStoreService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.session_manager_service = session_manager_service
+        # pylint: disable=too-many-arguments
+        super().__init__(feature_service)
+        self.feature_namespace_service = feature_namespace_service
+        self.entity_service = entity_service
         self.feature_list_service = feature_list_service
-        self.entity_validation_service = entity_validation_service
+        self.feature_readiness_service = feature_readiness_service
+        self.feature_preview_service = feature_preview_service
+        self.version_service = version_service
+        self.task_controller = task_controller
+        self.catalog_service = catalog_service
+        self.table_service = table_service
+        self.feature_namespace_controller = feature_namespace_controller
+        self.derive_primary_entity_helper = derive_primary_entity_helper
+        self.feature_or_target_metadata_extractor = feature_or_target_metadata_extractor
+        self.tile_job_log_service = tile_job_log_service
 
-    async def _get_feature_store_session(
-        self, graph: QueryGraph, node_name: str, feature_store_name: str, get_credential: Any
-    ) -> Tuple[FeatureStoreModel, BaseSession]:
+    async def submit_batch_feature_create_task(self, data: BatchFeatureCreate) -> Optional[Task]:
         """
-        Get feature store and session from a graph
+        Submit Feature Create Task
 
         Parameters
         ----------
-        graph: QueryGraph
-            Query graph to use
-        node_name: str
-            Name of node to use
-        feature_store_name: str
-            Name of feature store
-        get_credential: Any
-            Get credential handler function
+        data: BatchFeatureCreate
+            Batch Feature creation payload
 
         Returns
         -------
-        Tuple[FeatureStoreModel, BaseSession]
+        Optional[Task]
+            Task object
         """
-        feature_store_dict = graph.get_input_node(node_name).parameters.feature_store_details.dict()
-        feature_store = FeatureStoreModel(**feature_store_dict, name=feature_store_name)
-        session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store,
-            get_credential=get_credential,
+        # as there is no direct way to get the conflict resolved feature id for batch feature creation task,
+        # the conflict resolution should only support "raise" for public API. Therefore, we should not include
+        # the conflict resolution in the API payload schema (BatchFeatureCreate).
+        payload = BatchFeatureCreateTaskPayload(
+            **{
+                **data.dict(by_alias=True),
+                "conflict_resolution": "raise",
+                "user_id": self.service.user.id,
+                "catalog_id": self.service.catalog_id,
+            }
         )
-        return feature_store, session
+        task_id = await self.task_controller.task_manager.submit(payload=payload)
+        return await self.task_controller.task_manager.get_task(task_id=str(task_id))
 
-    async def shape(self, preview: FeatureStorePreview, get_credential: Any) -> FeatureStoreShape:
+    async def create_feature(
+        self, data: Union[FeatureCreate, FeatureNewVersionCreate]
+    ) -> FeatureModelResponse:
         """
-        Get the shape of a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+        Create Feature at persistent (GitDB or MongoDB)
 
         Parameters
         ----------
-        preview: FeatureStorePreview
-            FeatureStorePreview object
-        get_credential: Any
-            Get credential handler function
+        data: FeatureCreate | FeatureNewVersionCreate
+            Feature creation payload
 
         Returns
         -------
-        FeatureStoreShape
-            Row and column counts
+        FeatureModelResponse
+            Newly created feature object
         """
-        feature_store, session = await self._get_feature_store_session(
-            graph=preview.graph,
-            node_name=preview.node_name,
-            feature_store_name=preview.feature_store_name,
-            get_credential=get_credential,
-        )
-        shape_sql, num_cols = GraphInterpreter(
-            preview.graph, source_type=feature_store.type
-        ).construct_shape_sql(node_name=preview.node_name)
-        logger.debug("Execute shape SQL", extra={"shape_sql": shape_sql})
-        result = await session.execute_query(shape_sql)
-        assert result is not None
-        return FeatureStoreShape(
-            num_rows=result["count"].iloc[0],
-            num_cols=num_cols,
+        if isinstance(data, FeatureCreate):
+            document = await self.service.create_document(
+                data=FeatureServiceCreate(**data.dict(by_alias=True))
+            )
+        else:
+            document = await self.version_service.create_new_feature_version(data=data)
+
+        # update feature namespace readiness due to introduction of new feature
+        await self.feature_readiness_service.update_feature_namespace(
+            feature_namespace_id=document.feature_namespace_id,
+            return_document=False,
+        )
+        return await self.get(document_id=document.id)
+
+    async def get(
+        self, document_id: ObjectId, exception_detail: str | None = None
+    ) -> FeatureModelResponse:
+        document = await self.service.get_document(
+            document_id=document_id,
+            exception_detail=exception_detail,
+        )
+        namespace = await self.feature_namespace_service.get_document(
+            document_id=document.feature_namespace_id
+        )
+        output = FeatureModelResponse(
+            **document.dict(by_alias=True),
+            is_default=namespace.default_feature_id == document.id,
+            primary_entity_ids=await self.derive_primary_entity_helper.derive_primary_entity_ids(
+                entity_ids=document.entity_ids
+            ),
         )
+        return output
 
-    async def preview(
-        self, preview: FeatureStorePreview, limit: int, get_credential: Any
-    ) -> dict[str, Any]:
+    async def update_feature(
+        self,
+        feature_id: ObjectId,
+        data: FeatureUpdate,
+    ) -> FeatureModel:
         """
-        Preview a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+        Update Feature at persistent
 
         Parameters
         ----------
-        preview: FeatureStorePreview
-            FeatureStorePreview object
-        limit: int
-            Row limit on preview results
-        get_credential: Any
-            Get credential handler function
+        feature_id: ObjectId
+            Feature ID
+        data: FeatureUpdate
+            Feature update payload
 
         Returns
         -------
-        dict[str, Any]
-            Dataframe converted to json string
+        FeatureModel
+            Feature object with updated attribute(s)
         """
-        feature_store, session = await self._get_feature_store_session(
-            graph=preview.graph,
-            node_name=preview.node_name,
-            feature_store_name=preview.feature_store_name,
-            get_credential=get_credential,
-        )
-        preview_sql, type_conversions = GraphInterpreter(
-            preview.graph, source_type=feature_store.type
-        ).construct_preview_sql(node_name=preview.node_name, num_rows=limit)
-        result = await session.execute_query(preview_sql)
-        return dataframe_to_json(result, type_conversions)
+        if data.readiness:
+            await self.feature_readiness_service.update_feature(
+                feature_id=feature_id,
+                readiness=FeatureReadiness(data.readiness),
+                ignore_guardrails=bool(data.ignore_guardrails),
+                return_document=False,
+            )
+        return await self.get(document_id=feature_id)
 
-    async def sample(
-        self, sample: FeatureStoreSample, size: int, seed: int, get_credential: Any
-    ) -> dict[str, Any]:
+    async def delete_feature(self, feature_id: ObjectId) -> None:
         """
-        Sample a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+        Delete Feature at persistent
 
         Parameters
         ----------
-        sample: FeatureStoreSample
-            FeatureStoreSample object
-        size: int
-            Maximum rows to sample
-        seed: int
-            Random seed to use for sampling
-        get_credential: Any
-            Get credential handler function
+        feature_id: ObjectId
+            Feature ID
 
-        Returns
-        -------
-        dict[str, Any]
-            Dataframe converted to json string
-        """
-        feature_store, session = await self._get_feature_store_session(
-            graph=sample.graph,
-            node_name=sample.node_name,
-            feature_store_name=sample.feature_store_name,
-            get_credential=get_credential,
-        )
-        sample_sql, type_conversions = GraphInterpreter(
-            sample.graph, source_type=feature_store.type
-        ).construct_sample_sql(
-            node_name=sample.node_name,
-            num_rows=size,
-            seed=seed,
-            from_timestamp=sample.from_timestamp,
-            to_timestamp=sample.to_timestamp,
-            timestamp_column=sample.timestamp_column,
-        )
-        result = await session.execute_query(sample_sql)
-        return dataframe_to_json(result, type_conversions)
+        Raises
+        ------
+        DocumentDeletionError
+            * If the feature is not in draft readiness
+            * If the feature is the default feature and the default version mode is manual
+            * If the feature is in any saved feature list
+        """
+        feature = await self.service.get_document(document_id=feature_id)
+        feature_namespace = await self.feature_namespace_service.get_document(
+            document_id=feature.feature_namespace_id
+        )
+
+        if feature.readiness != FeatureReadiness.DRAFT:
+            raise DocumentDeletionError("Only feature with draft readiness can be deleted.")
+
+        if (
+            feature_namespace.default_feature_id == feature_id
+            and feature_namespace.default_version_mode == DefaultVersionMode.MANUAL
+        ):
+            raise DocumentDeletionError(
+                "Feature is the default feature of the feature namespace and the default version mode is manual. "
+                "Please set another feature as the default feature or change the default version mode to auto."
+            )
 
-    async def describe(
-        self, sample: FeatureStoreSample, size: int, seed: int, get_credential: Any
-    ) -> dict[str, Any]:
-        """
-        Sample a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+        if feature.feature_list_ids:
+            feature_list_info = []
+            async for feature_list in self.feature_list_service.list_documents_as_dict_iterator(
+                query_filter={"_id": {"$in": feature.feature_list_ids}}
+            ):
+                feature_list_info.append(
+                    {
+                        "id": str(feature_list["_id"]),
+                        "name": feature_list["name"],
+                        "version": VersionIdentifier(**feature_list["version"]).to_str(),
+                    }
+                )
 
-        Parameters
-        ----------
-        sample: FeatureStoreSample
-            FeatureStoreSample object
-        size: int
-            Maximum rows to sample
-        seed: int
-            Random seed to use for sampling
-        get_credential: Any
-            Get credential handler function
+            raise DocumentDeletionError(
+                f"Feature is still in use by feature list(s). Please remove the following feature list(s) first:\n"
+                f"{pformat(feature_list_info)}"
+            )
 
-        Returns
-        -------
-        dict[str, Any]
-            Dataframe converted to json string
-        """
-        feature_store, session = await self._get_feature_store_session(
-            graph=sample.graph,
-            node_name=sample.node_name,
-            feature_store_name=sample.feature_store_name,
-            get_credential=get_credential,
-        )
-
-        describe_sql, type_conversions, row_names, columns = GraphInterpreter(
-            sample.graph, source_type=feature_store.type
-        ).construct_describe_sql(
-            node_name=sample.node_name,
-            num_rows=size,
-            seed=seed,
-            from_timestamp=sample.from_timestamp,
-            to_timestamp=sample.to_timestamp,
-            timestamp_column=sample.timestamp_column,
-        )
-        logger.debug("Execute describe SQL", extra={"describe_sql": describe_sql})
-        result = await session.execute_query(describe_sql)
-        assert result is not None
-        results = pd.DataFrame(
-            result.values.reshape(len(columns), -1).T,
-            index=row_names,
-            columns=[str(column.name) for column in columns],
-        ).dropna(axis=0, how="all")
-        return dataframe_to_json(results, type_conversions, skip_prepare=True)
-
-    @staticmethod
-    def _update_point_in_time_if_needed(
-        point_in_time_and_serving_name_list: list[Dict[str, Any]], is_time_based: bool
-    ) -> Tuple[list[Dict[str, Any]], bool]:
-        """
-        Helper method to update point in time if needed.
+        # use transaction to ensure atomicity
+        async with self.service.persistent.start_transaction():
+            # delete feature from the persistent
+            await self.service.delete_document(document_id=feature_id)
+            await self.feature_readiness_service.update_feature_namespace(
+                feature_namespace_id=feature.feature_namespace_id,
+                deleted_feature_ids=[feature_id],
+                return_document=False,
+            )
+            feature_namespace = await self.feature_namespace_service.get_document(
+                document_id=feature.feature_namespace_id
+            )
+            if not feature_namespace.feature_ids:
+                # delete feature namespace if it has no more feature
+                await self.feature_namespace_service.delete_document(
+                    document_id=feature.feature_namespace_id
+                )
+
+    async def list_features(
+        self,
+        page: int = 1,
+        page_size: int = 10,
+        sort_by: str | None = "created_at",
+        sort_dir: Literal["asc", "desc"] = "desc",
+        search: str | None = None,
+        name: str | None = None,
+        version: str | None = None,
+        feature_list_id: ObjectId | None = None,
+        feature_namespace_id: ObjectId | None = None,
+    ) -> FeaturePaginatedList:
+        """
+        List documents stored at persistent (GitDB or MongoDB)
 
         Parameters
         ----------
-        point_in_time_and_serving_name_list: list[Dict[str, Any]]
-            list of dictionary containing point in time and serving name
-        is_time_based: bool
-            whether the feature is time based
+        page: int
+            Page number
+        page_size: int
+            Number of items per page
+        sort_by: str | None
+            Key used to sort the returning documents
+        sort_dir: "asc" or "desc"
+            Sorting the returning documents in ascending order or descending order
+        search: str | None
+            Search token to be used in filtering
+        name: str | None
+            Feature name to be used in filtering
+        version: str | None
+            Feature version to be used in filtering
+        feature_list_id: ObjectId | None
+            Feature list ID to be used in filtering
+        feature_namespace_id: ObjectId | None
+            Feature namespace ID to be used in filtering
 
         Returns
         -------
-        Tuple[list[Dict[str, Any]], bool]
-            updated list of dictionary, and whether the dictionary was updated with an arbitrary time. Updated will only return
-            True if the dictionary did not contain a point in time variable before.
+        FeaturePaginatedList
+            List of documents fulfilled the filtering condition
+        """
+        # pylint: disable=too-many-locals
+        params: Dict[str, Any] = {"search": search, "name": name}
+        if version:
+            params["version"] = VersionIdentifier.from_str(version).dict()
+
+        if feature_list_id:
+            feature_list_document = await self.feature_list_service.get_document(
+                document_id=feature_list_id
+            )
+            params["query_filter"] = {"_id": {"$in": feature_list_document.feature_ids}}
 
-        Raises
-        ------
-        MissingPointInTimeColumnError
-            raised if the point in time column is not provided in the dictionary for a time based feature
-        """
-        updated = False
-        for point_in_time_and_serving_name in point_in_time_and_serving_name_list:
-            if SpecialColumnName.POINT_IN_TIME not in point_in_time_and_serving_name:
-                if is_time_based:
-                    raise MissingPointInTimeColumnError(
-                        f"Point in time column not provided: {SpecialColumnName.POINT_IN_TIME}"
-                    )
-
-                # If it's not time based, and no time is provided, use an arbitrary time.
-                point_in_time_and_serving_name[SpecialColumnName.POINT_IN_TIME] = ARBITRARY_TIME
-                updated = True
-
-                # convert point in time to tz-naive UTC
-                point_in_time_and_serving_name[SpecialColumnName.POINT_IN_TIME] = pd.to_datetime(
-                    point_in_time_and_serving_name[SpecialColumnName.POINT_IN_TIME], utc=True
-                ).tz_localize(None)
+        if feature_namespace_id:
+            query_filter = params.get("query_filter", {}).copy()
+            query_filter["feature_namespace_id"] = feature_namespace_id
+            params["query_filter"] = query_filter
+
+        # list documents from persistent
+        document_data = await self.service.list_documents_as_dict(
+            page=page,
+            page_size=page_size,
+            sort_by=sort_by,
+            sort_dir=sort_dir,
+            **params,
+        )
+
+        # prepare mappings to add additional attributes
+        entity_id_to_entity = await self.derive_primary_entity_helper.get_entity_id_to_entity(
+            doc_list=document_data["data"]
+        )
+        namespace_ids = {document["feature_namespace_id"] for document in document_data["data"]}
+        namespace_id_to_default_id = {}
+        async for namespace in self.feature_namespace_service.list_documents_as_dict_iterator(
+            query_filter={"_id": {"$in": list(namespace_ids)}}
+        ):
+            namespace_id_to_default_id[namespace["_id"]] = namespace["default_feature_id"]
+
+        # prepare output
+        output = []
+        for feature in document_data["data"]:
+            default_feature_id = namespace_id_to_default_id.get(feature["feature_namespace_id"])
+            primary_entity_ids = await self.derive_primary_entity_helper.derive_primary_entity_ids(
+                entity_ids=feature["entity_ids"], entity_id_to_entity=entity_id_to_entity
+            )
+            output.append(
+                FeatureModelResponse(
+                    **feature,
+                    is_default=default_feature_id == feature["_id"],
+                    primary_entity_ids=primary_entity_ids,
+                )
+            )
 
-        return point_in_time_and_serving_name_list, updated
+        document_data["data"] = output
+        return self.paginated_document_class(**document_data)
 
-    async def preview_feature(
-        self, feature_preview: FeaturePreview, get_credential: Any
+    async def preview(
+        self, feature_preview: FeatureOrTargetPreview, get_credential: Any
     ) -> dict[str, Any]:
         """
         Preview a Feature
 
         Parameters
         ----------
-        feature_preview: FeaturePreview
-            FeaturePreview object
+        feature_preview: FeatureOrTargetPreview
+            FeatureOrTargetPreview object
         get_credential: Any
             Get credential handler function
 
         Returns
         -------
         dict[str, Any]
             Dataframe converted to json string
+
+        Raises
+        ------
+        HTTPException
+            Invalid request payload
         """
-        graph = feature_preview.graph
-        feature_node = graph.get_node_by_name(feature_preview.node_name)
-        operation_struction = feature_preview.graph.extract_operation_structure(feature_node)
-
-        # We only need to ensure that the point in time column is provided,
-        # if the feature aggregation is time based.
-        (
-            point_in_time_and_serving_name_list,
-            updated,
-        ) = PreviewService._update_point_in_time_if_needed(
-            feature_preview.point_in_time_and_serving_name_list, operation_struction.is_time_based
-        )
-
-        request_column_names = set(point_in_time_and_serving_name_list[0].keys())
-        feature_store, session = await self._get_feature_store_session(
-            graph=graph,
-            node_name=feature_preview.node_name,
-            feature_store_name=feature_preview.feature_store_name,
-            get_credential=get_credential,
-        )
-        parent_serving_preparation = (
-            await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
-                graph=graph,
-                nodes=[feature_node],
-                request_column_names=request_column_names,
-                feature_store=feature_store,
-            )
-        )
-        preview_sql = get_feature_preview_sql(
-            request_table_name=f"{REQUEST_TABLE_NAME}_{session.generate_session_unique_id()}",
-            graph=graph,
-            nodes=[feature_node],
-            point_in_time_and_serving_name_list=point_in_time_and_serving_name_list,
-            source_type=feature_store.type,
-            parent_serving_preparation=parent_serving_preparation,
-        )
-        result = await session.execute_query(preview_sql)
-        if result is None:
-            return {}
-        if updated:
-            result = result.drop(SpecialColumnName.POINT_IN_TIME, axis="columns")
-        return dataframe_to_json(result)
+        try:
+            return await self.feature_preview_service.preview_target_or_feature(
+                feature_or_target_preview=feature_preview, get_credential=get_credential
+            )
+        except (MissingPointInTimeColumnError, RequiredEntityNotProvidedError) as exc:
+            raise HTTPException(
+                status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=exc.args[0]
+            ) from exc
 
-    async def preview_featurelist(
-        self, featurelist_preview: FeatureListPreview, get_credential: Any
-    ) -> dict[str, Any]:
+    async def get_info(
+        self,
+        document_id: ObjectId,
+        verbose: bool,
+    ) -> FeatureInfo:
         """
-        Preview a FeatureList
+        Get document info given document ID
 
         Parameters
         ----------
-        featurelist_preview: FeatureListPreview
-            FeatureListPreview object
-        get_credential: Any
-            Get credential handler function
+        document_id: ObjectId
+            Document ID
+        verbose: bool
+            Flag to control verbose level
 
         Returns
         -------
-        dict[str, Any]
-            Dataframe converted to json string
+        InfoDocument
         """
-        # Check if any of the features are time based
-        has_time_based_feature = False
-        for feature_cluster in featurelist_preview.feature_clusters:
-            for feature_node_name in feature_cluster.node_names:
-                feature_node = feature_cluster.graph.get_node_by_name(feature_node_name)
-                operation_struction = feature_cluster.graph.extract_operation_structure(
-                    feature_node
-                )
-                if operation_struction.is_time_based:
-                    has_time_based_feature = True
-                    break
-        # Raise error if there's no point in time provided for time based features.
-        (
-            point_in_time_and_serving_name_list,
-            updated,
-        ) = PreviewService._update_point_in_time_if_needed(
-            featurelist_preview.point_in_time_and_serving_name_list, has_time_based_feature
-        )
-
-        result: Optional[pd.DataFrame] = None
-        group_join_keys = list(point_in_time_and_serving_name_list[0].keys())
-        for feature_cluster in featurelist_preview.feature_clusters:
-            request_column_names = set(group_join_keys)
-            feature_store = await self.feature_store_service.get_document(
-                feature_cluster.feature_store_id
-            )
-            parent_serving_preparation = await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
-                graph=feature_cluster.graph,
-                nodes=feature_cluster.nodes,
-                request_column_names=request_column_names,
-                feature_store=feature_store,
-            )
-            db_session = await self.session_manager_service.get_feature_store_session(
-                feature_store=feature_store,
-                get_credential=get_credential,
-            )
-            preview_sql = get_feature_preview_sql(
-                request_table_name=f"{REQUEST_TABLE_NAME}_{db_session.generate_session_unique_id()}",
-                graph=feature_cluster.graph,
-                nodes=feature_cluster.nodes,
-                point_in_time_and_serving_name_list=point_in_time_and_serving_name_list,
-                source_type=feature_store.type,
-                parent_serving_preparation=parent_serving_preparation,
-            )
-            _result = await db_session.execute_query(preview_sql)
-            if result is None:
-                result = _result
-            else:
-                result = result.merge(_result, on=group_join_keys)
-
-        if result is None:
-            return {}
-        if updated:
-            result = result.drop(SpecialColumnName.POINT_IN_TIME, axis="columns")
-
-        return dataframe_to_json(result)
+        feature = await self.service.get_document(document_id=document_id)
+        catalog = await self.catalog_service.get_document(feature.catalog_id)
+        data_id_to_doc = {}
+        async for doc in self.table_service.list_documents_as_dict_iterator(
+            query_filter={"_id": {"$in": feature.table_ids}}
+        ):
+            doc["catalog_name"] = catalog.name
+            data_id_to_doc[doc["_id"]] = doc
 
-    async def compute_historical_features(
-        self,
-        observation_set: Union[pd.DataFrame, ObservationTableModel],
-        featurelist_get_historical_features: FeatureListGetHistoricalFeatures,
-        get_credential: Any,
-        output_table_details: TableDetails,
-        progress_callback: Optional[Callable[[int, str], None]] = None,
-    ) -> None:
-        """
-        Get historical features for Feature List
-
-        Parameters
-        ----------
-        observation_set: pd.DataFrame
-            Observation set data
-        featurelist_get_historical_features: FeatureListGetHistoricalFeatures
-            FeatureListGetHistoricalFeatures object
-        get_credential: Any
-            Get credential handler function
-        output_table_details: TableDetails
-            Table details to write the results to
-        progress_callback: Optional[Callable[[int, str], None]]
-            Optional progress callback function
-        """
-        # multiple feature stores not supported
-        feature_clusters = featurelist_get_historical_features.feature_clusters
-        assert len(feature_clusters) == 1
-
-        feature_cluster = feature_clusters[0]
-        feature_store = await self.feature_store_service.get_document(
-            document_id=feature_cluster.feature_store_id
+        data_id_to_name = {key: value["name"] for key, value in data_id_to_doc.items()}
+        namespace_info = await self.feature_namespace_controller.get_info(
+            document_id=feature.feature_namespace_id,
+            verbose=verbose,
         )
-
-        if isinstance(observation_set, pd.DataFrame):
-            request_column_names = set(observation_set.columns)
-        else:
-            request_column_names = {col.name for col in observation_set.columns_info}
-
-        parent_serving_preparation = (
-            await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
-                graph=feature_cluster.graph,
-                nodes=feature_cluster.nodes,
-                request_column_names=request_column_names,
-                feature_store=feature_store,
-                serving_names_mapping=featurelist_get_historical_features.serving_names_mapping,
-            )
+        default_feature = await self.service.get_document(
+            document_id=namespace_info.default_feature_id
         )
+        versions_info = None
+        if verbose:
+            namespace = await self.feature_namespace_service.get_document(
+                document_id=feature.feature_namespace_id
+            )
+            versions_info = FeatureBriefInfoList.from_paginated_data(
+                await self.service.list_documents_as_dict(
+                    page=1,
+                    page_size=0,
+                    query_filter={"_id": {"$in": namespace.feature_ids}},
+                )
+            )
 
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store,
-            get_credential=get_credential,
-        )
+        op_struct = feature.extract_operation_structure()
+        metadata = await self.feature_or_target_metadata_extractor.extract(op_struct=op_struct)
 
-        feature_list_id = featurelist_get_historical_features.feature_list_id
-        try:
-            if feature_list_id is None:
-                is_feature_list_deployed = False
-            else:
-                feature_list = await self.feature_list_service.get_document(feature_list_id)
-                is_feature_list_deployed = feature_list.deployed
-        except DocumentNotFoundError:
-            is_feature_list_deployed = False
-
-        await get_historical_features(
-            session=db_session,
-            graph=feature_cluster.graph,
-            nodes=feature_cluster.nodes,
-            observation_set=observation_set,
-            serving_names_mapping=featurelist_get_historical_features.serving_names_mapping,
-            source_type=feature_store.type,
-            is_feature_list_deployed=is_feature_list_deployed,
-            parent_serving_preparation=parent_serving_preparation,
-            output_table_details=output_table_details,
-            progress_callback=progress_callback,
+        namespace_info_dict = namespace_info.dict()
+        # use feature list description instead of namespace description
+        namespace_description = namespace_info_dict.pop("description", None)
+        return FeatureInfo(
+            **namespace_info_dict,
+            version={"this": feature.version.to_str(), "default": default_feature.version.to_str()},
+            readiness={"this": feature.readiness, "default": default_feature.readiness},
+            table_feature_job_setting={
+                "this": _extract_table_feature_job_settings(
+                    feature=feature, table_id_to_name=data_id_to_name
+                ),
+                "default": _extract_table_feature_job_settings(
+                    feature=default_feature, table_id_to_name=data_id_to_name
+                ),
+            },
+            table_cleaning_operation={
+                "this": _extract_feature_table_cleaning_operations(
+                    feature=feature, table_id_to_name=data_id_to_name
+                ),
+                "default": _extract_feature_table_cleaning_operations(
+                    feature=default_feature, table_id_to_name=data_id_to_name
+                ),
+            },
+            versions_info=versions_info,
+            metadata=metadata,
+            namespace_description=namespace_description,
+            description=feature.description,
         )
 
-    async def feature_sql(self, feature_sql: FeatureSQL) -> str:
+    async def sql(self, feature_sql: FeatureSQL) -> str:
         """
         Get Feature SQL
 
         Parameters
         ----------
         feature_sql: FeatureSQL
-            FeatureGraph object
+            FeatureSQL object
 
         Returns
         -------
         str
-            SQL statements
-        """
-        graph = feature_sql.graph
-        feature_node = graph.get_node_by_name(feature_sql.node_name)
-
-        source_type = graph.get_input_node(
-            feature_sql.node_name
-        ).parameters.feature_store_details.type
-        preview_sql = get_feature_preview_sql(
-            request_table_name=REQUEST_TABLE_NAME,
-            graph=graph,
-            nodes=[feature_node],
-            source_type=source_type,
-        )
-        return preview_sql
-
-    async def featurelist_sql(self, featurelist_sql: FeatureListSQL) -> str:
-        """
-        Get FeatureList SQL
-
-        Parameters
-        ----------
-        featurelist_sql: FeatureListSQL
-            FeatureListSQL object
-
-        Returns
-        -------
-        str
-            SQL statements
-        """
-
-        preview_sqls = []
-        for feature_cluster in featurelist_sql.feature_clusters:
-            source_type = feature_cluster.graph.get_input_node(
-                feature_cluster.node_names[0]
-            ).parameters.feature_store_details.type
-            preview_sql = get_feature_preview_sql(
-                request_table_name=REQUEST_TABLE_NAME,
-                graph=feature_cluster.graph,
-                nodes=feature_cluster.nodes,
-                source_type=source_type,
-            )
-            preview_sqls.append(preview_sql)
-
-        return "\n\n".join(preview_sqls)
-
-    async def get_historical_features_sql(
-        self,
-        observation_set: pd.DataFrame,
-        featurelist_get_historical_features: FeatureListGetHistoricalFeatures,
-    ) -> str:
-        """
-        Get historical features SQL for Feature List
-
-        Parameters
-        ----------
-        observation_set: pd.DataFrame
-            Observation set data
-        featurelist_get_historical_features: FeatureListGetHistoricalFeatures
-            FeatureListGetHistoricalFeatures object
-
-        Returns
-        -------
-        str
-            SQL statements
+            Dataframe converted to json string
         """
-        # multiple feature stores not supported
-        feature_clusters = featurelist_get_historical_features.feature_clusters
-        assert len(feature_clusters) == 1
-        feature_cluster = feature_clusters[0]
-
-        source_type = feature_cluster.graph.get_input_node(
-            feature_cluster.node_names[0]
-        ).parameters.feature_store_details.type
-
-        expr, _ = get_historical_features_expr(
-            request_table_name=REQUEST_TABLE_NAME,
-            graph=feature_cluster.graph,
-            nodes=feature_cluster.nodes,
-            request_table_columns=observation_set.columns.tolist(),
-            source_type=source_type,
-            serving_names_mapping=featurelist_get_historical_features.serving_names_mapping,
-        )
-        return sql_to_string(expr, source_type=source_type)
+        return await self.feature_preview_service.feature_sql(feature_sql=feature_sql)
 
-    async def download_table(
-        self,
-        location: TabularSource,
-        get_credential: Any,
-    ) -> Optional[AsyncGenerator[bytes, None]]:
+    async def get_feature_job_logs(self, feature_id: ObjectId, hour_limit: int) -> dict[str, Any]:
         """
-        Download table from location.
+        Retrieve table preview for query graph node
 
         Parameters
         ----------
-        location: TabularSource
-            Location to download from
-        get_credential: Any
-            Get credential handler function
+        feature_id: ObjectId
+            Feature Id
+        hour_limit: int
+            Limit in hours on the job history to fetch
 
         Returns
         -------
-        AsyncGenerator[bytes, None]
-            Asynchronous bytes generator
-
-        Raises
-        ------
-        LimitExceededError
-            Table size exceeds the limit.
+        dict[str, Any]
+            Dataframe converted to json string
         """
-        feature_store = await self.feature_store_service.get_document(
-            document_id=location.feature_store_id
-        )
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store,
-            get_credential=get_credential,
-        )
-
-        # check size of the table
-        sql_expr = get_source_count_expr(source=location.table_details)
-        sql = sql_to_string(
-            sql_expr,
-            source_type=db_session.source_type,
-        )
-        result = await db_session.execute_query(sql)
-        assert result is not None
-        columns = await db_session.list_table_schema(**location.table_details.json_dict())
-        shape = (result["row_count"].iloc[0], len(columns))
-
-        logger.debug(
-            "Downloading table from feature store",
-            extra={
-                "location": location.json_dict(),
-                "shape": shape,
-            },
-        )
-
-        if shape[0] * shape[0] > MAX_TABLE_CELLS:
-            raise LimitExceededError(f"Table size {shape} exceeds download limit.")
-
-        sql_expr = get_source_expr(source=location.table_details)
-        sql = sql_to_string(
-            sql_expr,
-            source_type=db_session.source_type,
+        feature = await self.service.get_document(feature_id)
+        return await self.tile_job_log_service.get_feature_job_logs(
+            features=[ExtendedFeatureModel(**feature.dict(by_alias=True))],
+            hour_limit=hour_limit,
         )
-        return db_session.get_async_query_stream(sql)
```

### Comparing `featurebyte-0.3.1/featurebyte/service/relationship.py` & `featurebyte-0.4.0/featurebyte/service/relationship.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,43 +1,49 @@
 """
 RelationshipService class
 """
 from __future__ import annotations
 
-from typing import Any, TypeVar, cast
+from typing import TypeVar, cast
 
 from abc import abstractmethod
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
 from featurebyte.models.base import FeatureByteBaseDocumentModel, FeatureByteBaseModel
 from featurebyte.models.relationship import Parent, Relationship
 from featurebyte.persistent import Persistent
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
 from featurebyte.schema.entity import EntityServiceUpdate
 from featurebyte.schema.semantic import SemanticServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.entity import EntityService
+from featurebyte.service.mixin import OpsServiceMixin
 from featurebyte.service.semantic import SemanticService
 
 ParentT = TypeVar("ParentT", bound=Parent)
 BaseDocumentServiceT = BaseDocumentService[
     FeatureByteBaseDocumentModel, FeatureByteBaseModel, BaseDocumentServiceUpdateSchema
 ]
 
 
-class RelationshipService(BaseService):
+class RelationshipService(OpsServiceMixin):
     """
     RelationshipService class is responsible for manipulating object relationship and maintaining
     the expected relationship property (example, no cyclic relationship like A is an ancestor of B and
     B is also an ancestor of A).
     """
 
+    def __init__(
+        self,
+        persistent: Persistent,
+    ):
+        self.persistent = persistent
+
     @property
     @abstractmethod
     def document_service(self) -> BaseDocumentServiceT:
         """
         DocumentService that is used to update relationship attributes
 
         Raises
@@ -115,15 +121,15 @@
                 ),
                 return_document=True,
             )
             updated_document = cast(Relationship, updated_document)
 
             # update all objects which have child_id in their ancestor_ids
             query_filter = {"ancestor_ids": {"$in": [child_id]}}
-            async for obj in self.document_service.list_documents_iterator(
+            async for obj in self.document_service.list_documents_as_dict_iterator(
                 query_filter=query_filter
             ):
                 await self.document_service.update_document(
                     document_id=obj["_id"],
                     data=self.prepare_document_update_payload(
                         ancestor_ids=set(obj["ancestor_ids"]).union(updated_document.ancestor_ids),
                         parents=obj["parents"],
@@ -176,15 +182,15 @@
                 ),
                 return_document=True,
             )
             updated_document = cast(Relationship, updated_document)
 
             # update all objects which have child_id in their ancestor_ids
             query_filter = {"ancestor_ids": {"$in": [child_id]}}
-            async for obj in self.document_service.list_documents_iterator(
+            async for obj in self.document_service.list_documents_as_dict_iterator(
                 query_filter=query_filter
             ):
                 await self.document_service.update_document(
                     document_id=obj["_id"],
                     data=self.prepare_document_update_payload(
                         ancestor_ids=set(obj["ancestor_ids"]).difference(
                             self.include_object_id(parent_object.ancestor_ids, parent_id)
@@ -197,17 +203,17 @@
 
 
 class EntityRelationshipService(RelationshipService):
     """
     EntityRelationshipService is responsible to update relationship between different entities.
     """
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user, persistent, catalog_id)
-        self.entity_service = EntityService(user=user, persistent=persistent, catalog_id=catalog_id)
+    def __init__(self, persistent: Persistent, entity_service: EntityService):
+        super().__init__(persistent=persistent)
+        self.entity_service = entity_service
 
     @property
     def document_service(self) -> BaseDocumentServiceT:
         return self.entity_service  # type: ignore[return-value]
 
     @classmethod
     def prepare_document_update_payload(
@@ -217,19 +223,17 @@
 
 
 class SemanticRelationshipService(RelationshipService):
     """
     SemanticRelationshipService is responsible to update relationship between different semantics.
     """
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user, persistent, catalog_id)
-        self.semantic_service = SemanticService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+    def __init__(self, persistent: Persistent, semantic_service: SemanticService):
+        super().__init__(persistent=persistent)
+        self.semantic_service = semantic_service
 
     @property
     def document_service(self) -> BaseDocumentServiceT:
         return self.semantic_service  # type: ignore[return-value]
 
     @classmethod
     def prepare_document_update_payload(
```

### Comparing `featurebyte-0.3.1/featurebyte/service/relationship_info.py` & `featurebyte-0.4.0/featurebyte/service/relationship_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -34,15 +34,15 @@
             Related entity id
 
         Raises
         ------
         DocumentNotFoundError
             If relationship not found
         """
-        result = await self.list_documents(
+        result = await self.list_documents_as_dict(
             query_filter={
                 "entity_id": primary_entity_id,
                 "related_entity_id": related_entity_id,
             },
         )
         data = result["data"]
         if not data:
```

### Comparing `featurebyte-0.3.1/featurebyte/service/scd_table.py` & `featurebyte-0.4.0/featurebyte/service/scd_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/semantic.py` & `featurebyte-0.4.0/featurebyte/service/semantic.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/session_manager.py` & `featurebyte-0.4.0/featurebyte/service/session_manager.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,74 +1,77 @@
 """
 SessionManager service
 """
-from typing import Any
+from typing import Any, Optional
 
-from bson import ObjectId
 from pydantic import ValidationError
 
 from featurebyte.exception import CredentialsError
+from featurebyte.models.base import User
 from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.persistent import Persistent
 from featurebyte.service.session_validator import SessionValidatorService
 from featurebyte.session.base import BaseSession
 from featurebyte.session.manager import SessionManager
 from featurebyte.utils.credential import MongoBackedCredentialProvider
 
 
 class SessionManagerService:
     """
     SessionManagerService class is responsible for retrieving a session manager.
     """
 
     def __init__(
         self,
         user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
-        credential_provider: MongoBackedCredentialProvider,
+        mongo_backed_credential_provider: MongoBackedCredentialProvider,
         session_validator_service: SessionValidatorService,
     ):
         self.user = user
-        self.persistent = persistent
-        self.catalog_id = catalog_id
-        self.credential_provider = credential_provider
+        self.credential_provider = mongo_backed_credential_provider
         self.session_validator_service = session_validator_service
 
     async def get_feature_store_session(
-        self, feature_store: FeatureStoreModel, get_credential: Any
+        self,
+        feature_store: FeatureStoreModel,
+        get_credential: Any = None,
+        user_override: Optional[User] = None,
     ) -> BaseSession:
         """
         Get session for feature store
 
         Parameters
         ----------
         feature_store: FeatureStoreModel
             ExtendedFeatureStoreModel object
         get_credential: Any
             Get credential handler function
+        user_override: Optional[User]
+            User object to override
 
         Returns
         -------
         BaseSession
             BaseSession object
 
         Raises
         ------
         CredentialsError
             When the credentials used to access the feature store is missing or invalid
         """
+        user_to_use = self.user
+        if user_override is not None:
+            user_to_use = user_override
         try:
             if get_credential is not None:
                 credential = await get_credential(
-                    user_id=self.user.id, feature_store_name=feature_store.name
+                    user_id=user_to_use.id, feature_store_name=feature_store.name
                 )
             else:
                 credential = await self.credential_provider.get_credential(
-                    user_id=self.user.id, feature_store_name=feature_store.name
+                    user_id=user_to_use.id, feature_store_name=feature_store.name
                 )
             credentials = {feature_store.name: credential}
             session_manager = SessionManager(credentials=credentials)
             session = await session_manager.get_session(feature_store)
             await self.session_validator_service.validate_feature_store_exists(
                 feature_store.details
             )
```

### Comparing `featurebyte-0.3.1/featurebyte/service/session_validator.py` & `featurebyte-0.4.0/featurebyte/service/session_validator.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,16 @@
 """
 SessionValidator service
 """
 from typing import Any, Optional
 
-from bson import ObjectId
-
 from featurebyte.enum import SourceType, StrEnum
 from featurebyte.exception import FeatureStoreSchemaCollisionError, NoFeatureStorePresentError
 from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.persistent import Persistent
 from featurebyte.query_graph.node.schema import DatabaseDetails
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.session.base import BaseSession
 from featurebyte.session.manager import SessionManager
 from featurebyte.utils.credential import MongoBackedCredentialProvider
 
 logger = get_logger(__name__)
@@ -33,25 +30,20 @@
     SessionValidatorService class is responsible for validating whether a
     session that we're trying to initialize is valid.
     """
 
     def __init__(
         self,
         user: Any,
-        persistent: Persistent,
-        catalog_id: ObjectId,
-        credential_provider: MongoBackedCredentialProvider,
+        mongo_backed_credential_provider: MongoBackedCredentialProvider,
+        feature_store_service: FeatureStoreService,
     ):
         self.user = user
-        self.persistent = persistent
-        self.catalog_id = catalog_id
-        self.credential_provider = credential_provider
-        self.feature_store_service = FeatureStoreService(
-            user=self.user, persistent=self.persistent, catalog_id=catalog_id
-        )
+        self.credential_provider = mongo_backed_credential_provider
+        self.feature_store_service = feature_store_service
 
     @classmethod
     async def validate_existing_session(
         cls, session: BaseSession, users_feature_store_id: Optional[PydanticObjectId]
     ) -> ValidateStatus:
         """
         Validates whether the existing session is valid.
@@ -143,56 +135,21 @@
         NoFeatureStorePresentError
             error thrown when no feature store is present
         """
         users_feature_store_id = await self.get_feature_store_id_from_details(details)
         if users_feature_store_id is None:
             raise NoFeatureStorePresentError
 
-    async def validate_details(
-        self,
-        feature_store_name: str,
-        session_type: SourceType,
-        details: DatabaseDetails,
-        get_credential: Any,
-    ) -> ValidateStatus:
-        """
-        Validate whether the existing details exist in the persistent layer
-        or in the data warehouse.
-
-        Parameters
-        ----------
-        feature_store_name: str
-            feature store name
-        session_type: SourceType
-            session type
-        details: DatabaseDetails
-            database details
-        get_credential: Any
-            credential handler function
-
-        Returns
-        -------
-        ValidateStatus
-            The status of the validation
-        """
-        # Retrieve the feature store ID
-        users_feature_store_id = await self.get_feature_store_id_from_details(details)
-
-        # Check whether the feature store ID has been used in the data warehouse.
-        return await self.validate_feature_store_id_not_used_in_warehouse(
-            feature_store_name, session_type, details, get_credential, users_feature_store_id
-        )
-
     async def validate_feature_store_id_not_used_in_warehouse(
         self,
         feature_store_name: str,
         session_type: SourceType,
         details: DatabaseDetails,
-        get_credential: Any,
         users_feature_store_id: Optional[PydanticObjectId],
+        get_credential: Any = None,
     ) -> ValidateStatus:
         """
         Validate whether the existing details exist in the persistent layer
         or in the data warehouse.
 
         Parameters
         ----------
@@ -227,15 +184,15 @@
             database details
 
         Returns
         -------
         Optional[PydanticObjectId]
             Feature store ID if present. If not, returns None.
         """
-        response = await self.feature_store_service.list_documents(
+        response = await self.feature_store_service.list_documents_as_dict(
             query_filter={"details": details.dict()}
         )
 
         count = response["total"]
         does_exist = count != 0
         # We expect to see at most one entry. Error if there's more than one.
         assert count < 2
```

### Comparing `featurebyte-0.3.1/featurebyte/service/table.py` & `featurebyte-0.4.0/featurebyte/service/table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/service/table_columns_info.py` & `featurebyte-0.4.0/featurebyte/service/table_columns_info.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,79 +1,78 @@
 """
 TableColumnsInfoService
 """
 from __future__ import annotations
 
-from typing import Any, List, Tuple, Union
+from typing import List, Tuple, Union
 
 from collections import defaultdict
 
 from bson.objectid import ObjectId
 
 from featurebyte.enum import TableDataType
 from featurebyte.exception import DocumentUpdateError
-from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base import PydanticObjectId, User
 from featurebyte.models.entity import ParentEntity
 from featurebyte.models.feature_store import TableModel
 from featurebyte.models.relationship import RelationshipType
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.schema.entity import EntityServiceUpdate
 from featurebyte.schema.relationship_info import RelationshipInfoCreate
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.dimension_table import DimensionTableService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.event_table import EventTableService
-from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.item_table import ItemTableService
+from featurebyte.service.mixin import OpsServiceMixin
 from featurebyte.service.relationship import EntityRelationshipService
 from featurebyte.service.relationship_info import RelationshipInfoService
 from featurebyte.service.scd_table import SCDTableService
 from featurebyte.service.semantic import SemanticService
 
 TableDocumentService = Union[
     EventTableService, ItemTableService, DimensionTableService, SCDTableService
 ]
 
 
-class TableColumnsInfoService(BaseService):
+class TableColumnsInfoService(OpsServiceMixin):
     """
     TableColumnsInfoService is responsible to orchestrate the update of the table columns info.
     """
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_store_service = FeatureStoreService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.semantic_service = SemanticService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.entity_service = EntityService(user=user, persistent=persistent, catalog_id=catalog_id)
-        self.relationship_info_service = RelationshipInfoService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self.entity_relationship_service = EntityRelationshipService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
+    def __init__(
+        self,
+        user: User,
+        persistent: Persistent,
+        semantic_service: SemanticService,
+        entity_service: EntityService,
+        relationship_info_service: RelationshipInfoService,
+        entity_relationship_service: EntityRelationshipService,
+    ):
+        self.user = user
+        self.persistent = persistent
+        self.semantic_service = semantic_service
+        self.entity_service = entity_service
+        self.relationship_info_service = relationship_info_service
+        self.entity_relationship_service = entity_relationship_service
 
     @staticmethod
     async def _validate_column_info_id_field_values(
         columns_info: List[ColumnInfo],
         field_name: str,
         service: Union[EntityService, SemanticService],
         field_class_name: str,
     ) -> None:
         id_values = [
             getattr(col_info, field_name)
             for col_info in columns_info
             if getattr(col_info, field_name)
         ]
         found_id_values = [
-            ObjectId(doc["_id"])
+            doc.id
             async for doc in service.list_documents_iterator(
                 query_filter={"_id": {"$in": id_values}}
             )
         ]
         missing_id_values = sorted(set(id_values).difference(found_id_values))
         if missing_id_values:
             column_name_id_pairs = sorted(
```

### Comparing `featurebyte-0.3.1/featurebyte/service/table_status.py` & `featurebyte-0.4.0/featurebyte/service/table_status.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,35 +1,31 @@
 """
 TableStatusService class
 """
-from typing import Any
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentUpdateError
 from featurebyte.models.feature_store import TableStatus
 from featurebyte.persistent import Persistent
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_readiness import FeatureReadinessService
 from featurebyte.service.table_columns_info import TableDocumentService
 
 
-class TableStatusService(BaseService):
+class TableStatusService:
     """TableStatusService class"""
 
     def __init__(
         self,
-        user: Any,
         persistent: Persistent,
-        catalog_id: ObjectId,
         feature_service: FeatureService,
         feature_readiness_service: FeatureReadinessService,
     ):
-        super().__init__(user, persistent, catalog_id)
+        self.persistent = persistent
         self.feature_service = feature_service
         self.feature_readiness_service = feature_readiness_service
 
     async def update_status(
         self, service: TableDocumentService, document_id: ObjectId, status: TableStatus
     ) -> None:
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/service/task_manager.py` & `featurebyte-0.4.0/featurebyte/service/task_manager.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,117 +2,47 @@
 TaskManager service is responsible to submit task message
 """
 from __future__ import annotations
 
 from typing import Any, Optional, Union, cast
 
 import datetime
-from abc import abstractmethod
 from uuid import UUID
 
 from bson.objectid import ObjectId
+from celery import Celery
 
 from featurebyte.logging import get_logger
 from featurebyte.models.periodic_task import Crontab, Interval, PeriodicTask
 from featurebyte.models.task import Task as TaskModel
 from featurebyte.persistent import Persistent
 from featurebyte.schema.task import Task
 from featurebyte.schema.worker.task.base import BaseTaskPayload
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
 from featurebyte.service.periodic_task import PeriodicTaskService
-from featurebyte.worker import celery
 
 TaskId = Union[ObjectId, UUID]
 
 
 logger = get_logger(__name__)
 
 
-class AbstractTaskManager:
+class TaskManager:
     """
-    AbstractTaskManager defines interface for TaskManager
+    TaskManager class is responsible for submitting task request & task status retrieval
     """
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId) -> None:
-        """
-        TaskManager constructor
-
-        Parameters
-        ----------
-        user: Any
-            User
-        persistent: Persistent
-            Persistent
-        catalog_id: ObjectId
-            Catalog ID
-        """
+    def __init__(
+        self, user: Any, persistent: Persistent, celery: Celery, catalog_id: Optional[ObjectId]
+    ) -> None:
         self.user = user
         self.persistent = persistent
+        self.celery = celery
         self.catalog_id = catalog_id
 
-    @abstractmethod
-    async def submit(self, payload: BaseTaskPayload) -> TaskId:
-        """
-        Submit task request given task payload
-
-        Parameters
-        ----------
-        payload: BaseTaskPayload
-            Task payload object
-
-        Returns
-        -------
-        TaskId
-            Task identifier used to check task status
-        """
-
-    @abstractmethod
-    async def get_task(self, task_id: str) -> Optional[Task]:
-        """
-        Retrieve task status given ID
-
-        Parameters
-        ----------
-        task_id: str
-            Task ID
-
-        Returns
-        -------
-        Task
-        """
-
-    @abstractmethod
-    async def list_tasks(
-        self,
-        page: int = 1,
-        page_size: int = 10,
-        ascending: bool = True,
-    ) -> tuple[list[Task], int]:
-        """
-        List task statuses of this user
-
-        Parameters
-        ----------
-        page: int
-            Page number
-        page_size: int
-            Page size
-        ascending: bool
-            Sorting order
-
-        Returns
-        -------
-        tuple[list[Task], int]
-        """
-
-
-class TaskManager(AbstractTaskManager):
-    """
-    TaskManager class is responsible for submitting task request & task status retrieval
-    """
-
     async def submit(self, payload: BaseTaskPayload) -> TaskId:
         """
         Submit task to celery
 
         Parameters
         ----------
         payload: BaseTaskPayload
@@ -122,15 +52,15 @@
         -------
         TaskId
             Task ID
         """
         assert self.user.id == payload.user_id
         kwargs = payload.json_dict()
         kwargs["task_output_path"] = payload.task_output_path
-        task = celery.send_task(payload.task, kwargs=kwargs)
+        task = self.celery.send_task(payload.task, kwargs=kwargs)
         return cast(TaskId, task.id)
 
     async def get_task(self, task_id: str) -> Task | None:
         """
         Get task information
 
         Parameters
@@ -140,15 +70,15 @@
 
         Returns
         -------
         Task
             Task object
         """
         task_id = str(task_id)
-        task_result = celery.AsyncResult(task_id)
+        task_result = self.celery.AsyncResult(task_id)
         payload = {}
         output_path = None
         traceback = None
 
         # try to find in persistent first
         document = await self.persistent.find_one(
             collection_name=TaskModel.collection_name(),
@@ -169,17 +99,33 @@
             payload=payload,
             traceback=traceback,
         )
 
     async def list_tasks(
         self,
         page: int = 1,
-        page_size: int = 10,
+        page_size: int = DEFAULT_PAGE_SIZE,
         ascending: bool = True,
     ) -> tuple[list[Task], int]:
+        """
+        List tasks.
+
+        Parameters
+        ----------
+        page: int
+            Page number
+        page_size: int
+            Page size
+        ascending: bool
+            Sort direction
+
+        Returns
+        -------
+        tuple[list[Task], int]
+        """
         # Perform the query
         results, total = await self.persistent.find(
             collection_name=TaskModel.collection_name(),
             query_filter={},
             page=page,
             page_size=page_size,
             sort_by="date_done",
@@ -343,15 +289,15 @@
         """
         periodic_task_service = PeriodicTaskService(
             user=self.user,
             persistent=self.persistent,
             catalog_id=self.catalog_id,
         )
 
-        result = await periodic_task_service.list_documents(
+        result = await periodic_task_service.list_documents_as_dict(
             page=1,
             page_size=0,
             query_filter={"name": name},
         )
 
         data = result["data"]
         if data:
@@ -385,15 +331,15 @@
             Document Name
         """
         periodic_task_service = PeriodicTaskService(
             user=self.user,
             persistent=self.persistent,
             catalog_id=self.catalog_id,
         )
-        result = await periodic_task_service.list_documents(
+        result = await periodic_task_service.list_documents_as_dict(
             page=1,
             page_size=0,
             query_filter={"name": name},
         )
 
         data = result["data"]
         if not data:
```

### Comparing `featurebyte-0.3.1/featurebyte/service/user_service.py` & `featurebyte-0.4.0/featurebyte/service/user_service.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 """
 User service module
 """
 from typing import Optional
 
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.service.base_service import BaseService
 
 
-class UserService(BaseService):
+class UserService:
     """
     Basic no-op user service.
 
     We add a basic API here so that the SaaS version can easily override this with more functionality.
     """
 
     def get_user_name_for_id(self, user_id: Optional[PydanticObjectId]) -> str:
```

### Comparing `featurebyte-0.3.1/featurebyte/service/validator/production_ready_validator.py` & `featurebyte-0.4.0/featurebyte/service/validator/production_ready_validator.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 """
 Production ready validator
 """
 from typing import Any, Dict, List, cast
 
 from featurebyte import ColumnCleaningOperation, FeatureJobSetting
-from featurebyte.exception import NoChangesInFeatureVersionError
-from featurebyte.models.feature import FeatureModel, FeatureReadiness
+from featurebyte.exception import DocumentUpdateError, NoChangesInFeatureVersionError
+from featurebyte.models.feature import FeatureModel
+from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.models.feature_store import TableStatus
 from featurebyte.query_graph.enum import GraphNodeType, NodeType
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.generic import GroupByNode
 from featurebyte.query_graph.node.nested import BaseViewGraphNodeParameters
@@ -90,25 +91,25 @@
         feature_job_setting_diff: Dict[str, Any]
             feature job setting diffs
         cleaning_ops_diff: Dict[str, Any]
             cleaning operations differences
 
         Raises
         ------
-        ValueError
-            raised if there are any differences
+        DocumentUpdateError
+            raised if there are any differences between the promoted feature version and table defaults
         """
         if not feature_job_setting_diff and not cleaning_ops_diff:
             return
         diff_format_dict = {}
         if feature_job_setting_diff:
             diff_format_dict["feature_job_setting"] = feature_job_setting_diff
         if cleaning_ops_diff:
             diff_format_dict["cleaning_operations"] = cleaning_ops_diff
-        raise ValueError(
+        raise DocumentUpdateError(
             "Discrepancies found between the promoted feature version you are trying to promote to "
             "PRODUCTION_READY, and the input table.\n"
             f"{diff_format_dict}\n"
             "Please fix these issues first before trying to promote your feature to PRODUCTION_READY."
         )
 
     async def _assert_no_other_production_ready_feature(
@@ -120,28 +121,28 @@
         Parameters
         ----------
         promoted_feature: FeatureModel
             Feature being promoted to PRODUCTION_READY
 
         Raises
         ------
-        ValueError
+        DocumentUpdateError
             raised when there is another feature version with the same name that is production ready
         """
         query_filter = {
             "name": promoted_feature.name,
             "readiness": FeatureReadiness.PRODUCTION_READY.value,
         }
-        async for feature_doc in self.feature_service.list_documents_iterator(
+        async for feature in self.feature_service.list_documents_iterator(
             query_filter=query_filter
         ):
-            if feature_doc["_id"] != promoted_feature.id:
-                raise ValueError(
+            if feature.id != promoted_feature.id:
+                raise DocumentUpdateError(
                     f"Found another feature version that is already PRODUCTION_READY. Please deprecate the feature "
-                    f"\"{promoted_feature.name}\" with ID {feature_doc['_id']} first before promoting the promoted "
+                    f'"{promoted_feature.name}" with ID {feature.id} first before promoting the promoted '
                     "version as there can only be one feature version that is production ready at any point in time. "
                     f"We are unable to promote the feature with ID {promoted_feature.id} right now."
                 )
 
     async def _assert_no_deprecated_table(self, promoted_feature: FeatureModel) -> None:
         """
         Check to see if there are any deprecated tables.
@@ -149,27 +150,24 @@
         Parameters
         ----------
         promoted_feature: FeatureModel
             Feature being promoted to PRODUCTION_READY
 
         Raises
         ------
-        ValueError
+        DocumentUpdateError
             raise when deprecated tables are found
         """
         query_filter = {
             "_id": {"$in": promoted_feature.table_ids},
             "status": TableStatus.DEPRECATED.value,
         }
-        async for table_doc in self.table_service.list_documents_iterator(
-            query_filter=query_filter
-        ):
-            table_name = table_doc["name"]
-            raise ValueError(
-                f'Found a deprecated table "{table_name}" that is used by the feature "{promoted_feature.name}". '
+        async for table in self.table_service.list_documents_iterator(query_filter=query_filter):
+            raise DocumentUpdateError(
+                f'Found a deprecated table "{table.name}" that is used by the feature "{promoted_feature.name}". '
                 "We are unable to promote the feature to PRODUCTION_READY right now."
             )
 
     @staticmethod
     def _get_feature_job_setting_from_groupby_node(node: Node) -> FeatureJobSetting:
         """
         Get feature job setting from node.
```

### Comparing `featurebyte-0.3.1/featurebyte/service/view_construction.py` & `featurebyte-0.4.0/featurebyte/service/view_construction.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,52 +1,40 @@
 """
 ViewConstructionService class
 """
 from __future__ import annotations
 
 from typing import Any, cast
 
-from collections import defaultdict
-
 from bson import ObjectId
 
 from featurebyte import ColumnCleaningOperation, TableCleaningOperation
 from featurebyte.exception import GraphInconsistencyError
-from featurebyte.persistent import Persistent
 from featurebyte.query_graph.enum import GraphNodeType
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.graph import GraphNodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.input import InputNode
-from featurebyte.query_graph.node.metadata.operation import (
-    DerivedDataColumn,
-    OperationStructure,
-    SourceDataColumn,
-    ViewDataColumnType,
-)
 from featurebyte.query_graph.node.nested import (
     BaseGraphNode,
     BaseViewGraphNodeParameters,
     ViewMetadata,
 )
-from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.table import TableService
 
 
-class ViewConstructionService(BaseService):
+class ViewConstructionService:
     """
     ViewConstructionService class is responsible for constructing view graph nodes inside a query graph.
     This service will retrieve the table from the table service and construct the view graph nodes.
     """
 
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user, persistent, catalog_id)
-        self.table_service = TableService(user=user, persistent=persistent, catalog_id=catalog_id)
+    def __init__(self, table_service: TableService):
+        self.table_service = table_service
 
     @staticmethod
     def _get_additional_keyword_parameters_pairs(
         graph_node: BaseGraphNode,
         input_node_names: list[str],
         view_node_name_to_table_info: dict[str, tuple[Node, list[ColumnInfo], InputNode]],
     ) -> tuple[dict[str, Any], dict[str, Any]]:
@@ -174,34 +162,14 @@
             metadata=temp_view_graph_node.parameters.prune_metadata(
                 target_columns=target_columns, input_nodes=graph_input_nodes
             ),
             **create_view_kwargs,
         )
         return new_view_graph_node, new_view_columns_info
 
-    @staticmethod
-    def _prepare_table_id_to_table_column_names(
-        operation_structure: OperationStructure,
-    ) -> dict[ObjectId, set[str]]:
-        # prepare table ID to source column names mapping, use this mapping to prune the view graph node parameters
-        table_id_to_source_column_names: dict[ObjectId, set[str]] = defaultdict(set)
-        for col in operation_structure.columns:
-            if col.type == ViewDataColumnType.SOURCE:
-                assert isinstance(col, SourceDataColumn)
-                assert col.table_id is not None, "Source table ID is missing."
-                table_id_to_source_column_names[col.table_id].add(col.name)
-            else:
-                assert isinstance(col, DerivedDataColumn)
-                for derived_source_col in col.columns:
-                    assert derived_source_col.table_id is not None, "Source table ID is missing."
-                    table_id_to_source_column_names[derived_source_col.table_id].add(
-                        derived_source_col.name
-                    )
-        return table_id_to_source_column_names
-
     async def prepare_view_node_name_to_replacement_node(
         self,
         query_graph: QueryGraphModel,
         target_node: Node,
         table_cleaning_operations: list[TableCleaningOperation],
         use_source_settings: bool,
     ) -> dict[str, Node]:
@@ -211,43 +179,40 @@
 
         Parameters
         ----------
         query_graph: QueryGraphModel
             Feature model graph
         target_node: Node
             Target node of the feature model graph
-        table_cleaning_operations: Optional[List[TableCleaningOperation]]
+        table_cleaning_operations: List[TableCleaningOperation]
             Table cleaning operations
         use_source_settings: bool
             Whether to use source table's table cleaning operations
 
         Returns
         -------
         dict[str, Node]
 
         Raises
         ------
         GraphInconsistencyError
             If the graph has unexpected structure
         """
+        query_graph = QueryGraph(**query_graph.dict(by_alias=True))
         node_name_to_replacement_node: dict[str, Node] = {}
         table_name_to_column_cleaning_operations: dict[str, list[ColumnCleaningOperation]] = {
             table_clean_op.table_name: table_clean_op.column_cleaning_operations
             for table_clean_op in table_cleaning_operations
         }
         # view node name to (view graph node, view columns_info & table input node)
         view_node_name_to_table_info: dict[str, tuple[Node, list[ColumnInfo], InputNode]] = {}
 
-        # prepare operation structure of nodes
-        operation_structure_info = OperationStructureExtractor(graph=query_graph).extract(
-            node=target_node,
-            keep_all_source_columns=True,
-        )
-        table_id_to_source_column_names = self._prepare_table_id_to_table_column_names(
-            operation_structure=operation_structure_info.operation_structure_map[target_node.name]
+        # prepare table ID to required source column names mapping
+        table_id_to_source_column_names = query_graph.extract_table_id_to_table_column_names(
+            node=target_node
         )
 
         # since the graph node is topologically sorted, input to the view graph node will always be
         # processed before the view graph node itself
         for graph_node in query_graph.iterate_sorted_graph_nodes(
             graph_node_types=GraphNodeType.view_graph_node_types()
         ):
```

### Comparing `featurebyte-0.3.1/featurebyte/service/working_schema.py` & `featurebyte-0.4.0/featurebyte/service/working_schema.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 """
 WorkingSchemaService class
 """
 from __future__ import annotations
 
-from typing import Any
-
 from bson import ObjectId
-from pydantic import PrivateAttr
 
 from featurebyte.logging import get_logger
-from featurebyte.models.feature import FeatureModel
+from featurebyte.models.base import User
 from featurebyte.persistent import Persistent
-from featurebyte.service.base_service import BaseService
 from featurebyte.service.feature import FeatureService
+from featurebyte.service.feature_manager import FeatureManagerService
 from featurebyte.service.online_enable import OnlineEnableService
-from featurebyte.service.task_manager import TaskManager
+from featurebyte.service.tile_registry_service import TileRegistryService
 from featurebyte.session.base import BaseSession, MetadataSchemaInitializer
 
 logger = get_logger(__name__)
 
 
 async def drop_all_objects(session: BaseSession) -> None:
     """
@@ -36,27 +33,32 @@
         except NotImplementedError:
             logger.info(f"drop_all_objects_in_working_schema not implemented for {session}")
             return
     else:
         return
 
 
-class WorkingSchemaService(BaseService):
+class WorkingSchemaService:
     """
     WorkingSchemaService is responsible for managing the working schema in the data warehouse
     """
 
-    _task_manager: TaskManager = PrivateAttr()
-
-    def __init__(self, user: Any, persistent: Persistent, catalog_id: ObjectId):
-        super().__init__(user, persistent, catalog_id)
-        self.feature_service = FeatureService(
-            user=user, persistent=persistent, catalog_id=catalog_id
-        )
-        self._task_manager = TaskManager(user=user, persistent=persistent, catalog_id=catalog_id)
+    def __init__(
+        self,
+        user: User,
+        persistent: Persistent,
+        feature_service: FeatureService,
+        feature_manager_service: FeatureManagerService,
+        tile_registry_service: TileRegistryService,
+    ):
+        self.user = user
+        self.persistent = persistent
+        self.feature_service = feature_service
+        self.feature_manager_service = feature_manager_service
+        self.tile_registry_service = tile_registry_service
 
     async def recreate_working_schema(
         self, feature_store_id: ObjectId, session: BaseSession
     ) -> None:
         """
         Resets the data warehouse working schema by dropping everything and recreating
 
@@ -68,14 +70,21 @@
             BaseSession object
         """
 
         # Drop everything in the working schema. It would be easier to drop the schema directly and
         # then recreate, but the assumption is that the account might not have this privilege.
         await drop_all_objects(session)
 
+        # Clear the tile registry because the tile tables in the data warehouse are now wiped out.
+        await self.persistent.delete_many(
+            collection_name=self.tile_registry_service.collection_name,
+            query_filter={"feature_store_id": ObjectId(feature_store_id)},
+            user_id=self.user.id,
+        )
+
         # Initialize working schema. This covers registering tables, functions and procedures.
         initializer = session.initializer()
         if not initializer:
             return
         await initializer.initialize()
 
         # Update feature store id in the metadata schema. This is typically done on creation of
@@ -88,21 +97,24 @@
         await self._reschedule_online_enabled_features(feature_store_id, session)
 
     async def _reschedule_online_enabled_features(
         self, feature_store_id: ObjectId, session: BaseSession
     ) -> None:
         # activate use of raw query filter to retrieve all documents regardless of catalog membership
         with self.feature_service.allow_use_raw_query_filter():
-            online_enabled_feature_docs = self.feature_service.list_documents_iterator(
-                query_filter={
-                    "tabular_source.feature_store_id": feature_store_id,
-                    "online_enabled": True,
-                },
+            query_filter = {
+                "tabular_source.feature_store_id": feature_store_id,
+                "online_enabled": True,
+            }
+            online_enabled_features = self.feature_service.list_documents_iterator(
+                query_filter=query_filter,
                 use_raw_query_filter=True,
             )
 
-            async for feature_doc in online_enabled_feature_docs:
-                logger.info(f'Rescheduling jobs for online enabled feature: {feature_doc["name"]}')
-                feature = FeatureModel(**feature_doc)
+            async for feature in online_enabled_features:
+                logger.info(f"Rescheduling jobs for online enabled feature: {feature.name}")
                 await OnlineEnableService.update_data_warehouse_with_session(
-                    session=session, feature=feature, task_manager=self._task_manager
+                    session=session,
+                    feature_manager_service=self.feature_manager_service,
+                    feature=feature,
+                    is_recreating_schema=True,
                 )
```

### Comparing `featurebyte-0.3.1/featurebyte/session/base.py` & `featurebyte-0.4.0/featurebyte/session/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 Session class
 """
 from __future__ import annotations
 
-from typing import Any, AsyncGenerator, ClassVar, List, Optional, OrderedDict
+from typing import Any, AsyncGenerator, ClassVar, Optional, OrderedDict
 
 import asyncio
 import contextvars
 import functools
 import os
 import time
 from abc import ABC, abstractmethod
@@ -22,18 +22,29 @@
 
 from featurebyte.common.path_util import get_package_root
 from featurebyte.common.utils import (
     create_new_arrow_stream_writer,
     dataframe_from_arrow_stream,
     pa_table_to_record_batches,
 )
-from featurebyte.enum import DBVarType, InternalName, SourceType, StrEnum
+from featurebyte.enum import (
+    DBVarType,
+    InternalName,
+    MaterializedTableNamePrefix,
+    SourceType,
+    StrEnum,
+)
 from featurebyte.exception import QueryExecutionTimeOut
 from featurebyte.logging import get_logger
-from featurebyte.query_graph.sql.common import get_fully_qualified_table_name, sql_to_string
+from featurebyte.models.user_defined_function import UserDefinedFunctionModel
+from featurebyte.query_graph.sql.common import (
+    get_fully_qualified_table_name,
+    quoted_identifier,
+    sql_to_string,
+)
 
 MINUTES_IN_SECONDS = 60
 HOUR_IN_SECONDS = 60 * MINUTES_IN_SECONDS
 DEFAULT_EXECUTE_QUERY_TIMEOUT_SECONDS = 10 * MINUTES_IN_SECONDS
 LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS = 24 * HOUR_IN_SECONDS
 
 
@@ -182,14 +193,34 @@
             Table name
 
         Returns
         -------
         OrderedDict[str, DBVarType]
         """
 
+    async def check_user_defined_function(
+        self,
+        user_defined_function: UserDefinedFunctionModel,
+        timeout: float = DEFAULT_EXECUTE_QUERY_TIMEOUT_SECONDS,
+    ) -> None:
+        """
+        Check if a user defined function exists in the feature store
+
+        Parameters
+        ----------
+        user_defined_function: UserDefinedFunctionModel
+            User defined function model
+        timeout: float
+            Timeout in seconds
+        """
+        await self.execute_query(
+            query=user_defined_function.generate_test_sql(source_type=self.source_type),
+            timeout=timeout,
+        )
+
     async def fetch_query_stream_impl(self, cursor: Any) -> AsyncGenerator[pa.RecordBatch, None]:
         """
         Stream pyarrow record batches from cursor
 
         Parameters
         ----------
         cursor : Any
@@ -518,22 +549,14 @@
         self.session = session
         self.metadata_schema_initializer = MetadataSchemaInitializer(session)
 
     @abstractmethod
     async def create_schema(self) -> None:
         """Create the featurebyte working schema"""
 
-    @abstractmethod
-    async def list_functions(self) -> list[str]:
-        """Retrieve list of functions in the working schema"""
-
-    @abstractmethod
-    async def list_procedures(self) -> list[str]:
-        """Retrieve list of procedures in the working schema"""
-
     @property
     @abstractmethod
     def sql_directory_name(self) -> str:
         """Directory name containing the SQL initialization scripts"""
 
     @property
     @abstractmethod
@@ -555,14 +578,27 @@
     @abstractmethod
     async def drop_all_objects_in_working_schema(self) -> None:
         """
         Reset state of working schema by dropping all existing objects (tables, functions,
         procedures, etc)
         """
 
+    @abstractmethod
+    async def drop_object(self, object_type: str, name: str) -> None:
+        """
+        Drop an object of a given type in the working schema
+
+        Parameters
+        ----------
+        object_type : str
+            Type of object to drop
+        name : str
+            Name of object to drop
+        """
+
     async def initialize(self) -> None:
         """Entry point to set up the featurebyte working schema"""
 
         if not await self.should_update_schema():
             return
 
         if not await self.schema_exists():
@@ -600,14 +636,30 @@
         bool
         """
         available_schemas = self._normalize_casings(
             await self.session.list_schemas(database_name=self.session.database_name)
         )
         return self._normalize_casing(self.session.schema_name) in available_schemas
 
+    async def list_objects(self, object_type: str) -> pd.DataFrame:
+        """
+        List objects of a given type in the working schema
+
+        Parameters
+        ----------
+        object_type : str
+            Type of object to list
+
+        Returns
+        -------
+        pd.DataFrame
+        """
+        query = f"SHOW {object_type} IN {self._schema_qualifier}"
+        return await self.session.execute_query(query)
+
     async def register_missing_functions(self, functions: list[dict[str, Any]]) -> None:
         """Register functions defined in the snowflake sql directory.
 
         Note that this will overwrite any existing functions. We should be look to handle this
         properly in the future (potentially by versioning) to prevent breaking changes from
         being released automatically.
 
@@ -735,23 +787,73 @@
             output.append(sql_object)
 
         return output
 
     @classmethod
     def _normalize_casing(cls, identifier: str) -> str:
         # Some database warehouses convert the names returned from list_tables(), list_schemas(),
-        # list_functions() etc to always upper case (Snowflake) or lower case (Databricks). To unify
+        # list_objects() etc to always upper case (Snowflake) or lower case (Databricks). To unify
         # the handling between different engines, this converts the identifiers used internally for
         # initialization purpose to be always upper case.
         return identifier.upper()
 
     @classmethod
     def _normalize_casings(cls, identifiers: list[str]) -> list[str]:
         return [cls._normalize_casing(x) for x in identifiers]
 
+    @classmethod
+    def remove_materialized_tables(cls, table_names: list[str]) -> list[str]:
+        """
+        Remove materialized tables from the list of table names
+
+        Parameters
+        ----------
+        table_names: list[str]
+            List of table names to filter
+
+        Returns
+        -------
+        list[str]
+        """
+        out = []
+        materialized_table_prefixes = {
+            cls._normalize_casing(name) for name in MaterializedTableNamePrefix.all()
+        }
+        for table_name in table_names:
+            for prefix in materialized_table_prefixes:
+                if cls._normalize_casing(table_name).startswith(prefix):
+                    break
+            else:
+                out.append(table_name)
+        return out
+
+    async def list_droppable_tables_in_working_schema(self) -> list[str]:
+        """
+        List tables in the working schema that can be dropped without losing data. These are the
+        tables that will be reinstated by WorkingSchemaService when recreating the working schema.
+
+        Returns
+        -------
+        list[str]
+        """
+        table_names = await self.session.list_tables(
+            self.session.database_name, self.session.schema_name
+        )
+        return self.remove_materialized_tables(table_names)
+
+    @property
+    def _schema_qualifier(self) -> str:
+        db_quoted = sql_to_string(
+            quoted_identifier(self.session.database_name), self.session.source_type
+        )
+        schema_quoted = sql_to_string(
+            quoted_identifier(self.session.schema_name), self.session.source_type
+        )
+        return f"{db_quoted}.{schema_quoted}"
+
 
 class MetadataSchemaInitializer:
     """Responsible for initializing the metadata schema table
     in the working schema.
 
     Parameters
     ----------
@@ -761,40 +863,34 @@
 
     SCHEMA_NOT_REGISTERED = -1
     SCHEMA_NO_RESULTS_FOUND = -2
 
     def __init__(self, session: BaseSession):
         self.session = session
 
-    def create_metadata_table_queries(self, current_migration_version: int) -> List[str]:
-        """Queries to create metadata table
+    async def create_metadata_table_if_not_exists(self, current_migration_version: int) -> None:
+        """Create metadata table if it doesn't exist
 
         Parameters
         ----------
         current_migration_version: int
             Current migration version
-
-        Returns
-        -------
-        List[str]
         """
-        return [
-            (
-                "CREATE TABLE IF NOT EXISTS METADATA_SCHEMA ( "
-                "WORKING_SCHEMA_VERSION INT, "
-                f"{InternalName.MIGRATION_VERSION} INT, "
-                "FEATURE_STORE_ID VARCHAR, "
-                "CREATED_AT TIMESTAMP DEFAULT SYSDATE() "
-                ") AS "
-                "SELECT 0 AS WORKING_SCHEMA_VERSION, "
-                f"{current_migration_version} AS {InternalName.MIGRATION_VERSION}, "
-                "NULL AS FEATURE_STORE_ID, "
-                "SYSDATE() AS CREATED_AT;"
-            )
-        ]
+        await self.session.execute_query(
+            "CREATE TABLE IF NOT EXISTS METADATA_SCHEMA ( "
+            "WORKING_SCHEMA_VERSION INT, "
+            f"{InternalName.MIGRATION_VERSION} INT, "
+            "FEATURE_STORE_ID VARCHAR, "
+            "CREATED_AT TIMESTAMP DEFAULT SYSDATE() "
+            ") AS "
+            "SELECT 0 AS WORKING_SCHEMA_VERSION, "
+            f"{current_migration_version} AS {InternalName.MIGRATION_VERSION}, "
+            "NULL AS FEATURE_STORE_ID, "
+            "SYSDATE() AS CREATED_AT;"
+        )
 
     async def update_metadata_schema_version(self, new_version: int) -> None:
         """Inserts default information into the metadata schema.
 
         Parameters
         ----------
         new_version : int
@@ -812,18 +908,16 @@
         from featurebyte.migration.run import (  # pylint: disable=import-outside-toplevel, cyclic-import
             retrieve_all_migration_methods,
         )
 
         current_migration_version = max(
             retrieve_all_migration_methods(data_warehouse_migrations_only=True)
         )
-        metadata_table_queries = self.create_metadata_table_queries(current_migration_version)
-        for query in metadata_table_queries:
-            await self.session.execute_query(query)
         logger.debug("Creating METADATA_SCHEMA table")
+        await self.create_metadata_table_if_not_exists(current_migration_version)
 
     async def update_feature_store_id(self, new_feature_store_id: str) -> None:
         """Inserts default information into the metadata schema.
 
         Parameters
         ----------
         new_feature_store_id : str
```

### Comparing `featurebyte-0.3.1/featurebyte/session/base_spark.py` & `featurebyte-0.4.0/featurebyte/session/base_spark.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 """
 BaseSparkSession class
 """
 from __future__ import annotations
 
-from typing import Any, List, Optional, cast
+from typing import Any, Optional, cast
 
 import os
 from abc import ABC
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import PrivateAttr
 
 from featurebyte import StorageType
 from featurebyte.common.path_util import get_package_root
 from featurebyte.enum import DBVarType, InternalName
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import (
+    AzureBlobStorageCredential,
     GCSStorageCredential,
     S3StorageCredential,
     StorageCredential,
 )
 from featurebyte.session.base import BaseSchemaInitializer, BaseSession, MetadataSchemaInitializer
 from featurebyte.session.simple_storage import (
+    AzureBlobStorage,
     FileMode,
     FileSimpleStorage,
     GCSStorage,
     S3SimpleStorage,
     SimpleStorage,
 )
 
@@ -102,14 +104,27 @@
                 raise NotImplementedError(
                     f"Unsupported storage credential for GCS: {self.storage_credential.__class__.__name__}"
                 )
             self._storage = GCSStorage(
                 storage_url=self.storage_url,
                 storage_credential=self.storage_credential,
             )
+        elif self.storage_type == StorageType.AZURE:
+            if self.storage_credential is None:
+                raise NotImplementedError("Storage credential is required for Azure Blob Storage")
+            if self.storage_credential is None or not isinstance(
+                self.storage_credential, AzureBlobStorageCredential
+            ):
+                raise NotImplementedError(
+                    f"Unsupported storage credential for Azure Blob Storage: {self.storage_credential.__class__.__name__}"
+                )
+            self._storage = AzureBlobStorage(
+                storage_url=self.storage_url,
+                storage_credential=self.storage_credential,
+            )
         else:
             raise NotImplementedError("Unsupported remote storage type")
 
     def test_storage_connection(self) -> None:
         """
         Test storage connection
         """
@@ -231,27 +246,39 @@
 
 class BaseSparkMetadataSchemaInitializer(MetadataSchemaInitializer):
     """BaseSpark metadata initializer class"""
 
     def __init__(self, session: BaseSparkSession):
         super().__init__(session)
 
-    def create_metadata_table_queries(self, current_migration_version: int) -> List[str]:
-        """Query to create metadata table
+    async def metadata_table_exists(self) -> bool:
+        """
+        Check if metadata table exists
+
+        Returns
+        -------
+        bool
+        """
+        try:
+            await self.session.execute_query("SELECT * FROM METADATA_SCHEMA")
+        except self.session._no_schema_error:  # pylint: disable=protected-access
+            return False
+        return True
+
+    async def create_metadata_table_if_not_exists(self, current_migration_version: int) -> None:
+        """Create metadata table if it doesn't exist
 
         Parameters
         ----------
         current_migration_version: int
             Current migration version
-
-        Returns
-        -------
-        List[str]
         """
-        return [
+        if await self.metadata_table_exists():
+            return
+        for query in [
             (
                 f"""
                 CREATE TABLE IF NOT EXISTS METADATA_SCHEMA (
                     WORKING_SCHEMA_VERSION INT,
                     {InternalName.MIGRATION_VERSION} INT,
                     FEATURE_STORE_ID STRING,
                     CREATED_AT TIMESTAMP
@@ -264,34 +291,42 @@
                 SELECT
                     0 AS WORKING_SCHEMA_VERSION,
                     {current_migration_version} AS {InternalName.MIGRATION_VERSION},
                     NULL AS FEATURE_STORE_ID,
                     CURRENT_TIMESTAMP() AS CREATED_AT
                 """
             ),
-        ]
+        ]:
+            await self.session.execute_query(query)
 
 
 class BaseSparkSchemaInitializer(BaseSchemaInitializer):
     """BaseSpark schema initializer class"""
 
     def __init__(self, session: BaseSparkSession):
         super().__init__(session=session)
         self.metadata_schema_initializer = BaseSparkMetadataSchemaInitializer(session)
 
     @property
     def current_working_schema_version(self) -> int:
-        return 1
+        return 2
 
     @property
     def sql_directory_name(self) -> str:
         return "spark"
 
     async def drop_all_objects_in_working_schema(self) -> None:
-        raise NotImplementedError()
+        if not await self.schema_exists():
+            return
+
+        for function in await self._list_functions():
+            await self.drop_object("FUNCTION", function)
+
+        for name in await self.list_droppable_tables_in_working_schema():
+            await self.drop_object("TABLE", name)
 
     @property
     def udf_jar_local_path(self) -> str:
         """
         Get path of udf jar file
 
         Returns
@@ -321,31 +356,34 @@
         udf_jar_file_name = os.path.basename(self.udf_jar_local_path)
         return f"{self.session.storage_spark_url}/{udf_jar_file_name}"  # type: ignore[attr-defined]
 
     async def create_schema(self) -> None:
         create_schema_query = f"CREATE SCHEMA {self.session.schema_name}"
         await self.session.execute_query(create_schema_query)
 
-    async def list_functions(self) -> list[str]:
+    async def drop_object(self, object_type: str, name: str) -> None:
+        query = f"DROP {object_type} {name}"
+        await self.session.execute_query(query)
+
+    async def list_objects(self, object_type: str) -> pd.DataFrame:
+        query = f"SHOW {object_type}"
+        return await self.session.execute_query(query)
+
+    async def _list_functions(self) -> list[str]:
         def _function_name_to_identifier(function_name: str) -> str:
             # function names returned from SHOW FUNCTIONS are three part fully qualified, but
             # identifiers are based on function names only
             return function_name.rsplit(".", 1)[1]
 
-        df_result = await self.session.execute_query(
-            f"SHOW USER FUNCTIONS IN {self.session.schema_name}"
-        )
+        df_result = await self.list_objects("USER FUNCTIONS")
         out = []
         if df_result is not None:
             out.extend(df_result["function"].apply(_function_name_to_identifier))
         return out
 
-    async def list_procedures(self) -> list[str]:
-        return []
-
     async def register_missing_objects(self) -> None:
         # check storage connection is working
         session = cast(BaseSparkSession, self.session)
         session.test_storage_connection()
 
         # upload jar file to storage
         udf_jar_file_name = os.path.basename(self.udf_jar_local_path)
```

### Comparing `featurebyte-0.3.1/featurebyte/session/databricks.py` & `featurebyte-0.4.0/featurebyte/session/databricks.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/session/hive.py` & `featurebyte-0.4.0/featurebyte/session/hive.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/session/manager.py` & `featurebyte-0.4.0/featurebyte/session/manager.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/session/simple_storage.py` & `featurebyte-0.4.0/featurebyte/session/simple_storage.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 from featurebyte.models.credential import (
     AzureBlobStorageCredential,
     GCSStorageCredential,
     S3StorageCredential,
     StorageCredential,
 )
+from featurebyte.session.webhdfs import webhdfs_delete, webhdfs_open
 
 FileMode = Literal["r", "w", "rb", "wb"]
 
 
 class SimpleStorage(ABC):
     """
     Base class for simple storage manager class
@@ -58,16 +59,16 @@
         return {}
 
     def test_connection(self) -> None:
         """
         Test connection to storage
         """
         conn_test_filename = f"_conn_test_{ObjectId()}"
-        with self.open(path=conn_test_filename, mode="w") as file_obj:
-            file_obj.write("OK")
+        with self.open(path=conn_test_filename, mode="wb") as file_obj:
+            file_obj.write(b"OK")
         self.delete_object(path=conn_test_filename)
 
     @abstractmethod
     def delete_object(self, path: str) -> None:
         """
         Delete object from storage
 
@@ -176,15 +177,15 @@
     ) -> None:
         super().__init__(storage_url=storage_url, storage_credential=storage_credential)
 
         self.client = GCSClient.from_service_account_info(
             info=storage_credential.service_account_info
         )
         protocol, path = storage_url.split("//")
-        assert protocol == "gs:"
+        assert protocol == "gs:", "GCSStorage: Protocol must be gs for storage_url"
         parts = path.split("/")
         if len(parts) == 0:
             raise ValueError("Bucket is missing in storage url")
         self.bucket = parts[0]
         if len(parts) > 1:
             self.key_prefix = "/".join(parts[1:])
             self.base_url = f"gs://{self.bucket}/{self.key_prefix}"
@@ -218,15 +219,15 @@
                 "EndpointSuffix=core.windows.net"
             ),
             credential=AzureNamedKeyCredential(
                 name=storage_credential.account_name, key=storage_credential.account_key
             ),
         )
         protocol, path = storage_url.split("//")
-        assert protocol == "azure:"
+        assert protocol == "azure:", "AzureBlobStorage: Protocol must be azure for storage_url"
         parts = path.split("/")
         if len(parts) == 0:
             raise ValueError("Container is missing in storage url")
         self.container = parts[0]
         if len(parts) > 1:
             self.key_prefix = "/".join(parts[1:])
             self.base_url = f"azure://{self.container}/{self.key_prefix}"
@@ -237,7 +238,52 @@
     def _get_transport_params(self) -> Dict[str, Any]:
         return {"client": self.client}
 
     def delete_object(self, path: str) -> None:
         path = path.rstrip("/")
         key = f"{self.key_prefix}/{path}" if self.key_prefix else path
         self.client.get_container_client(container=self.container).delete_blob(blob=key)  # type: ignore
+
+
+class WebHDFSStorage(SimpleStorage):
+    """
+    Simple WebHDFS storage class
+    """
+
+    def __init__(
+        self,
+        storage_url: str,
+        kerberos: bool = False,
+    ) -> None:
+        self.kerberos = kerberos
+        super().__init__(storage_url=storage_url)
+        protocol, path = storage_url.split("//")
+        assert protocol in {
+            "http:",
+            "https:",
+        }, "WebHDFS: protocol must be http or https for storage_url"
+        self.ssl = protocol == "https:"
+        parts = path.split("/")
+        if len(parts) == 0:
+            raise ValueError("HDFS hostname is missing in storage url")
+        self.hostname = parts[0]
+        if len(parts) > 1:
+            self.key_prefix = "/".join(parts[1:])
+            self.base_url = f"webhdfs://{self.hostname}/{self.key_prefix}"
+        else:
+            self.key_prefix = ""
+            self.base_url = f"webhdfs://{self.hostname}"
+
+    @contextmanager
+    def open(self, path: str, mode: FileMode) -> Any:
+        path = path.lstrip("/")
+        with webhdfs_open(
+            f"{self.base_url}/{path}",
+            mode=mode,
+            kerberos=self.kerberos,
+            ssl=self.ssl,
+        ) as file_obj:
+            yield file_obj
+
+    def delete_object(self, path: str) -> None:
+        path = path.rstrip("/")
+        webhdfs_delete(f"{self.base_url}/{path}", kerberos=self.kerberos, ssl=self.ssl)
```

### Comparing `featurebyte-0.3.1/featurebyte/session/snowflake.py` & `featurebyte-0.4.0/featurebyte/session/snowflake.py`

 * *Files 6% similar despite different names*

```diff
@@ -307,88 +307,50 @@
 
     @property
     def sql_directory_name(self) -> str:
         return "snowflake"
 
     @property
     def current_working_schema_version(self) -> int:
-        return 22
+        return 23
 
     async def create_schema(self) -> None:
         create_schema_query = f'CREATE SCHEMA "{self.session.schema_name}"'
         await self.session.execute_query(create_schema_query)
 
-    @property
-    def _schema_qualifier(self) -> str:
-        return f'"{self.session.database_name}"."{self.session.schema_name}"'
-
-    def _fully_qualified(self, name: str) -> str:
-        if "(" in name:
-            # handle functions with arguments, e.g. MY_UDF(a INT, b INT)
-            parts = name.split("(", 1)
-            name = f"{quoted_identifier(parts[0])}({parts[1]}"
-        else:
-            name = f"{quoted_identifier(name)}"
-        return f"{self._schema_qualifier}.{name}"
-
-    async def _list_objects(self, object_type: str) -> pd.DataFrame:
-        query = f"SHOW {object_type} IN SCHEMA {self._schema_qualifier}"
-        return await self.session.execute_query(query)
-
-    async def list_functions(self) -> list[str]:
-        df_result = await self._list_objects("USER FUNCTIONS")
-        out = []
-        if df_result is not None:
-            df_result = df_result[df_result["schema_name"] == self.session.schema_name]
-            out.extend(df_result["name"])
-        return out
-
-    async def list_procedures(self) -> list[str]:
-        df_result = await self._list_objects("PROCEDURES")
-        out = []
-        if df_result is not None:
-            df_result = df_result[df_result["schema_name"] == self.session.schema_name]
-            out.extend(df_result["name"])
-        return out
+    async def drop_object(self, object_type: str, name: str) -> None:
+        query = f"DROP {object_type} {self._fully_qualified(name)}"
+        await self.session.execute_query(query)
 
     @staticmethod
     def _format_arguments_to_be_droppable(arguments_list: list[str]) -> list[str]:
         return [arguments.split(" RETURN", 1)[0] for arguments in arguments_list]
 
-    async def _drop_object(self, object_type: str, name: str) -> None:
-        query = f"DROP {object_type} {self._fully_qualified(name)}"
-        await self.session.execute_query(query)
-
-    async def _drop_tasks(self) -> None:
-        tasks = await self._list_objects("TASKS")
-        while tasks.shape[0]:
-            # Drop tasks in a loop since new tasks might get added while the initial list of tasks
-            # are getting dropped (each shell task schedule new task as they are running)
-            for name in tasks["name"]:
-                await self._drop_object("TASK", name)
-            tasks = await self._list_objects("TASKS")
-
     async def drop_all_objects_in_working_schema(self) -> None:
         if not await self.schema_exists():
             return
 
-        objects = await self._list_objects("USER FUNCTIONS")
+        objects = await self.list_objects("USER FUNCTIONS")
         if objects.shape[0]:
             for func_name_with_args in self._format_arguments_to_be_droppable(
                 objects["arguments"].tolist()
             ):
-                await self._drop_object("FUNCTION", func_name_with_args)
+                await self.drop_object("FUNCTION", func_name_with_args)
 
-        objects = await self._list_objects("USER PROCEDURES")
+        objects = await self.list_objects("USER PROCEDURES")
         if objects.shape[0]:
             for func_name_with_args in self._format_arguments_to_be_droppable(
                 objects["arguments"].tolist()
             ):
-                await self._drop_object("PROCEDURE", func_name_with_args)
+                await self.drop_object("PROCEDURE", func_name_with_args)
 
-        await self._drop_tasks()
+        for name in await self.list_droppable_tables_in_working_schema():
+            await self.drop_object("TABLE", name)
 
-        table_names = await self.session.list_tables(
-            self.session.database_name, self.session.schema_name
-        )
-        for name in table_names:
-            await self._drop_object("TABLE", name)
+    def _fully_qualified(self, name: str) -> str:
+        if "(" in name:
+            # handle functions with arguments, e.g. MY_UDF(a INT, b INT)
+            parts = name.split("(", 1)
+            name = f"{quoted_identifier(parts[0])}({parts[1]}"
+        else:
+            name = f"{quoted_identifier(name)}"
+        return f"{self._schema_qualifier}.{name}"
```

### Comparing `featurebyte-0.3.1/featurebyte/session/spark.py` & `featurebyte-0.4.0/featurebyte/routes/app_container_config.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,256 +1,273 @@
 """
-SparkSession class
+App container config module.
+
+This contains all our registrations for dependency injection.
 """
-# pylint: disable=duplicate-code
 from __future__ import annotations
 
-from typing import Any, AsyncGenerator, Optional, OrderedDict
+from typing import Dict, List, Optional, Tuple
+
+import inspect
+from dataclasses import dataclass
+
+from featurebyte.models.base import CAMEL_CASE_TO_SNAKE_CASE_PATTERN
 
-import collections
+ARGS_AND_KWARGS = {"args", "kwargs"}
 
-import pandas as pd
-import pyarrow as pa
-from pandas.core.dtypes.common import is_datetime64_dtype, is_float_dtype
-from pyarrow import Schema
-from pydantic import Field
-from pyhive.exc import OperationalError
-from pyhive.hive import Cursor
 
-from featurebyte.enum import DBVarType, SourceType
-from featurebyte.logging import get_logger
-from featurebyte.session.base_spark import BaseSparkSession
-from featurebyte.session.hive import AuthType, HiveConnection
+def _get_class_name(class_name: str, name_override: Optional[str] = None) -> str:
+    """
+    Helper method to get a class name.
+
+    This method will convert a camel case formatted name, to a snake case formatted name.
 
-logger = get_logger(__name__)
+    Examples:
 
+    - `TestClass` -> `test_class`
+    - 'SCDTable' -> 'scd_table'
 
-class SparkSession(BaseSparkSession):
+    Parameters
+    ----------
+    class_name: type
+        class
+    name_override: str
+        name override
+
+    Returns
+    -------
+    str
+        name
     """
-    Spark session class
+    if name_override is not None:
+        return name_override
+    return CAMEL_CASE_TO_SNAKE_CASE_PATTERN.sub(r"_\1", class_name).lower()
+
+
+def _get_constructor_params_from_class(
+    class_: type, dependency_override: Optional[dict[str, str]] = None
+) -> List[str]:
     """
+    Helper method to get constructor params from class.
 
-    _no_schema_error = OperationalError
+    Parameters
+    ----------
+    class_: type
+        class
+
+    Returns
+    -------
+    list[str]
+        constructor params
+    """
+    sig = inspect.signature(class_.__init__)  # type: ignore[misc]
+    keys = list(sig.parameters.keys())
+    keys_to_skip = 1  # skip the `self` param
+    params = []
+    dep_override = dependency_override or {}
+    for key in keys[keys_to_skip:]:
+        # Skip args and kwargs parameters that appears in the default constructor.
+        if key in ARGS_AND_KWARGS:
+            continue
+        param_type = sig.parameters[key]
+        type_annotation = param_type.name
+        if isinstance(type_annotation, type):
+            type_annotation = type_annotation.__name__
+        class_name = _get_class_name(type_annotation)
+        if key in dep_override:
+            class_name = dep_override[key]
+        params.append(class_name)
+    return params
 
-    port: int
-    use_http_transport: bool
-    use_ssl: bool
-    access_token: Optional[str]
-    source_type: SourceType = Field(SourceType.SPARK, const=True)
-
-    def __init__(self, **data: Any) -> None:
-        super().__init__(**data)
-
-        auth = None
-        scheme = None
-
-        # determine transport scheme
-        if self.use_http_transport:
-            scheme = "https" if self.use_ssl else "http"
-
-        # determine auth mechanism
-        if self.access_token:
-            auth = AuthType.TOKEN
-
-        self._connection = HiveConnection(
-            host=self.host,
-            http_path=self.http_path,
-            catalog=self.database_name,
-            database=self.schema_name,
-            port=self.port,
-            access_token=self.access_token,
-            auth=auth,
-            scheme=scheme,
-        )
-        # Always use UTC for session timezone
-        cursor = self._connection.cursor()
-        cursor.execute("SET TIME ZONE 'UTC'")
-        cursor.close()
-
-    def __del__(self) -> None:
-        if self._connection:
-            self._connection.close()
-
-    @classmethod
-    def is_threadsafe(cls) -> bool:
-        return False
-
-    async def list_databases(self) -> list[str]:
-        databases = await self.execute_query("SHOW CATALOGS")
-        output = []
-        if databases is not None:
-            output.extend(databases["catalog"])
-        return output
-
-    async def list_schemas(self, database_name: str | None = None) -> list[str]:
-        schemas = await self.execute_query(f"SHOW SCHEMAS IN `{database_name}`")
-        output = []
-        if schemas is not None:
-            output.extend(schemas.get("namespace", schemas.get("databaseName")))
-            # in DataBricks the header is databaseName instead of namespace
-        return output
-
-    async def list_tables(
-        self, database_name: str | None = None, schema_name: str | None = None
-    ) -> list[str]:
-        tables = await self.execute_query(f"SHOW TABLES IN `{database_name}`.`{schema_name}`")
-        output = []
-        if tables is not None:
-            output.extend(tables["tableName"])
-        return output
 
-    async def list_table_schema(
-        self,
-        table_name: str | None,
-        database_name: str | None = None,
-        schema_name: str | None = None,
-    ) -> OrderedDict[str, DBVarType]:
-        schema = await self.execute_query(
-            f"DESCRIBE `{database_name}`.`{schema_name}`.`{table_name}`"
-        )
-        column_name_type_map = collections.OrderedDict()
-        if schema is not None:
-            for _, (column_name, var_info) in schema[["col_name", "data_type"]].iterrows():
-                # Sometimes describe include metadata after column details with and empty row as a separator.
-                # Skip the remaining entries once we run into an empty column name
-                if column_name == "":
-                    break
-                column_name_type_map[column_name] = self._convert_to_internal_variable_type(
-                    var_info.upper()
-                )
-        return column_name_type_map
+@dataclass
+class ClassDefinition:
+    """
+    Basic class definition
+    """
 
-    def _get_pyarrow_type(self, datatype: str) -> pa.types:
-        """
-        Get pyarrow type from Spark data type
+    # Note that we have a custom name, instead of using the name of the type directly.
+    # This allows us to provide overrides to the names, and also allows us to better support multiple classes with
+    # the same name.
+    name: str
+    class_: type
+    dependencies: List[str]
 
-        Parameters
-        ----------
-        datatype: str
-            Spark data type
 
-        Returns
-        -------
-        pa.types
+class AppContainerConfig:
+    """
+    App container config holds all the dependencies for our application.
+    """
+
+    def __init__(self) -> None:
+        self.classes_with_deps: List[ClassDefinition] = []
+        self.dependency_mapping: Dict[str, ClassDefinition] = {}
+
+    def get_class_def_mapping(self) -> Dict[str, ClassDefinition]:
+        """
+        Get class definitions, keyed by name.
         """
-        datatype = datatype.upper()
-        mapping = {
-            "STRING_TYPE": pa.string(),
-            "TINYINT_TYPE": pa.int8(),
-            "SMALLINT_TYPE": pa.int16(),
-            "INT_TYPE": pa.int32(),
-            "BIGINT_TYPE": pa.int64(),
-            "BINARY_TYPE": pa.large_binary(),
-            "BOOLEAN_TYPE": pa.bool_(),
-            "DATE_TYPE": pa.timestamp("ns", tz=None),
-            "TIME_TYPE": pa.time32("ms"),
-            "DOUBLE_TYPE": pa.float64(),
-            "FLOAT_TYPE": pa.float32(),
-            "DECIMAL_TYPE": pa.float64(),
-            "INTERVAL_TYPE": pa.duration("ns"),
-            "NULL_TYPE": pa.null(),
-            "TIMESTAMP_TYPE": pa.timestamp("ns", tz=None),
-            "ARRAY_TYPE": pa.string(),
-            "MAP_TYPE": pa.string(),
-            "STRUCT_TYPE": pa.string(),
-        }
-        if datatype.startswith("INTERVAL"):
-            pyarrow_type = pa.int64()
-        else:
-            pyarrow_type = mapping.get(datatype)
-
-        if not pyarrow_type:
-            # warn and fallback to string for unrecognized types
-            logger.warning("Cannot infer pyarrow type", extra={"datatype": datatype})
-            pyarrow_type = pa.string()
-        return pyarrow_type
+        # Return if already populated
+        if self.dependency_mapping:
+            return self.dependency_mapping
+
+        # Populate
+        for dep in self.classes_with_deps:
+            self.dependency_mapping[dep.name] = dep
+        return self.dependency_mapping
 
-    def _process_batch_data(self, data: pd.DataFrame, schema: Schema) -> pd.DataFrame:
+    def register_class(
+        self,
+        class_: type,
+        dependency_override: Optional[Dict[str, str]] = None,
+        name_override: Optional[str] = None,
+        force_no_deps: bool = False,
+    ) -> None:
         """
-        Process batch data before converting to PyArrow record batch.
+        Register a class, with dependencies if needed.
 
         Parameters
         ----------
-        data: pd.DataFrame
-            Data to process
-        schema: Schema
-            Schema of the data
+        class_: type
+            type we are registering
+        dependency_override: Optional[Dict[str, str]]
+            We will normally look up dependencies by the name of the variable specified in the constructor of the
+            class that we're registering. You can provide an override if you want to explicitly specify a class
+            that we should inject instead. This is common when trying to initialize a class that inherits a constructor
+            from a parent class, and the name in the constructor is something generic. For example, within our repo,
+            simple controllers will typically inherit a constructor that has a parameter called "service". This will
+            typically be the service that corresponds to the controller. However, since service is just a generic name,
+            we can provide an override from `service` -> `controllers_service` to tell the dependency injector to
+            look for `controllers_service` instead when trying to initialize this controller.
+        name_override: str
+            name override. The default name of this dependency is the class name, converted to snake case. If you
+            want to override the name, provide a name here.
+        force_no_deps: bool
+            force no dependencies. This should only be used for instances that are directly injected into the
+            instance_map, and are not constructed dynamically.
+        """
+        deps = _get_constructor_params_from_class(class_, dependency_override)
+        if force_no_deps:
+            deps = []
+        self.classes_with_deps.append(
+            ClassDefinition(
+                name=_get_class_name(class_.__name__, name_override),
+                class_=class_,
+                dependencies=deps,
+            )
+        )
 
-        Returns
-        -------
-        pd.DataFrame
-            Processed data
+    def _validate_duplicate_names(self) -> None:
+        """
+        Validate that there's no duplicate names registered.
+
+        Raises
+        ------
+        ValueError
+            raised when a name has been defined already.
         """
-        for i, column in enumerate(schema.names):
-            # Convert decimal columns to float
-            if schema.field(i).type == pa.float64() and not is_float_dtype(data[column]):
-                data[column] = data[column].astype(float)
-            elif isinstance(schema.field(i).type, pa.TimestampType) and not is_datetime64_dtype(
-                data[column]
-            ):
-                data[column] = pd.to_datetime(data[column])
-        return data
+        seen_names = set()
+        for definition in self.classes_with_deps:
+            definition_name = definition.name
+            if definition_name in seen_names:
+                raise ValueError(
+                    f"error creating dependency map. {definition_name} has been defined already. "
+                    "Consider changing the name of the dependency."
+                )
+            seen_names.add(definition_name)
 
-    def _read_batch(self, cursor: Cursor, schema: Schema, batch_size: int = 1000) -> pa.RecordBatch:
+    def _is_cyclic_dfs(
+        self,
+        class_def: ClassDefinition,
+        visited_nodes: dict[str, bool],
+        recursive_stack: dict[str, bool],
+        class_def_mapping: dict[str, ClassDefinition],
+    ) -> Tuple[bool, list[str]]:
         """
-        Fetch a batch of rows from a query result, returning them as a PyArrow record batch.
+        DFS helper function to detect circular dependencies.
 
         Parameters
         ----------
-        cursor: Cursor
-            Cursor to fetch data from
-        schema: Schema
-            Schema of the data to fetch
-        batch_size: int
-            Number of rows to fetch at a time
+        class_def: ClassDefinition
+            class definition we are currently visiting
+        visited_nodes: dict[str, bool]
+            dictionary of visited nodes
+        recursive_stack: dict[str, bool]
+            dictionary of nodes currently in the recursive stack
+        class_def_mapping: dict[str, ClassDefinition]
+            dictionary of class definitions
 
         Returns
         -------
-        pa.RecordBatch
-            None if no more rows are available
+        bool
+            True if there's a circular dependency, False otherwise.
+
+        Raises
+        ------
+        ValueError
         """
-        results = cursor.fetchmany(batch_size)
-        return pa.record_batch(
-            self._process_batch_data(
-                pd.DataFrame(results if results else None, columns=schema.names), schema
-            ),
-            schema=schema,
-        )
+        # Mark current node as visited and adds to recursion stack.
+        visited_nodes[class_def.name] = True
+        recursive_stack[class_def.name] = True
+
+        # Iterate through the dependencies
+        # If any dependency has been visited before, and is in the current recursive stack, the
+        # dependency graph is cyclic.
+        for neighbour_name in class_def_mapping[class_def.name].dependencies:
+            if neighbour_name not in class_def_mapping:
+                raise ValueError(
+                    f"Unable to find dependency {neighbour_name} in class_def_mappings for {class_def.name}. This is likely "
+                    "because we have either not registered the dependency, or the variable name of "
+                    "the dependency in the constructor isn't a snake case formatted name of the class "
+                    "you are trying to inject."
+                )
+            neighbour = class_def_mapping[neighbour_name]
+            if not visited_nodes.get(neighbour.name, False):
+                is_cyclic, path = self._is_cyclic_dfs(
+                    neighbour, visited_nodes, recursive_stack, class_def_mapping
+                )
+                if is_cyclic:
+                    return True, path
+            elif recursive_stack[neighbour.name]:
+                cyclic_path = list(recursive_stack.keys())
+                cyclic_path.append(neighbour.name)
+                return True, cyclic_path
+
+        # The node needs to be popped from stack before function ends
+        recursive_stack[class_def.name] = False
+        return False, []
 
-    def fetchall_arrow(self, cursor: Cursor) -> pa.Table:
+    def _validate_circular_dependencies(self) -> None:
         """
-        Fetch all (remaining) rows of a query result, returning them as a PyArrow table.
+        Validate that there are no circular dependencies.
 
-        Parameters
-        ----------
-        cursor: Cursor
-            Cursor to fetch data from
+        We do this by iterating through the graph dependencies in a DFS manner, and look for a back edge.
 
-        Returns
-        -------
-        pa.Table
+        Raises
+        ------
+        ValueError
+            raised when a circular dependency is detected.
         """
-        schema = pa.schema(
-            {metadata[0]: self._get_pyarrow_type(metadata[1]) for metadata in cursor.description}
-        )
-        record_batches = []
-        while True:
-            record_batch = self._read_batch(cursor, schema)
-            record_batches.append(record_batch)
-            if record_batch.num_rows == 0:
-                break
-        return pa.Table.from_batches(record_batches)
-
-    def fetch_query_result_impl(self, cursor: Any) -> pd.DataFrame | None:
-        arrow_table = self.fetchall_arrow(cursor)
-        return arrow_table.to_pandas()
-
-    async def fetch_query_stream_impl(self, cursor: Any) -> AsyncGenerator[pa.RecordBatch, None]:
-        # fetch results in batches
-        schema = pa.schema(
-            {metadata[0]: self._get_pyarrow_type(metadata[1]) for metadata in cursor.description}
-        )
-        while True:
-            record_batch = self._read_batch(cursor, schema)
-            yield record_batch
-            if record_batch.num_rows == 0:
-                break
+        class_def_mapping = self.get_class_def_mapping()
+        # Visited nodes keeps track of whether we have been to this node before.
+        visited_nodes: dict[str, bool] = {}
+        # Recursive stack keeps track of nodes that are currently being visited in the recursive call.
+        # This is to allow us to see if there's a back edge.
+        recursive_stack: dict[str, bool] = {}
+        for node in self.classes_with_deps:
+            # Only need to recurse on nodes we have not been to before.
+            if not visited_nodes.get(node.name, False):
+                is_cyclic, path = self._is_cyclic_dfs(
+                    node, visited_nodes, recursive_stack, class_def_mapping
+                )
+                if is_cyclic:
+                    path_str = " -> ".join(path)
+                    raise ValueError(
+                        f"There's a circular dependency in the dependency graph.\n{path_str}"
+                    )
+
+    def validate(self) -> None:
+        """
+        Validate the correctness of the config.
+        """
+        self._validate_duplicate_names()
+        self._validate_circular_dependencies()
```

### Comparing `featurebyte-0.3.1/featurebyte/session/sqlite.py` & `featurebyte-0.4.0/featurebyte/session/sqlite.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/common.py` & `featurebyte-0.4.0/featurebyte/sql/common.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_GET_RANK.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_GET_RANK.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql` & `featurebyte-0.4.0/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/sql/spark/featurebyte-hive-udf-1.0.3-SNAPSHOT-all.jar` & `featurebyte-0.4.0/featurebyte/sql/spark/featurebyte-hive-udf-1.0.3-SNAPSHOT-all.jar`

 * *Files 11% similar despite different names*

#### zipinfo {}

```diff
@@ -1,25 +1,25 @@
-Zip file size: 29690 bytes, number of entries: 23
-drwxr-xr-x  2.0 unx        0 b- defN 23-Jun-08 15:54 META-INF/
--rw-r--r--  2.0 unx       25 b- defN 23-Jun-08 15:54 META-INF/MANIFEST.MF
-drwxr-xr-x  2.0 unx        0 b- defN 23-Jun-08 15:54 com/
-drwxr-xr-x  2.0 unx        0 b- defN 23-Jun-08 15:54 com/featurebyte/
-drwxr-xr-x  2.0 unx        0 b- defN 23-Jun-08 15:54 com/featurebyte/hive/
-drwxr-xr-x  2.0 unx        0 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/
--rw-r--r--  2.0 unx     5593 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictRank.class
--rw-r--r--  2.0 unx     5161 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/ObjectAggregate$ObjectAggregatorEvaluator.class
--rw-r--r--  2.0 unx     2544 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/ObjectAggregate.class
--rw-r--r--  2.0 unx     5098 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictUDF.class
--rw-r--r--  2.0 unx     1070 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/ObjectAggregate$1.class
--rw-r--r--  2.0 unx     6348 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictCosineSimilarity.class
--rw-r--r--  2.0 unx     3769 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/TimezoneOffsetToSecond.class
--rw-r--r--  2.0 unx     2570 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictSingleStringArgumentUDF.class
--rw-r--r--  2.0 unx     2741 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/ObjectDelete.class
--rw-r--r--  2.0 unx     5565 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/IndexToTimestamp.class
--rw-r--r--  2.0 unx      963 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/ObjectAggregate$MapAggregationBuffer.class
--rw-r--r--  2.0 unx     5044 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/TimestampToIndex.class
--rw-r--r--  2.0 unx     3796 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictMostFrequent.class
--rw-r--r--  2.0 unx     2923 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictNumUnique.class
--rw-r--r--  2.0 unx     3721 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictEntropy.class
--rw-r--r--  2.0 unx     3869 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictRelativeFrequency.class
--rw-r--r--  2.0 unx     3730 b- defN 23-Jun-08 15:54 com/featurebyte/hive/udf/CountDictMostFrequentValue.class
-23 files, 64530 bytes uncompressed, 25488 bytes compressed:  60.5%
+Zip file size: 29714 bytes, number of entries: 23
+drwxr-xr-x  2.0 unx        0 b- defN 23-Jul-25 03:12 META-INF/
+-rw-r--r--  2.0 unx       25 b- defN 23-Jul-25 03:12 META-INF/MANIFEST.MF
+drwxr-xr-x  2.0 unx        0 b- defN 23-Jul-25 03:12 com/
+drwxr-xr-x  2.0 unx        0 b- defN 23-Jul-25 03:12 com/featurebyte/
+drwxr-xr-x  2.0 unx        0 b- defN 23-Jul-25 03:12 com/featurebyte/hive/
+drwxr-xr-x  2.0 unx        0 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/
+-rw-r--r--  2.0 unx     5044 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/TimestampToIndex.class
+-rw-r--r--  2.0 unx     3796 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictMostFrequent.class
+-rw-r--r--  2.0 unx     2923 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictNumUnique.class
+-rw-r--r--  2.0 unx     3730 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictMostFrequentValue.class
+-rw-r--r--  2.0 unx     5098 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictUDF.class
+-rw-r--r--  2.0 unx     2741 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/ObjectDelete.class
+-rw-r--r--  2.0 unx     2544 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/ObjectAggregate.class
+-rw-r--r--  2.0 unx     2570 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictSingleStringArgumentUDF.class
+-rw-r--r--  2.0 unx     3769 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/TimezoneOffsetToSecond.class
+-rw-r--r--  2.0 unx     3721 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictEntropy.class
+-rw-r--r--  2.0 unx     6348 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictCosineSimilarity.class
+-rw-r--r--  2.0 unx     5593 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictRank.class
+-rw-r--r--  2.0 unx     3884 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/CountDictRelativeFrequency.class
+-rw-r--r--  2.0 unx     5161 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/ObjectAggregate$ObjectAggregatorEvaluator.class
+-rw-r--r--  2.0 unx      963 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/ObjectAggregate$MapAggregationBuffer.class
+-rw-r--r--  2.0 unx     5565 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/IndexToTimestamp.class
+-rw-r--r--  2.0 unx     1070 b- defN 23-Jul-25 03:12 com/featurebyte/hive/udf/ObjectAggregate$1.class
+23 files, 64545 bytes uncompressed, 25512 bytes compressed:  60.5%
```

#### zipnote TEMP/diffoscope_to41xxjj_/tmp5kfxlf1q_.zip

```diff
@@ -12,59 +12,59 @@
 
 Filename: com/featurebyte/hive/
 Comment: 
 
 Filename: com/featurebyte/hive/udf/
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictRank.class
+Filename: com/featurebyte/hive/udf/TimestampToIndex.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectAggregate$ObjectAggregatorEvaluator.class
+Filename: com/featurebyte/hive/udf/CountDictMostFrequent.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectAggregate.class
+Filename: com/featurebyte/hive/udf/CountDictNumUnique.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictUDF.class
+Filename: com/featurebyte/hive/udf/CountDictMostFrequentValue.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectAggregate$1.class
+Filename: com/featurebyte/hive/udf/CountDictUDF.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictCosineSimilarity.class
+Filename: com/featurebyte/hive/udf/ObjectDelete.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/TimezoneOffsetToSecond.class
+Filename: com/featurebyte/hive/udf/ObjectAggregate.class
 Comment: 
 
 Filename: com/featurebyte/hive/udf/CountDictSingleStringArgumentUDF.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectDelete.class
+Filename: com/featurebyte/hive/udf/TimezoneOffsetToSecond.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/IndexToTimestamp.class
+Filename: com/featurebyte/hive/udf/CountDictEntropy.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectAggregate$MapAggregationBuffer.class
+Filename: com/featurebyte/hive/udf/CountDictCosineSimilarity.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/TimestampToIndex.class
+Filename: com/featurebyte/hive/udf/CountDictRank.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictMostFrequent.class
+Filename: com/featurebyte/hive/udf/CountDictRelativeFrequency.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictNumUnique.class
+Filename: com/featurebyte/hive/udf/ObjectAggregate$ObjectAggregatorEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictEntropy.class
+Filename: com/featurebyte/hive/udf/ObjectAggregate$MapAggregationBuffer.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictRelativeFrequency.class
+Filename: com/featurebyte/hive/udf/IndexToTimestamp.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictMostFrequentValue.class
+Filename: com/featurebyte/hive/udf/ObjectAggregate$1.class
 Comment: 
 
 Zip file comment:
```

#### com/featurebyte/hive/udf/CountDictRelativeFrequency.class

##### procyon -ec {}

```diff
@@ -35,15 +35,16 @@
     public Object evaluate(final GenericUDF.DeferredObject[] arguments) throws HiveException {
         if (arguments[0].get() == null || arguments[1].get() == null) {
             return null;
         }
         final Map<String, Object> counts = this.inputMapOI.getMap(arguments[0].get());
         final String key = this.getStringArgument(arguments);
         if (!counts.containsKey(key)) {
-            return null;
+            this.output.set(0.0);
+            return this.output;
         }
         final double keyValue = this.convertMapValueAsDouble(counts.get((Object)key));
         double total = 0.0;
         for (final Object value : counts.values()) {
             if (value != null) {
                 final double doubleValue = this.convertMapValueAsDouble(value);
                 if (Double.isNaN(doubleValue)) {
```

### Comparing `featurebyte-0.3.1/featurebyte/sql/tile_common.py` & `featurebyte-0.4.0/featurebyte/sql/tile_common.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,38 +1,30 @@
 """
 Base Class for Tile Schedule Instance
 """
-from typing import Any, List
+from typing import Any
 
 from abc import ABC, abstractmethod
 
+from featurebyte.models.tile import TileCommonParameters
 from featurebyte.session.base import BaseSession
-from featurebyte.sql.base import BaselSqlModel
+from featurebyte.sql.base import BaseSqlModel
 
 
-class TileCommon(BaselSqlModel, ABC):
+class TileCommon(TileCommonParameters, BaseSqlModel, ABC):
     """
     Base class for Tile Operation Classes
     """
 
-    tile_id: str
-    aggregation_id: str
-    tile_modulo_frequency_second: int
-    blind_spot_second: int
-    frequency_minute: int
-
-    sql: str
-    entity_column_names: List[str]
-    value_column_names: List[str]
-    value_column_types: List[str]
-
-    class Config:
-        """Model configuration"""
+    class Config(TileCommonParameters.Config):
+        """
+        Config class to allow services to be passed in as arguments
+        """
 
-        extra = "forbid"
+        arbitrary_types_allowed = True
 
     def __init__(self, session: BaseSession, **kwargs: Any):
         """
         Initialize Tile Operation Instance
 
         Parameters
         ----------
```

### Comparing `featurebyte-0.3.1/featurebyte/sql/tile_generate.py` & `featurebyte-0.4.0/featurebyte/sql/tile_generate.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,56 +2,56 @@
 Databricks Tile Generate Job Script
 """
 from typing import Optional
 
 import dateutil.parser
 
 from featurebyte.common import date_util
-from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
+from featurebyte.models.tile import TileType
+from featurebyte.service.tile_registry_service import TileRegistryService
 from featurebyte.sql.common import construct_create_table_query, retry_sql
 from featurebyte.sql.tile_common import TileCommon
 from featurebyte.sql.tile_registry import TileRegistry
 
 logger = get_logger(__name__)
 
 
 class TileGenerate(TileCommon):
     """
     Tile Generate script
     """
 
-    tile_type: str
+    tile_type: TileType
     last_tile_start_str: Optional[str]
+    tile_registry_service: TileRegistryService
 
     async def execute(self) -> None:
         """
         Execute tile generate operation
         """
         # pylint: disable=too-many-statements
         tile_table_exist_flag = await self.table_exists(self.tile_id)
-        logger.debug(f"tile_table_exist_flag: {tile_table_exist_flag}")
-
-        # 2. Update TILE_REGISTRY & Add New Columns TILE Table
-        tile_sql = self.sql.replace("'", "''")
 
         # pylint: disable=duplicate-code
         await TileRegistry(
             session=self._session,
-            sql=tile_sql,
+            sql=self.sql,
             table_name=self.tile_id,
             table_exist=tile_table_exist_flag,
-            tile_modulo_frequency_second=self.tile_modulo_frequency_second,
+            time_modulo_frequency_second=self.time_modulo_frequency_second,
             blind_spot_second=self.blind_spot_second,
             frequency_minute=self.frequency_minute,
             entity_column_names=self.entity_column_names,
             value_column_names=self.value_column_names,
             value_column_types=self.value_column_types,
             tile_id=self.tile_id,
             aggregation_id=self.aggregation_id,
+            feature_store_id=self.feature_store_id,
+            tile_registry_service=self.tile_registry_service,
         ).execute()
 
         tile_sql = self._construct_tile_sql_with_index()
 
         entity_insert_cols = []
         entity_filter_cols = []
         for element in self.entity_column_names:
@@ -70,26 +70,22 @@
             element = element.strip()
             value_insert_cols.append("b." + element)
             value_update_cols.append("a." + element + " = b." + element)
 
         value_insert_cols_str = ",".join(value_insert_cols)
         value_update_cols_str = ",".join(value_update_cols)
 
-        logger.debug(f"entity_insert_cols_str: {entity_insert_cols_str}")
-        logger.debug(f"entity_filter_cols_str: {entity_filter_cols_str}")
-        logger.debug(f"value_insert_cols_str: {value_insert_cols_str}")
-        logger.debug(f"value_update_cols_str: {value_update_cols_str}")
-
         # insert new records and update existing records
         if not tile_table_exist_flag:
             logger.debug(f"creating tile table: {self.tile_id}")
             create_sql = construct_create_table_query(self.tile_id, tile_sql, session=self._session)
             await retry_sql(self._session, create_sql)
             logger.debug(f"done creating table: {self.tile_id}")
         else:
+            logger.debug("merging into tile table", extra={"tile_id": self.tile_id})
             if self.entity_column_names:
                 on_condition_str = f"a.INDEX = b.INDEX AND {entity_filter_cols_str}"
                 insert_str = f"INDEX, {self.entity_column_names_str}, {self.value_column_names_str}, CREATED_AT"
                 values_str = f"b.INDEX, {entity_insert_cols_str}, {value_insert_cols_str}, current_timestamp()"
             else:
                 on_condition_str = "a.INDEX = b.INDEX"
                 insert_str = f"INDEX, {self.value_column_names_str}, CREATED_AT"
@@ -103,34 +99,33 @@
                     when not matched then
                         insert ({insert_str})
                             values ({values_str})
             """
             await retry_sql(session=self._session, sql=merge_sql)
 
         if self.last_tile_start_str:
-            logger.debug(f"last_tile_start_str: {self.last_tile_start_str}")
-
             ind_value = date_util.timestamp_utc_to_tile_index(
                 dateutil.parser.isoparse(self.last_tile_start_str),
-                self.tile_modulo_frequency_second,
+                self.time_modulo_frequency_second,
                 self.blind_spot_second,
                 self.frequency_minute,
             )
 
-            logger.debug(f"ind_value: {ind_value}")
+            logger.debug(
+                "Using specified last_tile_start_str",
+                extra={"last_tile_start_str": self.last_tile_start_str, "ind_value": ind_value},
+            )
 
-            update_tile_last_ind_sql = f"""
-                UPDATE TILE_REGISTRY
-                    SET
-                        LAST_TILE_INDEX_{self.tile_type} = {ind_value},
-                        {InternalName.TILE_LAST_START_DATE}_{self.tile_type} = to_timestamp('{self.last_tile_start_str}')
-                WHERE TILE_ID = '{self.tile_id}'
-                AND AGGREGATION_ID = '{self.aggregation_id}'
-            """
-            await retry_sql(self._session, update_tile_last_ind_sql)
+            await self.tile_registry_service.update_last_run_metadata(
+                tile_id=self.tile_id,
+                aggregation_id=self.aggregation_id,
+                tile_type=self.tile_type,
+                tile_index=ind_value,
+                tile_end_date=dateutil.parser.isoparse(self.last_tile_start_str),
+            )
 
     def _construct_tile_sql_with_index(self) -> str:
         if self.entity_column_names:
             entity_and_value_column_names_str = (
                 f"{self.entity_column_names_str}, {self.value_column_names_str}"
             )
         else:
```

### Comparing `featurebyte-0.3.1/featurebyte/sql/tile_generate_entity_tracking.py` & `featurebyte-0.4.0/featurebyte/sql/tile_generate_entity_tracking.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,21 +2,21 @@
 Tile Generate entity tracking Job script
 """
 from typing import Any, List
 
 from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
 from featurebyte.session.base import BaseSession
-from featurebyte.sql.base import BaselSqlModel
+from featurebyte.sql.base import BaseSqlModel
 from featurebyte.sql.common import construct_create_table_query, retry_sql
 
 logger = get_logger(__name__)
 
 
-class TileGenerateEntityTracking(BaselSqlModel):
+class TileGenerateEntityTracking(BaseSqlModel):
     """
     Tile Generate entity tracking script
     """
 
     entity_column_names: List[str]
     tile_id: str
     entity_table: str
```

### Comparing `featurebyte-0.3.1/featurebyte/sql/tile_generate_schedule.py` & `featurebyte-0.4.0/featurebyte/sql/tile_monitor.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,264 +1,199 @@
 """
-Tile Generate Schedule script
+Tile Monitor Job
 """
-from typing import Any, Dict, List, Optional
+import os
 
-from datetime import datetime, timedelta
+from sqlglot import expressions
 
-import dateutil.parser
-from pydantic import Field
-
-from featurebyte.common import date_util
 from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
-from featurebyte.sql.common import retry_sql
+from featurebyte.query_graph.sql.ast.literal import make_literal_value
+from featurebyte.query_graph.sql.common import get_qualified_column_identifier, sql_to_string
+from featurebyte.service.tile_registry_service import TileRegistryService
+from featurebyte.sql.common import construct_create_table_query, retry_sql
 from featurebyte.sql.tile_common import TileCommon
-from featurebyte.sql.tile_generate import TileGenerate
-from featurebyte.sql.tile_monitor import TileMonitor
-from featurebyte.sql.tile_schedule_online_store import TileScheduleOnlineStore
+from featurebyte.sql.tile_registry import TileRegistry
 
 logger = get_logger(__name__)
 
 
-class TileGenerateSchedule(TileCommon):
+class TileMonitor(TileCommon):
     """
-    Tile Generate Schedule script
+    Tile Monitor script
     """
 
-    offline_period_minute: int
+    monitor_sql: str
     tile_type: str
-    monitor_periods: int
-    job_schedule_ts: Optional[str] = Field(default=None)
+    tile_registry_service: TileRegistryService
 
-    # pylint: disable=too-many-locals,too-many-statements
     async def execute(self) -> None:
         """
-        Execute tile generate schedule operation
-
-        Raises
-        ------
-        Exception
-            Related exception from the triggered stored procedures if it fails
+        Execute tile monitor operation
         """
-        date_format = "%Y-%m-%d %H:%M:%S"
-        used_job_schedule_ts = self.job_schedule_ts or datetime.now().strftime(date_format)
-        candidate_last_tile_end_ts = dateutil.parser.isoparse(used_job_schedule_ts)
-
-        # derive the correct job schedule ts based on input job schedule ts
-        # the input job schedule ts might be between 2 intervals
-        last_tile_end_ts = self._derive_correct_job_ts(
-            candidate_last_tile_end_ts, self.frequency_minute, self.tile_modulo_frequency_second
-        )
-        logger.debug(
-            "Tile end ts details",
-            extra={
-                "last_tile_end_ts": last_tile_end_ts,
-                "candidate_last_tile_end_ts": candidate_last_tile_end_ts,
-            },
-        )
-
-        last_tile_end_ts = last_tile_end_ts - timedelta(seconds=self.blind_spot_second)
-        tile_type = self.tile_type.upper()
-        lookback_period = self.frequency_minute * (self.monitor_periods + 1)
-        tile_id = self.tile_id.upper()
-
-        tile_end_ts = last_tile_end_ts
-        if tile_type == "OFFLINE":
-            lookback_period = self.offline_period_minute
-            tile_end_ts = tile_end_ts - timedelta(minutes=lookback_period)
-
-        tile_start_ts = tile_end_ts - timedelta(minutes=lookback_period)
-        tile_start_ts_str = tile_start_ts.strftime(date_format)
-        monitor_tile_start_ts_str = tile_start_ts_str
-
-        # use the last_tile_start_date from tile registry as tile_start_ts_str if it is earlier than tile_start_ts_str
-        registry_df = await retry_sql(
-            self._session,
-            f"SELECT LAST_TILE_START_DATE_ONLINE FROM TILE_REGISTRY WHERE TILE_ID = '{self.tile_id}' AND LAST_TILE_START_DATE_ONLINE IS NOT NULL",
-        )
-
-        if registry_df is not None and len(registry_df) > 0:
-            registry_last_tile_start_ts = registry_df["LAST_TILE_START_DATE_ONLINE"].iloc[0]
-            logger.info(f"Last tile start date from registry - {registry_last_tile_start_ts}")
-
-            if registry_last_tile_start_ts.strftime(date_format) < tile_start_ts.strftime(
-                date_format
-            ):
-                logger.info(
-                    f"Use last tile start date from registry - {registry_last_tile_start_ts} instead of {tile_start_ts_str}"
+        # Disable tile monitoring for now since it is not yet user facing but the current
+        # implementation incurs significant cost when many features are deployed.
+        if not int(os.environ.get("FEATUREBYTE_TILE_MONITORING_ENABLED", "0")):
+            return
+
+        tile_table_exist_flag = await self.table_exists(self.tile_id)
+        logger.debug(f"tile_table_exist_flag: {tile_table_exist_flag}")
+
+        if not tile_table_exist_flag:
+            logger.info(f"tile table {self.tile_id} does not exist")
+        else:
+            tile_sql = self.monitor_sql.replace("'", "''")
+
+            await TileRegistry(
+                session=self._session,
+                sql=tile_sql,
+                table_name=self.tile_id,
+                table_exist=True,
+                time_modulo_frequency_second=self.time_modulo_frequency_second,
+                blind_spot_second=self.blind_spot_second,
+                frequency_minute=self.frequency_minute,
+                entity_column_names=self.entity_column_names,
+                value_column_names=self.value_column_names,
+                value_column_types=self.value_column_types,
+                tile_id=self.tile_id,
+                aggregation_id=self.aggregation_id,
+                feature_store_id=self.feature_store_id,
+                tile_registry_service=self.tile_registry_service,
+            ).execute()
+
+            new_tile_sql = f"""
+                select
+                    F_INDEX_TO_TIMESTAMP(
+                        INDEX,
+                        {self.time_modulo_frequency_second},
+                        {self.blind_spot_second},
+                        {self.frequency_minute}
+                    ) as {InternalName.TILE_START_DATE},
+                    INDEX,
+                    {self.entity_column_names_str},
+                    {self.value_column_names_str}
+                from ({self.monitor_sql})
+            """
+
+            entity_filter_cols_str = " AND ".join(
+                [
+                    f"a.{self.quote_column(c)} = b.{self.quote_column(c)}"
+                    for c in self.entity_column_names
+                ]
+            )
+            value_select_cols_str = " , ".join(
+                [f"b.{c} as OLD_{c}" for c in self.value_column_names]
+            )
+            value_filter_cols_str = " OR ".join(
+                [
+                    f"{c} != OLD_{c} or ({c} is not null and OLD_{c} is null)"
+                    for c in self.value_column_names
+                ]
+            )
+
+            offset_expr = expressions.Add(
+                this=expressions.Mul(
+                    this=make_literal_value(self.frequency_minute),
+                    expression=make_literal_value(60),
+                ),
+                expression=make_literal_value(self.blind_spot_second),
+            )
+            expected_created_at_expr = self.adapter.dateadd_microsecond(
+                quantity_expr=expressions.Mul(this=offset_expr, expression=make_literal_value(1e6)),
+                timestamp_expr=get_qualified_column_identifier(InternalName.TILE_START_DATE, "a"),
+            )
+            compare_sql = f"""
+                select * from
+                    (select
+                        a.*,
+                        {value_select_cols_str},
+                        cast('{self.tile_type}' as string) as TILE_TYPE,
+                        {sql_to_string(expected_created_at_expr, source_type=self._session.source_type)} as EXPECTED_CREATED_AT,
+                        current_timestamp() as CREATED_AT
+                    from
+                        ({new_tile_sql}) a left outer join {self.tile_id} b
+                    on
+                        a.INDEX = b.INDEX AND {entity_filter_cols_str})
+                where {value_filter_cols_str}
+            """
+
+            monitor_table_name = f"{self.tile_id}_MONITOR"
+            tile_monitor_exist_flag = await self.table_exists(monitor_table_name)
+            logger.debug(f"tile_monitor_exist_flag: {tile_monitor_exist_flag}")
+
+            if not tile_monitor_exist_flag:
+                create_sql = construct_create_table_query(
+                    monitor_table_name, compare_sql, session=self._session
                 )
-                tile_start_ts_str = registry_last_tile_start_ts.strftime(date_format)
-
-        session_id = f"{tile_id}|{datetime.now()}"
-        audit_insert_sql = f"""INSERT INTO TILE_JOB_MONITOR
-        (
-            TILE_ID,
-            AGGREGATION_ID,
-            TILE_TYPE,
-            SESSION_ID,
-            STATUS,
-            MESSAGE,
-            CREATED_AT
-        )
-            VALUES
-        (
-            '{tile_id}',
-            '{self.aggregation_id}',
-            '{tile_type}',
-            '{session_id}',
-            '<STATUS>',
-            '<MESSAGE>',
-            current_timestamp()
-        )"""
-        logger.debug(audit_insert_sql)
-
-        insert_sql = audit_insert_sql.replace("<STATUS>", "STARTED").replace("<MESSAGE>", "")
-        logger.debug(insert_sql)
-        await retry_sql(self._session, insert_sql)
-
-        monitor_end_ts = tile_end_ts - timedelta(minutes=self.frequency_minute)
-        monitor_tile_end_ts_str = monitor_end_ts.strftime(date_format)
-
-        monitor_input_sql = self.sql.replace(
-            f"{InternalName.TILE_START_DATE_SQL_PLACEHOLDER}", "'" + monitor_tile_start_ts_str + "'"
-        ).replace(
-            f"{InternalName.TILE_END_DATE_SQL_PLACEHOLDER}", "'" + monitor_tile_end_ts_str + "'"
-        )
-
-        tile_end_ts_str = tile_end_ts.strftime(date_format)
-        generate_input_sql = self.sql.replace(
-            f"{InternalName.TILE_START_DATE_SQL_PLACEHOLDER}", "'" + tile_start_ts_str + "'"
-        ).replace(f"{InternalName.TILE_END_DATE_SQL_PLACEHOLDER}", "'" + tile_end_ts_str + "'")
-
-        logger.info(
-            "Tile Schedule information",
-            extra={
-                "tile_id": tile_id,
-                "tile_start_ts_str": tile_start_ts_str,
-                "tile_end_ts_str": tile_end_ts_str,
-                "tile_type": tile_type,
-            },
-        )
-
-        tile_monitor_ins = TileMonitor(
-            session=self._session,
-            tile_id=tile_id,
-            tile_modulo_frequency_second=self.tile_modulo_frequency_second,
-            blind_spot_second=self.blind_spot_second,
-            frequency_minute=self.frequency_minute,
-            sql=generate_input_sql,
-            monitor_sql=monitor_input_sql,
-            entity_column_names=self.entity_column_names,
-            value_column_names=self.value_column_names,
-            value_column_types=self.value_column_types,
-            tile_type=self.tile_type,
-            aggregation_id=self.aggregation_id,
-        )
-
-        tile_generate_ins = TileGenerate(
-            session=self._session,
-            tile_id=tile_id,
-            tile_modulo_frequency_second=self.tile_modulo_frequency_second,
-            blind_spot_second=self.blind_spot_second,
-            frequency_minute=self.frequency_minute,
-            sql=generate_input_sql,
-            entity_column_names=self.entity_column_names,
-            value_column_names=self.value_column_names,
-            value_column_types=self.value_column_types,
-            tile_type=self.tile_type,
-            last_tile_start_str=tile_end_ts_str,
-            aggregation_id=self.aggregation_id,
-        )
-
-        tile_online_store_ins = TileScheduleOnlineStore(
-            session=self._session,
-            aggregation_id=self.aggregation_id,
-            job_schedule_ts_str=last_tile_end_ts.strftime(date_format),
-        )
-
-        step_specs: List[Dict[str, Any]] = [
-            {
-                "name": "tile_monitor",
-                "trigger": tile_monitor_ins,
-                "status": {
-                    "fail": "MONITORED_FAILED",
-                    "success": "MONITORED",
-                },
-            },
-            {
-                "name": "tile_generate",
-                "trigger": tile_generate_ins,
-                "status": {
-                    "fail": "GENERATED_FAILED",
-                    "success": "GENERATED",
-                },
-            },
-            {
-                "name": "tile_online_store",
-                "trigger": tile_online_store_ins,
-                "status": {
-                    "fail": "ONLINE_STORE_FAILED",
-                    "success": "COMPLETED",
-                },
-            },
-        ]
-
-        for spec in step_specs:
-            try:
-                logger.info(f"Calling {spec['name']}")
-                tile_ins: TileCommon = spec["trigger"]
-                await tile_ins.execute()
-                logger.info(f"End of calling {spec['name']}")
-            except Exception as exception:
-                message = str(exception).replace("'", "")
-                fail_code = spec["status"]["fail"]
-
-                ex_insert_sql = audit_insert_sql.replace("<STATUS>", fail_code).replace(
-                    "<MESSAGE>", message
+                await retry_sql(self._session, create_sql)
+            else:
+                tile_registry_ins = TileRegistry(
+                    session=self._session,
+                    sql=tile_sql,
+                    table_name=monitor_table_name,
+                    table_exist=True,
+                    time_modulo_frequency_second=self.time_modulo_frequency_second,
+                    blind_spot_second=self.blind_spot_second,
+                    frequency_minute=self.frequency_minute,
+                    entity_column_names=self.entity_column_names,
+                    value_column_names=self.value_column_names,
+                    value_column_types=self.value_column_types,
+                    tile_id=self.tile_id,
+                    aggregation_id=self.aggregation_id,
+                    feature_store_id=self.feature_store_id,
+                    tile_registry_service=self.tile_registry_service,
+                )
+                logger.info("Calling tile_registry.execute")
+                await tile_registry_ins.execute()
+                logger.info("End of calling tile_registry.execute")
+
+                # spark does not support insert with partial columns
+                # need to use merge for insertion
+                entity_column_names_str_src = " , ".join(
+                    [f"b.{c}" for c in self.entity_column_names_str.split(",")]
+                )
+                old_value_insert_cols_str_target = " , ".join(
+                    [f"OLD_{c}" for c in self.value_column_names]
+                )
+                value_insert_cols_str = " , ".join([f"b.{c}" for c in self.value_column_names])
+                old_value_insert_cols_str = " , ".join(
+                    [f"b.OLD_{c}" for c in self.value_column_names]
                 )
-                logger.error(f"fail_insert_sql exception: {exception}")
-                await retry_sql(self._session, ex_insert_sql)
-                raise exception
-
-            success_code = spec["status"]["success"]
-            insert_sql = audit_insert_sql.replace("<STATUS>", success_code).replace("<MESSAGE>", "")
-            await retry_sql(self._session, insert_sql)
-
-    def _derive_correct_job_ts(
-        self, input_dt: datetime, frequency_minutes: int, time_modulo_frequency_seconds: int
-    ) -> datetime:
-        """
-        Derive correct job schedule datetime
-
-        Parameters
-        ----------
-        input_dt: datetime
-            input job schedule datetime
-        frequency_minutes: int
-            frequency in minutes
-        time_modulo_frequency_seconds: int
-            time modulo frequency in seconds
-
-        Returns
-        -------
-        datetime
-        """
-        input_dt = input_dt.replace(tzinfo=None)
-
-        next_job_time = date_util.get_next_job_datetime(
-            input_dt=input_dt,
-            frequency_minutes=frequency_minutes,
-            time_modulo_frequency_seconds=time_modulo_frequency_seconds,
-        )
-
-        logger.debug(
-            "Inside derive_correct_job_ts",
-            extra={"next_job_time": next_job_time, "input_dt": input_dt},
-        )
-
-        if next_job_time == input_dt:
-            # if next_job_time is same as input_dt, then return next_job_time
-            return next_job_time
 
-        # if next_job_time is not same as input_dt, then return (next_job_time - frequency_minutes)
-        return next_job_time - timedelta(minutes=frequency_minutes)
+                insert_sql = f"""
+                    MERGE into {monitor_table_name} a using ({compare_sql}) b
+                        ON a.INDEX = b.INDEX AND a.CREATED_AT = b.CREATED_AT
+                    WHEN NOT MATCHED THEN
+                        INSERT
+                        (
+                            {InternalName.TILE_START_DATE},
+                            INDEX,
+                            {self.entity_column_names_str},
+                            {self.value_column_names_str},
+                            {old_value_insert_cols_str_target},
+                            TILE_TYPE,
+                            EXPECTED_CREATED_AT,
+                            CREATED_AT
+                        ) VALUES
+                        (
+                            b.{InternalName.TILE_START_DATE},
+                            b.INDEX,
+                            {entity_column_names_str_src},
+                            {value_insert_cols_str},
+                            {old_value_insert_cols_str},
+                            b.TILE_TYPE,
+                            b.EXPECTED_CREATED_AT,
+                            b.CREATED_AT
+                        )
+                """
+                await retry_sql(session=self._session, sql=insert_sql)
+
+            insert_monitor_summary_sql = f"""
+                INSERT INTO TILE_MONITOR_SUMMARY(TILE_ID, TILE_START_DATE, TILE_TYPE, CREATED_AT)
+                SELECT
+                    '{self.tile_id}' as TILE_ID,
+                    {InternalName.TILE_START_DATE} as TILE_START_DATE,
+                    TILE_TYPE,
+                    current_timestamp()
+                FROM ({compare_sql})
+            """
+            await retry_sql(self._session, insert_monitor_summary_sql)
```

### Comparing `featurebyte-0.3.1/featurebyte/storage/base.py` & `featurebyte-0.4.0/featurebyte/storage/base.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/storage/local.py` & `featurebyte-0.4.0/featurebyte/storage/local.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/tile/scheduler.py` & `featurebyte-0.4.0/featurebyte/service/tile_scheduler.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,83 +1,69 @@
 """
-FeatureByte Tile Scheduler
+TileSchedulerService class
 """
-from typing import Any, Optional
+from typing import Optional
 
 from bson import ObjectId
-from pydantic import BaseModel, PrivateAttr
 
+from featurebyte.models.base import User
 from featurebyte.models.periodic_task import Interval, PeriodicTask
+from featurebyte.models.tile import TileScheduledJobParameters
 from featurebyte.schema.worker.task.tile import TileTaskPayload
 from featurebyte.service.task_manager import TaskManager
 
 
-class TileScheduler(BaseModel):
+class TileSchedulerService:
     """
-    FeatureByte Scheduler using apscheduler
+    TileSchedulerService is responsible for scheduling tile tasks
     """
 
-    _task_manager: TaskManager = PrivateAttr()
-
-    def __init__(self, task_manager: TaskManager, **kw: Any) -> None:
-        """
-        Instantiate TileScheduler instance
-
-        Parameters
-        ----------
-        task_manager: TaskManager
-            Task Manager instance
-        kw: Any
-            constructor arguments
-        """
-        super().__init__(**kw)
-        self._task_manager = task_manager
+    def __init__(
+        self,
+        user: User,
+        catalog_id: ObjectId,
+        task_manager: TaskManager,
+    ):
+        self.user = user
+        self.catalog_id = catalog_id
+        self.task_manager = task_manager
 
     async def start_job_with_interval(
         self,
         job_id: str,
         interval_seconds: int,
         time_modulo_frequency_second: int,
-        instance: Any,
-        user_id: Optional[ObjectId],
+        parameters: TileScheduledJobParameters,
         feature_store_id: ObjectId,
-        catalog_id: ObjectId,
     ) -> None:
         """
         Start job with Interval seconds
 
         Parameters
         ----------
         job_id: str
             job id
         interval_seconds: int
             interval between runs
         time_modulo_frequency_second: int
             time modulo frequency in seconds
-        instance: Any
-            instance of the class to be run
-        user_id: Optional[ObjectId]
-            input user id
+        parameters: TileScheduledJobParameters
+            Tile scheduled job parameters
         feature_store_id: ObjectId
             feature store id
-        catalog_id: ObjectId
-            catalog id
         """
-
         payload = TileTaskPayload(
             name=job_id,
-            module_path=instance.__class__.__module__,
-            class_name=instance.__class__.__name__,
-            instance_str=instance.json(),
-            user_id=user_id if user_id else self._task_manager.user.id,
+            user_id=self.user.id,
             feature_store_id=feature_store_id,
-            catalog_id=catalog_id,
+            catalog_id=self.catalog_id,
+            parameters=parameters,
         )
 
-        await self._task_manager.schedule_interval_task(
+        await self.task_manager.schedule_interval_task(
             name=job_id,
             payload=payload,
             interval=Interval(every=interval_seconds, period="seconds"),
             time_modulo_frequency_second=time_modulo_frequency_second,
         )
 
     async def stop_job(self, job_id: str) -> None:
@@ -85,23 +71,23 @@
         Stop job
 
         Parameters
         ----------
         job_id: str
             job id to be stopped
         """
-        await self._task_manager.delete_periodic_task_by_name(job_id)
+        await self.task_manager.delete_periodic_task_by_name(job_id)
 
     async def get_job_details(self, job_id: str) -> Optional[PeriodicTask]:
         """
         Get Jobs from input job store
 
         Parameters
         ----------
         job_id: str
             job id
 
         Returns
         ----------
             Job Instance
         """
-        return await self._task_manager.get_periodic_task_by_name(name=job_id)
+        return await self.task_manager.get_periodic_task_by_name(name=job_id)
```

### Comparing `featurebyte-0.3.1/featurebyte/tile/tile_cache.py` & `featurebyte-0.4.0/featurebyte/tile/tile_cache.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,32 +4,38 @@
 from __future__ import annotations
 
 from typing import Callable, Optional, cast
 
 import time
 from dataclasses import dataclass
 
+from bson import ObjectId
 from sqlglot import expressions, parse_one
 from sqlglot.expressions import Expression, select
 
-from featurebyte.common.tile_util import tile_manager_from_session
 from featurebyte.enum import InternalName, SourceType, SpecialColumnName
 from featurebyte.logging import get_logger
 from featurebyte.models.tile import TileSpec
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
 from featurebyte.query_graph.sql.ast.datetime import TimedeltaExtractNode
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     apply_serving_names_mapping,
     quoted_identifier,
     sql_to_string,
 )
 from featurebyte.query_graph.sql.interpreter import GraphInterpreter, TileGenSql
+from featurebyte.query_graph.sql.tile_util import (
+    construct_entity_table_query,
+    get_earliest_tile_start_date_expr,
+    get_previous_job_epoch_expr,
+)
+from featurebyte.service.tile_manager import TileManagerService
 from featurebyte.session.base import BaseSession
 
 logger = get_logger(__name__)
 
 
 @dataclass
 class OnDemandTileComputeRequest:
@@ -37,17 +43,22 @@
 
     tile_table_id: str
     aggregation_id: str
     tracker_sql: str
     tile_compute_sql: str
     tile_gen_info: TileGenSql
 
-    def to_tile_manager_input(self) -> tuple[TileSpec, str]:
+    def to_tile_manager_input(self, feature_store_id: ObjectId) -> tuple[TileSpec, str]:
         """Returns a tuple required by FeatureListManager to compute tiles on-demand
 
+        Parameters
+        ----------
+        feature_store_id: ObjectId
+            Feature store id
+
         Returns
         -------
         tuple[TileSpec, str]
             Tuple of TileSpec and temp table name
         """
         entity_column_names = self.tile_gen_info.entity_columns[:]
         if self.tile_gen_info.value_by_column is not None:
@@ -60,30 +71,37 @@
             column_names=self.tile_gen_info.columns,
             entity_column_names=entity_column_names,
             value_column_names=self.tile_gen_info.tile_value_columns,
             value_column_types=self.tile_gen_info.tile_value_types,
             tile_id=self.tile_table_id,
             aggregation_id=self.aggregation_id,
             category_column_name=self.tile_gen_info.value_by_column,
+            feature_store_id=feature_store_id,
         )
         return tile_spec, self.tracker_sql
 
 
 class TileCache:
     """Responsible for on-demand tile computation for historical features
 
     Parameters
     ----------
     session : BaseSession
         Session object to interact with database
     """
 
-    def __init__(self, session: BaseSession):
+    def __init__(
+        self,
+        session: BaseSession,
+        tile_manager_service: TileManagerService,
+        feature_store_id: ObjectId,
+    ):
         self.session = session
-        self.tile_manager = tile_manager_from_session(session=session, task_manager=None)
+        self.tile_manager_service = tile_manager_service
+        self.feature_store_id = feature_store_id
         self._materialized_temp_table_names: set[str] = set()
 
     @property
     def adapter(self) -> BaseAdapter:
         """
         Returns an instance of adapter for engine specific SQL expressions generation
 
@@ -169,18 +187,18 @@
         required_requests : list[OnDemandTileComputeRequest]
             List of required compute requests (where entity table is non-empty)
         progress_callback: Optional[Callable[[int, str], None]]
             Optional progress callback function
         """
         tile_inputs = []
         for request in required_requests:
-            tile_input = request.to_tile_manager_input()
+            tile_input = request.to_tile_manager_input(feature_store_id=self.feature_store_id)
             tile_inputs.append(tile_input)
-        await self.tile_manager.generate_tiles_on_demand(
-            tile_inputs=tile_inputs, progress_callback=progress_callback
+        await self.tile_manager_service.generate_tiles_on_demand(
+            session=self.session, tile_inputs=tile_inputs, progress_callback=progress_callback
         )
 
     async def cleanup_temp_tables(self) -> None:
         """Drops all the temp tables that was created by TileCache"""
         for temp_table_name in self._materialized_temp_table_names:
             await self.session.execute_query(f"DROP TABLE IF EXISTS {temp_table_name}")
         self._materialized_temp_table_names = set()
@@ -518,38 +536,33 @@
         last_tile_start_date_expr = self._get_last_tile_start_date_expr(
             point_in_time_epoch_expr, tile_info
         )
         start_date_expr, end_date_expr = self._get_tile_start_end_date_expr(
             point_in_time_epoch_expr, tile_info
         )
 
-        # Tile compute sql uses original table columns instead of serving names
-        serving_names_to_keys = [
-            f"{quoted_identifier(serving_name).sql()} AS {quoted_identifier(col).sql()}"
-            for serving_name, col in zip(tile_info.serving_names, tile_info.entity_columns)
-        ]
-
-        # This is the groupby keys used to construct the entity table
-        serving_names = [f"{quoted_identifier(col).sql()}" for col in tile_info.serving_names]
-
+        # Entity table can be constructed from the working table by filtering for rows with outdated
+        # tiles that require recomputation
         tile_cache_working_table_name = (
             f"{InternalName.TILE_CACHE_WORKING_TABLE.value}_{request_id}"
         )
-        entity_table_expr = (
+        entity_source_expr = (
             select(
-                *serving_names_to_keys,
                 expressions.alias_(
                     last_tile_start_date_expr, InternalName.TILE_LAST_START_DATE.value
                 ),
-                expressions.alias_(end_date_expr, InternalName.ENTITY_TABLE_END_DATE.value),
-                expressions.alias_(start_date_expr, InternalName.ENTITY_TABLE_START_DATE.value),
             )
             .from_(tile_cache_working_table_name)
             .where(working_table_filter)
-            .group_by(*serving_names)
+        )
+        entity_table_expr = construct_entity_table_query(
+            tile_info=tile_info,
+            entity_source_expr=entity_source_expr,
+            start_date_expr=start_date_expr,
+            end_date_expr=end_date_expr,
         )
 
         tile_compute_sql = cast(
             str,
             tile_info.sql_template.render(
                 {
                     InternalName.ENTITY_TABLE_SQL_PLACEHOLDER: entity_table_expr.subquery(),
@@ -587,54 +600,14 @@
                 expressions.Max(this=point_in_time_identifier)
             )
         else:
             point_in_time_epoch_expr = self.adapter.to_epoch_seconds(point_in_time_identifier)
         return point_in_time_epoch_expr
 
     @staticmethod
-    def _get_previous_job_epoch_expr(
-        point_in_time_epoch_expr: Expression, tile_info: TileGenSql
-    ) -> Expression:
-        """Get the SQL expression for the epoch second of previous feature job
-
-        Parameters
-        ----------
-        point_in_time_epoch_expr : Expression
-            Expression for point-in-time in epoch second
-        tile_info : TileGenSql
-            Tile table information
-
-        Returns
-        -------
-        str
-        """
-        frequency = make_literal_value(tile_info.frequency)
-        time_modulo_frequency = make_literal_value(tile_info.time_modulo_frequency)
-
-        # FLOOR((POINT_IN_TIME - TIME_MODULO_FREQUENCY) / FREQUENCY)
-        previous_job_index_expr = expressions.Floor(
-            this=expressions.Div(
-                this=expressions.Paren(
-                    this=expressions.Sub(
-                        this=point_in_time_epoch_expr, expression=time_modulo_frequency
-                    )
-                ),
-                expression=frequency,
-            )
-        )
-
-        # PREVIOUS_JOB_INDEX * FREQUENCY + TIME_MODULO_FREQUENCY
-        previous_job_epoch_expr = expressions.Add(
-            this=expressions.Mul(this=previous_job_index_expr, expression=frequency),
-            expression=time_modulo_frequency,
-        )
-
-        return previous_job_epoch_expr
-
-    @staticmethod
     def _get_last_tile_start_date_expr(
         point_in_time_epoch_expr: Expression, tile_info: TileGenSql
     ) -> Expression:
         """Get the SQL expression for the "last tile start date" corresponding to the point-in-time
 
         Parameters
         ----------
@@ -644,17 +617,15 @@
             Tile table information
 
         Returns
         -------
         Expression
         """
         # Convert point in time to feature job time, then last tile start date
-        previous_job_epoch_expr = TileCache._get_previous_job_epoch_expr(
-            point_in_time_epoch_expr, tile_info
-        )
+        previous_job_epoch_expr = get_previous_job_epoch_expr(point_in_time_epoch_expr, tile_info)
         blind_spot = make_literal_value(tile_info.blind_spot)
         frequency = make_literal_value(tile_info.frequency)
 
         # TO_TIMESTAMP(PREVIOUS_JOB_EPOCH_EXPR - BLIND_SPOT - FREQUENCY
         last_tile_start_date_expr = expressions.Anonymous(
             this="TO_TIMESTAMP",
             expressions=[
@@ -681,47 +652,39 @@
         tile_info : TileGenSql
             Tile table information
 
         Returns
         -------
         Tuple[Expression, Expression]
         """
-        previous_job_epoch_expr = self._get_previous_job_epoch_expr(
-            point_in_time_epoch_expr, tile_info
-        )
+        previous_job_epoch_expr = get_previous_job_epoch_expr(point_in_time_epoch_expr, tile_info)
         frequency = make_literal_value(tile_info.frequency)
         blind_spot = make_literal_value(tile_info.blind_spot)
         time_modulo_frequency = make_literal_value(tile_info.time_modulo_frequency)
 
         # TO_TIMESTAMP(PREVIOUS_JOB_EPOCH - BLIND_SPOT)
         end_date_expr = expressions.Anonymous(
             this="TO_TIMESTAMP",
             expressions=[expressions.Sub(this=previous_job_epoch_expr, expression=blind_spot)],
         )
-
-        # DATEADD(s, TIME_MODULO_FREQUENCY - BLIND_SPOT, CAST('1970-01-01' AS TIMESTAMP))
-        tile_boundaries_offset = expressions.Paren(
-            this=expressions.Sub(this=time_modulo_frequency, expression=blind_spot)
-        )
-        tile_boundaries_offset_microsecond = TimedeltaExtractNode.convert_timedelta_unit(
-            tile_boundaries_offset, "second", "microsecond"
-        )
-        frequency_microsecond = TimedeltaExtractNode.convert_timedelta_unit(
-            frequency, "second", "microsecond"
-        )
-        earliest_start_date_expr = self.adapter.dateadd_microsecond(
-            tile_boundaries_offset_microsecond,
-            cast(Expression, parse_one("CAST('1970-01-01' AS TIMESTAMP)")),
+        earliest_start_date_expr = get_earliest_tile_start_date_expr(
+            adapter=self.adapter,
+            time_modulo_frequency=time_modulo_frequency,
+            blind_spot=blind_spot,
         )
+
         # This expression will be evaluated in a group by statement with the entity value as the
         # group by key. We can use ANY_VALUE because the recorded last tile start date is the same
         # across all rows within the group.
         recorded_last_tile_start_date_expr = self.adapter.any_value(
             expressions.Identifier(this=tile_info.aggregation_id)
         )
+        frequency_microsecond = TimedeltaExtractNode.convert_timedelta_unit(
+            frequency, "second", "microsecond"
+        )
         start_date_expr = expressions.Case(
             ifs=[
                 expressions.If(
                     this=expressions.Is(
                         this=recorded_last_tile_start_date_expr, expression=expressions.Null()
                     ),
                     true=earliest_start_date_expr,
```

### Comparing `featurebyte-0.3.1/featurebyte/utils/credential.py` & `featurebyte-0.4.0/featurebyte/utils/credential.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/utils/messaging.py` & `featurebyte-0.4.0/featurebyte/utils/messaging.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/utils/persistent.py` & `featurebyte-0.4.0/featurebyte/utils/persistent.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/progress.py` & `featurebyte-0.4.0/featurebyte/worker/progress.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/schedulers.py` & `featurebyte-0.4.0/featurebyte/worker/schedulers.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/base.py` & `featurebyte-0.4.0/featurebyte/worker/task/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 """
 Base models for task and task payload
 """
 from __future__ import annotations
 
-from typing import Any, Dict, cast
+from typing import Any, Dict, Optional
 
 from abc import abstractmethod
 from enum import Enum
 
-from featurebyte.routes.app_container import AppContainer
+from featurebyte.routes.lazy_app_container import LazyAppContainer
+from featurebyte.routes.registry import app_container_config
 from featurebyte.schema.worker.progress import ProgressModel
 from featurebyte.schema.worker.task.base import BaseTaskPayload
-from featurebyte.service.task_manager import TaskManager
 
 TASK_MAP: Dict[Enum, type[BaseTask]] = {}
 
 
-class BaseTask:
+class BaseTask:  # pylint: disable=too-many-instance-attributes
     """
     Base class for Task
     """
 
     payload_class: type[BaseTaskPayload] = BaseTaskPayload
 
     def __init__(
@@ -28,25 +28,27 @@
         payload: dict[str, Any],
         progress: Any,
         user: Any,
         get_persistent: Any,
         get_storage: Any,
         get_temp_storage: Any,
         get_credential: Any,
+        get_celery: Any,
     ):
         if self.payload_class == BaseTaskPayload:
             raise NotImplementedError
         self.payload = self.payload_class(**payload)
         self.user = user
         self.get_persistent = get_persistent
         self.get_storage = get_storage
         self.get_temp_storage = get_temp_storage
         self.get_credential = get_credential
+        self.get_celery = get_celery
         self.progress = progress
-        self._app_container = None
+        self._app_container: Optional[LazyAppContainer] = None
 
     def __init_subclass__(cls, **kwargs: Any) -> None:
         super().__init_subclass__(**kwargs)
 
         assert isinstance(cls.payload_class.command, Enum)
         command = cls.payload_class.command
         if command in TASK_MAP:
@@ -65,35 +67,32 @@
             Optional message
         """
         if self.progress:
             progress = ProgressModel(percent=percent, message=message)
             self.progress.put(progress.dict(exclude_none=True))
 
     @property
-    def app_container(self) -> AppContainer:
+    def app_container(self) -> LazyAppContainer:
         """
         Get an AppContainer instance
 
         Returns
         -------
-        AppContainer
+        LazyAppContainer
         """
         if self._app_container is None:
-            self._app_container = AppContainer.get_instance(
+            self._app_container = LazyAppContainer(
                 user=self.user,
                 persistent=self.get_persistent(),
                 temp_storage=self.get_temp_storage(),
-                task_manager=TaskManager(
-                    user=self.user,
-                    persistent=self.get_persistent(),
-                    catalog_id=self.payload.catalog_id,
-                ),
+                celery=self.get_celery(),
                 storage=self.get_storage(),
-                container_id=self.payload.catalog_id,
+                catalog_id=self.payload.catalog_id,
+                app_container_config=app_container_config,
             )
-        return cast(AppContainer, self._app_container)
+        return self._app_container
 
     @abstractmethod
     async def execute(self) -> Any:
         """
         Execute the task
         """
```

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/batch_feature_table.py` & `featurebyte-0.4.0/featurebyte/worker/task/batch_feature_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/batch_request_table.py` & `featurebyte-0.4.0/featurebyte/worker/task/batch_request_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/deployment_create_update.py` & `featurebyte-0.4.0/featurebyte/worker/task/deployment_create_update.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/feature_job_setting_analysis.py` & `featurebyte-0.4.0/featurebyte/worker/task/feature_job_setting_analysis.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,16 +17,14 @@
     FeatureJobSettingAnalysisData,
     FeatureJobSettingAnalysisModel,
 )
 from featurebyte.schema.worker.task.feature_job_setting_analysis import (
     FeatureJobSettingAnalysisBackTestTaskPayload,
     FeatureJobSettingAnalysisTaskPayload,
 )
-from featurebyte.service.event_table import EventTableService
-from featurebyte.service.feature_job_setting_analysis import FeatureJobSettingAnalysisService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.session.manager import SessionManager
 from featurebyte.worker.task.base import BaseTask
 
 logger = get_logger(__name__)
 
 
@@ -42,17 +40,15 @@
         Execute the task
         """
         self.update_progress(percent=0, message="Preparing data")
         payload = cast(FeatureJobSettingAnalysisTaskPayload, self.payload)
         persistent = self.get_persistent()
 
         # retrieve event data
-        event_table_service = EventTableService(
-            user=self.user, persistent=persistent, catalog_id=self.payload.catalog_id
-        )
+        event_table_service = self.app_container.event_table_service
         event_table = await event_table_service.get_document(document_id=payload.event_table_id)
 
         # retrieve feature store
         feature_store_service = FeatureStoreService(
             user=self.user, persistent=persistent, catalog_id=self.payload.catalog_id
         )
         feature_store = await feature_store_service.get_document(
@@ -77,15 +73,15 @@
             event_timestamp_column=event_table.event_timestamp_column,
             sql_query_func=db_session.execute_query,
         )
 
         self.update_progress(percent=5, message="Running Analysis")
         analysis = await create_feature_job_settings_analysis(
             event_dataset=event_dataset,
-            **payload.json_dict(),
+            **payload.dict(by_alias=True),
         )
 
         # store analysis doc in persistent
         analysis_doc = FeatureJobSettingAnalysisModel(
             _id=payload.output_document_id,
             user_id=payload.user_id,
             name=payload.name,
@@ -93,16 +89,16 @@
             analysis_options=analysis.analysis_options.dict(),
             analysis_parameters=analysis.analysis_parameters.dict(),
             analysis_result=analysis.analysis_result.dict(),
             analysis_report=analysis.to_html(),
         )
 
         self.update_progress(percent=95, message="Saving Analysis")
-        feature_job_settings_analysis_service = FeatureJobSettingAnalysisService(
-            user=self.user, persistent=persistent, catalog_id=self.payload.catalog_id
+        feature_job_settings_analysis_service = (
+            self.app_container.feature_job_setting_analysis_service
         )
         analysis_doc = await feature_job_settings_analysis_service.create_document(
             data=analysis_doc
         )
         assert analysis_doc.id == payload.output_document_id
 
         # store analysis data in storage
@@ -127,19 +123,18 @@
 
     async def execute(self) -> None:
         """
         Execute the task
         """
         self.update_progress(percent=0, message="Preparing table")
         payload = cast(FeatureJobSettingAnalysisBackTestTaskPayload, self.payload)
-        persistent = self.get_persistent()
 
         # retrieve analysis doc from persistent
-        feature_job_settings_analysis_service = FeatureJobSettingAnalysisService(
-            user=self.user, persistent=persistent, catalog_id=self.payload.catalog_id
+        feature_job_settings_analysis_service = (
+            self.app_container.feature_job_setting_analysis_service
         )
         analysis_doc = await feature_job_settings_analysis_service.get_document(
             document_id=payload.feature_job_setting_analysis_id
         )
         document = analysis_doc.dict(by_alias=True)
 
         # retrieve analysis data from storage
```

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/historical_feature_table.py` & `featurebyte-0.4.0/featurebyte/worker/task/historical_feature_table.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 """
 HistoricalFeatureTable creation task
 """
 from __future__ import annotations
 
 from typing import Any, cast
 
-from pathlib import Path
-
 from featurebyte.logging import get_logger
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.schema.worker.task.historical_feature_table import (
     HistoricalFeatureTableTaskPayload,
 )
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
-from featurebyte.service.observation_table import ObservationTableService
-from featurebyte.service.preview import PreviewService
+from featurebyte.service.historical_features import HistoricalFeaturesService
 from featurebyte.worker.task.base import BaseTask
 from featurebyte.worker.task.mixin import DataWarehouseMixin
+from featurebyte.worker.util.observation_set_helper import ObservationSetHelper
 
 logger = get_logger(__name__)
 
 
 class HistoricalFeatureTableTask(DataWarehouseMixin, BaseTask):
     """
     HistoricalFeatureTableTask creates a HistoricalFeatureTable by computing historical features
@@ -34,46 +32,35 @@
         """
         payload = cast(HistoricalFeatureTableTaskPayload, self.payload)
         feature_store = await self.app_container.feature_store_service.get_document(
             document_id=payload.feature_store_id
         )
         db_session = await self.get_db_session(feature_store)
 
-        app_container = self.app_container
-
-        if payload.observation_table_id is not None:
-            # ObservationTable as observation set
-            assert payload.observation_set_storage_path is None
-            observation_table_service: ObservationTableService = (
-                app_container.observation_table_service
-            )
-            observation_set = await observation_table_service.get_document(
-                payload.observation_table_id
-            )
-        else:
-            # In-memory DataFrame as observation set
-            assert payload.observation_set_storage_path is not None
-            observation_set = await self.get_temp_storage().get_dataframe(
-                Path(payload.observation_set_storage_path)
-            )
+        observation_set_helper: ObservationSetHelper = self.app_container.observation_set_helper
+        observation_set = await observation_set_helper.get_observation_set(
+            payload.observation_table_id, payload.observation_set_storage_path
+        )
 
         historical_feature_table_service: HistoricalFeatureTableService = (
-            app_container.historical_feature_table_service
+            self.app_container.historical_feature_table_service
         )
         location = await historical_feature_table_service.generate_materialized_table_location(
             self.get_credential, payload.feature_store_id
         )
 
         async with self.drop_table_on_error(
             db_session=db_session, table_details=location.table_details
         ):
-            preview_service: PreviewService = app_container.preview_service
-            await preview_service.compute_historical_features(
+            historical_features_service: HistoricalFeaturesService = (
+                self.app_container.historical_features_service
+            )
+            await historical_features_service.compute(
                 observation_set=observation_set,
-                featurelist_get_historical_features=payload.featurelist_get_historical_features,
+                compute_request=payload.featurelist_get_historical_features,
                 get_credential=self.get_credential,
                 output_table_details=location.table_details,
                 progress_callback=self.update_progress,
             )
             (
                 columns_info,
                 num_rows,
```

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/materialized_table_delete.py` & `featurebyte-0.4.0/featurebyte/worker/task/materialized_table_delete.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,16 +7,17 @@
 
 from featurebyte.models.materialized_table import MaterializedTableModel
 from featurebyte.schema.worker.task.materialized_table_delete import (
     MaterializedTableCollectionName,
     MaterializedTableDeleteTaskPayload,
 )
 from featurebyte.service.validator.materialized_table_delete import (
+    ObservationTableDeleteValidator,
     check_delete_batch_request_table,
-    check_delete_observation_table,
+    check_delete_static_source_table,
 )
 from featurebyte.worker.task.base import BaseTask
 from featurebyte.worker.task.mixin import DataWarehouseMixin
 
 
 class MaterializedTableDeleteTask(DataWarehouseMixin, BaseTask):
     """
@@ -53,18 +54,19 @@
         )
         await self.app_container.batch_feature_table_service.delete_document(
             document_id=document.id
         )
         return cast(MaterializedTableModel, document)
 
     async def _delete_observation_table(self) -> MaterializedTableModel:
-        document = await check_delete_observation_table(
-            observation_table_service=self.app_container.observation_table_service,
-            historical_feature_table_service=self.app_container.historical_feature_table_service,
-            document_id=self.task_payload.document_id,
+        validator: ObservationTableDeleteValidator = (
+            self.app_container.observation_table_delete_validator
+        )
+        document = await validator.check_delete_observation_table(
+            observation_table_id=self.task_payload.document_id,
         )
         await self.app_container.observation_table_service.delete_document(
             document_id=self.task_payload.document_id
         )
         return cast(MaterializedTableModel, document)
 
     async def _delete_historical_feature_table(self) -> MaterializedTableModel:
@@ -72,24 +74,44 @@
             document_id=self.task_payload.document_id
         )
         await self.app_container.historical_feature_table_service.delete_document(
             document_id=document.id
         )
         return cast(MaterializedTableModel, document)
 
+    async def _delete_target_table(self) -> MaterializedTableModel:
+        document = await self.app_container.target_table_service.get_document(
+            document_id=self.task_payload.document_id
+        )
+        await self.app_container.target_table_service.delete_document(document_id=document.id)
+        return cast(MaterializedTableModel, document)
+
+    async def _delete_static_source_table(self) -> MaterializedTableModel:
+        document = await check_delete_static_source_table(
+            static_source_table_service=self.app_container.static_source_table_service,
+            table_service=self.app_container.table_service,
+            document_id=self.task_payload.document_id,
+        )
+        await self.app_container.static_source_table_service.delete_document(
+            document_id=self.task_payload.document_id
+        )
+        return cast(MaterializedTableModel, document)
+
     async def execute(self) -> Any:
         """
         Execute Deployment Create & Update Task
         """
         # table to delete action mapping
         table_to_delete_action = {
             MaterializedTableCollectionName.BATCH_REQUEST: self._delete_batch_request_table,
             MaterializedTableCollectionName.BATCH_FEATURE: self._delete_batch_feature_table,
             MaterializedTableCollectionName.OBSERVATION: self._delete_observation_table,
             MaterializedTableCollectionName.HISTORICAL_FEATURE: self._delete_historical_feature_table,
+            MaterializedTableCollectionName.STATIC_SOURCE: self._delete_static_source_table,
+            MaterializedTableCollectionName.TARGET: self._delete_target_table,
         }
 
         # delete document stored at mongo
         deleted_document = await table_to_delete_action[self.task_payload.collection_name]()
 
         # delete table stored at data warehouse
         feature_store = await self.app_container.feature_store_service.get_document(
```

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/mixin.py` & `featurebyte-0.4.0/featurebyte/worker/task/mixin.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/observation_table.py` & `featurebyte-0.4.0/featurebyte/worker/task/observation_table.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/test_task.py` & `featurebyte-0.4.0/featurebyte/worker/task/test_task.py`

 * *Files identical despite different names*

### Comparing `featurebyte-0.3.1/featurebyte/worker/task/tile_task.py` & `featurebyte-0.4.0/featurebyte/worker/task/tile_task.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 """
 Test task
 """
 from __future__ import annotations
 
 from typing import Any, cast
 
-import importlib
-import json
-
 from featurebyte.logging import get_logger
 from featurebyte.schema.worker.task.tile import TileTaskPayload
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.session.manager import SessionManager
 from featurebyte.worker.task.base import BaseTask
 
 logger = get_logger(__name__)
@@ -28,22 +25,14 @@
         """
         Execute Tile task
         """
         logger.debug("Tile task started")
 
         payload = cast(TileTaskPayload, self.payload)
 
-        module = importlib.import_module(payload.module_path)
-        instance_class = getattr(module, payload.class_name)
-        instance_json = json.loads(payload.instance_str)
-
-        logger.debug(f"module: {payload.module_path}")
-        logger.debug(f"class_name: {payload.class_name}")
-        logger.debug(f"instance_str: {payload.instance_str}")
-
         # get feature store
         feature_store_service = FeatureStoreService(
             user=self.user,
             persistent=self.get_persistent(),
             catalog_id=payload.catalog_id,
         )
         feature_store = await feature_store_service.get_document(
@@ -56,12 +45,12 @@
                 feature_store.name: await self.get_credential(
                     user_id=payload.user_id, feature_store_name=feature_store.name
                 )
             }
         )
         db_session = await session_manager.get_session(feature_store)
 
-        instance = instance_class(session=db_session, **instance_json)
-
-        await instance.execute()
+        await self.app_container.tile_task_executor.execute(
+            session=db_session, params=payload.parameters
+        )
 
         logger.debug("Tile task ended")
```

### Comparing `featurebyte-0.3.1/featurebyte/worker/task_executor.py` & `featurebyte-0.4.0/featurebyte/worker/task_executor.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,30 +2,33 @@
 This module contains TaskExecutor class
 """
 from __future__ import annotations
 
 from typing import Any, Awaitable, Optional
 
 import asyncio
+import os
+from abc import abstractmethod
 from concurrent.futures import ThreadPoolExecutor
 from threading import Thread
 from uuid import UUID
 
 import gevent
+from celery import Task
 from celery.exceptions import SoftTimeLimitExceeded
 
 from featurebyte.config import get_home_path
 from featurebyte.enum import WorkerCommand
 from featurebyte.logging import get_logger
 from featurebyte.models.base import User
 from featurebyte.utils.credential import MongoBackedCredentialProvider
 from featurebyte.utils.messaging import Progress
 from featurebyte.utils.persistent import get_persistent
 from featurebyte.utils.storage import get_storage, get_temp_storage
-from featurebyte.worker import celery
+from featurebyte.worker import get_celery
 from featurebyte.worker.task.base import TASK_MAP
 
 logger = get_logger(__name__)
 
 
 def start_background_loop(loop: asyncio.AbstractEventLoop) -> None:
     """
@@ -100,96 +103,116 @@
             user=User(id=payload.get("user_id")),
             payload=payload,
             progress=progress,
             get_persistent=get_persistent,
             get_credential=credential_provider.get_credential,
             get_storage=get_storage,
             get_temp_storage=get_temp_storage,
+            get_celery=get_celery,
         )
+        self._setup_worker_config()
 
+    def _setup_worker_config(self) -> None:
+        """
+        Setup featurebyte config file for the worker
+        """
         home_path = get_home_path()
         if not home_path.exists():
             home_path.mkdir(parents=True)
 
         # override config file of the featurebyte-worker
+        featurebyte_server = os.environ.get("FEATUREBYTE_SERVER", "http://featurebyte-server:8088")
         config_path = home_path.joinpath("config.yaml")
         config_path.write_text(
             "# featurebyte-worker config file\n"
             "profile:\n"
             "  - name: worker\n"
-            "    api_url: http://featurebyte-server:8088\n\n"
+            f"    api_url: {featurebyte_server}\n\n"
+            "default_profile: worker\n\n",
+            encoding="utf-8",
         )
 
     async def execute(self) -> Any:
         """
         Execute the task
         """
         await self.task.execute()
 
 
-async def execute_task(request_id: UUID, **payload: Any) -> Any:
+class BaseCeleryTask(Task):
+    """
+    Base Celery task
     """
-    Execute Celery task
 
-    Parameters
-    ----------
-    request_id: UUID
-        Request ID
-    payload: Any
-        Task payload
+    name = "base_task"
+    progress_class = Progress
+    executor_class = TaskExecutor
 
-    Returns
-    -------
-    Any
-    """
-    progress = Progress(user_id=payload.get("user_id"), task_id=request_id)
-    executor = TaskExecutor(payload=payload, progress=progress)
-    # send initial progress to indicate task is started
-    progress.put({"percent": 0})
-    try:
-        return_val = await executor.execute()
-        # send final progress to indicate task is completed
-        progress.put({"percent": 100})
-        return return_val
-    finally:
-        # indicate stream is closed
-        progress.put({"percent": -1})
+    async def execute_task(self: Any, request_id: UUID, **payload: Any) -> Any:
+        """
+        Execute Celery task
 
+        Parameters
+        ----------
+        request_id: UUID
+            Request ID
+        payload: Any
+            Task payload
+
+        Returns
+        -------
+        Any
+        """
+        progress = self.progress_class(user_id=payload.get("user_id"), task_id=request_id)
+        executor = self.executor_class(payload=payload, progress=progress)
+        # send initial progress to indicate task is started
+        progress.put({"percent": 0})
+        try:
+            return_val = await executor.execute()
+            # send final progress to indicate task is completed
+            progress.put({"percent": 100})
+            return return_val
+        finally:
+            # indicate stream is closed
+            progress.put({"percent": -1})
 
-@celery.task(bind=True)
-def execute_io_task(self: Any, **payload: Any) -> Any:
-    """
-    Execute Celery task
+    @abstractmethod
+    def run(self: Any, *args: Any, **payload: Any) -> Any:
+        """
+        Execute Celery task
 
-    Parameters
-    ----------
-    self: Any
-        Celery Task
-    payload: Any
-        Task payload
+        Parameters
+        ----------
+        args: Any
+            Task arguments
+        payload: Any
+            Task payload
+
+        Returns
+        -------
+        Any
+        """
+        raise NotImplementedError
 
-    Returns
-    -------
-    Any
+
+class IOBoundTask(BaseCeleryTask):
+    """
+    Celery task for IO bound task
     """
-    # gevent celery worker pool does not support soft time limit,
-    # so we let "run_async" handle the timeout enforcement
-    return run_async(execute_task(self.request.id, **payload), timeout=self.request.timelimit[1])
 
+    name = "featurebyte.worker.task_executor.execute_io_task"
 
-@celery.task(bind=True)
-def execute_cpu_task(self: Any, **payload: Any) -> Any:
-    """
-    Execute Celery task
+    def run(self: Any, *args: Any, **payload: Any) -> Any:
+        return run_async(
+            self.execute_task(self.request.id, **payload), timeout=self.request.timelimit[1]
+        )
 
-    Parameters
-    ----------
-    self: Any
-        Celery Task
-    payload: Any
-        Task payload
 
-    Returns
-    -------
-    Any
+class CPUBoundTask(BaseCeleryTask):
+    """
+    Celery task for CPU bound task
     """
-    return asyncio.run(execute_task(self.request.id, **payload))
+
+    name = "featurebyte.worker.task_executor.execute_cpu_task"
+
+    def run(self: Any, *args: Any, **payload: Any) -> Any:
+        return asyncio.run(self.execute_task(self.request.id, **payload))
```

### Comparing `featurebyte-0.3.1/pyproject.toml` & `featurebyte-0.4.0/pyproject.toml`

 * *Files 4% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 allow_redefinition = false
 check_untyped_defs = true
 color_output = true
 disallow_any_generics = true
 disallow_incomplete_defs = true
 disallow_subclassing_any = false
 disallow_untyped_defs = true
-exclude = ['tests', 'docs', 'docker', '.github', 'featurebyte/api/templates']
+exclude = ['tests', 'docs', 'docker', '.github', 'featurebyte/api/templates', 'notebooks/prebuilt_catalogs.py']
 ignore_missing_imports = true
 implicit_reexport = false
 no_implicit_optional = true
 pretty = true
 python_version = 3.8
 show_column_numbers = true
 show_error_codes = true
@@ -88,24 +88,24 @@
     "featurebyte/sql/spark/*.jar",
 ]
 keywords = []
 license = "Elastic License 2.0"
 name = "featurebyte"
 readme = "README.md"
 repository = "https://github.com/featurebyte/featurebyte"
-version = "0.3.1"
+version = "0.4.0"
 
 [tool.poetry.dependencies]
 PyYAML = "^6.0"
+aiobotocore = { version = "^2.4.0", extras = ["boto3"] }
 aiofiles = "^22.1.0"
 aioredis = { version = "^2.0.1", optional = true }
 alive-progress = "^3.1.1"
 asyncache = "^0.3.1"
 black = "^23.3.0"
-boto3 = { version = "1.24.59", optional = true }   # Upstream aiobotocore only allows a specific version of boto3
 cachetools = { version = "^5.2.0", optional = true }
 celery = { version = "^5.2.6", extras = ["redis"], optional = true }
 celerybeat-mongo = { version = "^0.2.0", optional = true }
 cryptography = "^40.0.2"
 databricks-cli = { version = "^0.17.3", optional = true }
 databricks-sql-connector = { version = "^2.5.0", optional = true }
 fastapi = { version =  "^0.95.1", optional = true }
@@ -119,45 +119,49 @@
 orjson = "^3.8.3"
 pandas = "^1.5.3"
 pdfkit = { version = "^1.0.0", optional = true }
 pyarrow = "^10"
 pydantic = "^1.9.6"
 pyhive = { version = "^0.6.5", optional = true }
 pymongo = "^4.1.1"
-python = ">=3.8,<4.0"
+python = ">=3.8,<3.13"
 python-multipart = "^0.0.6"
 python-on-whales = "^0.60.0"
 redis = {version = "^5.0.0b1", optional = true, allow-prereleases = true}
 requests = "^2.27.1"
+requests-kerberos = { version = "^0.14.0", optional = true }
 rich = "^13.3.4"
 sasl = { version = "^0.3.1", optional = true }
 smart-open = { version = "^6.3.0", extras = ["azure", "gcs"], optional = true }
-snowflake-connector-python = { version = "^3.0.3,!=3.0.4", optional = true }
+snowflake-connector-python = { version = "^3.0.4", optional = true }
 sqlglot = "^10.1.3,<10.4"  # SQL generation doesn't match as >10.4 double quotes are missing
 thrift-sasl = { version = "^0.4.3", optional = true }
 typeguard = "^2.13.3"
 typer = "^0.7.0"
+typing-extensions = "4.5.0"
 uvicorn = { version = "^0.21.1", extras = ["standard"], optional = true }
 websocket-client = "^1.5.1"
 wheel = "0.40.0"
 
 [tool.poetry.extras]
-server = ["cachetools", "databricks-cli", "fastapi", "motor", "snowflake-connector-python", "uvicorn", "pdfkit", "pyhive", "sasl", "thrift-sasl", "boto3", "smart-open", "celery", "redis", "celerybeat-mongo", "databricks-sql-connector", "featurebyte-freeware", "gevent", "aioredis"]
+server = ["cachetools", "databricks-cli", "fastapi", "motor", "snowflake-connector-python", "uvicorn", "pdfkit", "pyhive", "sasl", "thrift-sasl", "boto3", "smart-open", "celery", "redis", "celerybeat-mongo", "databricks-sql-connector", "featurebyte-freeware", "gevent", "aioredis", "requests-kerberos"]
 
 [tool.poetry.group.dev.dependencies]
 freezegun = "^1.2.1"
 httpx = "^0.24.0"
 junitparser = "^2.8.0"
+jupyterlab = "^4.0.2"
 mongomock = "^4.0.0"
 mongomock-motor = "^0.0.12"
 pip-licenses = "^3.5.4"
 pre-commit = "^2.20.0"
 pytest = "^7.2.0"
 pytest-asyncio = "^0.19.0"
 pytest-cov = "^4.0.0"
+pytest-rerunfailures = "^11.1.2"
 pytest-timeout = "^2.1.0"
 pytest-xdist = "^3.0.2"
 pyupgrade = "^2.37.2"
 toml-sort = "^0.20.0"
 
 [tool.poetry.group.docs.dependencies]
 docstring-parser = "^0.15"
@@ -231,9 +235,10 @@
 min-similarity-lines = 10
 
 [tool.pytest.ini_options]
 addopts = ["--strict-markers", "-v", "--doctest-modules", "--durations=50", "--doctest-continue-on-failure", "--ignore-glob=tests/fixtures/*"] # Extra options:
 doctest_optionflags = ["NUMBER", "NORMALIZE_WHITESPACE", "IGNORE_EXCEPTION_DETAIL"]
 markers = [
     "disable_task_manager_mock: disable use of autouse task manager fixture",
+    "no_mock_websocket_client: skip mocking websocket client",
 ]
 norecursedirs = ["hooks", "*.egg", ".eggs", "dist", "build", "docs", ".tox", ".git", "__pycache__"]
```

### Comparing `featurebyte-0.3.1/PKG-INFO` & `featurebyte-0.4.0/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 Metadata-Version: 2.1
 Name: featurebyte
-Version: 0.3.1
+Version: 0.4.0
 Summary: Python Library for FeatureOps
 Home-page: https://featurebyte.com
 License: Elastic License 2.0
 Author: FeatureByte
 Author-email: it-admin@featurebyte.com
-Requires-Python: >=3.8,<4.0
+Requires-Python: >=3.8,<3.13
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
 Classifier: License :: Other/Proprietary License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Provides-Extra: server
 Requires-Dist: PyYAML (>=6.0,<7.0)
+Requires-Dist: aiobotocore[boto3] (>=2.4.0,<3.0.0)
 Requires-Dist: aiofiles (>=22.1.0,<23.0.0)
 Requires-Dist: aioredis (>=2.0.1,<3.0.0) ; extra == "server"
 Requires-Dist: alive-progress (>=3.1.1,<4.0.0)
 Requires-Dist: asyncache (>=0.3.1,<0.4.0)
 Requires-Dist: black (>=23.3.0,<24.0.0)
-Requires-Dist: boto3 (==1.24.59) ; extra == "server"
 Requires-Dist: cachetools (>=5.2.0,<6.0.0) ; extra == "server"
 Requires-Dist: celery[redis] (>=5.2.6,<6.0.0) ; extra == "server"
 Requires-Dist: celerybeat-mongo (>=0.2.0,<0.3.0) ; extra == "server"
 Requires-Dist: cryptography (>=40.0.2,<41.0.0)
 Requires-Dist: databricks-cli (>=0.17.3,<0.18.0) ; extra == "server"
 Requires-Dist: databricks-sql-connector (>=2.5.0,<3.0.0) ; extra == "server"
 Requires-Dist: fastapi (>=0.95.1,<0.96.0) ; extra == "server"
@@ -46,22 +46,24 @@
 Requires-Dist: pydantic (>=1.9.6,<2.0.0)
 Requires-Dist: pyhive (>=0.6.5,<0.7.0) ; extra == "server"
 Requires-Dist: pymongo (>=4.1.1,<5.0.0)
 Requires-Dist: python-multipart (>=0.0.6,<0.0.7)
 Requires-Dist: python-on-whales (>=0.60.0,<0.61.0)
 Requires-Dist: redis (>=5.0.0b1,<6.0.0) ; extra == "server"
 Requires-Dist: requests (>=2.27.1,<3.0.0)
+Requires-Dist: requests-kerberos (>=0.14.0,<0.15.0) ; extra == "server"
 Requires-Dist: rich (>=13.3.4,<14.0.0)
 Requires-Dist: sasl (>=0.3.1,<0.4.0) ; extra == "server"
 Requires-Dist: smart-open[azure,gcs] (>=6.3.0,<7.0.0) ; extra == "server"
-Requires-Dist: snowflake-connector-python (>=3.0.3,<4.0.0,!=3.0.4) ; extra == "server"
+Requires-Dist: snowflake-connector-python (>=3.0.4,<4.0.0) ; extra == "server"
 Requires-Dist: sqlglot (>=10.1.3,<10.4)
 Requires-Dist: thrift-sasl (>=0.4.3,<0.5.0) ; extra == "server"
 Requires-Dist: typeguard (>=2.13.3,<3.0.0)
 Requires-Dist: typer (>=0.7.0,<0.8.0)
+Requires-Dist: typing-extensions (==4.5.0)
 Requires-Dist: uvicorn[standard] (>=0.21.1,<0.22.0) ; extra == "server"
 Requires-Dist: websocket-client (>=1.5.1,<2.0.0)
 Requires-Dist: wheel (==0.40.0)
 Project-URL: Documentation, https://docs.featurebyte.com
 Project-URL: Repository, https://github.com/featurebyte/featurebyte
 Description-Content-Type: text/markdown
```

